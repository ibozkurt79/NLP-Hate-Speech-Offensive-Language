{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "author": "Alper.Halbutogullari@gmail.com Â© 2018"
   },
   "source": [
    "# NLP for Twitter Sentiment Analysis (Hate Speech/Offensive Lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If at any point you need to install a package, come here and uncomment the corresponding line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y pyspark\n",
    "#!conda install -y -c conda-forge findspark\n",
    "#!conda install -y keras\n",
    "#!conda install -y tqdm\n",
    "#!conda install -y gensim\n",
    "#!pip install wget\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error related to JAVA_HOME or SPARK_HOME, you either don't have the jdk/pyspark installed or the paths are not set. Make sure you installed them. You may set the paths using the following (Warning: the paths may be different in your system!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Java\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\Spark\\spark-2.4.0-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input, Dense, Dropout, concatenate, Activation, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model, Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from shutil import unpack_archive\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, auc, explained_variance_score, roc_curve\n",
    "\n",
    "import pyspark as ps\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, IDF, NGram, StringIndexer, Tokenizer, VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new SparkContext\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # create SparkContext on 4 CPUs\n",
    "    # feel free to increase it if you have more cores on your machine\n",
    "    sc = ps.SparkContext('local[4]')\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Created a new SparkContext\")\n",
    "except ValueError:\n",
    "    warnings.warn(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[4]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter data\n",
    "\n",
    "### Download and load the data\n",
    "\n",
    "You may either get and use your own data, or use the one I prepared. Just set the following variable and run the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you only have to download the data once. You may "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"twitter-hate-speech-classifier.csv\", encoding='latin1')\n",
    "#f    = open(\"twitter-hate-speech-classifier.csv\",\"rb\")\n",
    "#df = f.read().decode(errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>does_this_tweet_contain_hate_speech</th>\n",
       "      <th>does_this_tweet_contain_hate_speech:confidence</th>\n",
       "      <th>_created_at</th>\n",
       "      <th>orig__golden</th>\n",
       "      <th>orig__last_judgment_at</th>\n",
       "      <th>orig__trusted_judgments</th>\n",
       "      <th>orig__unit_id</th>\n",
       "      <th>orig__unit_state</th>\n",
       "      <th>_updated_at</th>\n",
       "      <th>orig_does_this_tweet_contain_hate_speech</th>\n",
       "      <th>does_this_tweet_contain_hate_speech_gold</th>\n",
       "      <th>does_this_tweet_contain_hate_speech_gold_reason</th>\n",
       "      <th>does_this_tweet_contain_hate_speechconfidence</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853718217</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>615561535.0</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.666196e+09</td>\n",
       "      <td>Warning: penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>853718218</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>615561723.0</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.295121e+08</td>\n",
       "      <td>Fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>853718219</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>615562039.0</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.956238e+08</td>\n",
       "      <td>@sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853718220</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>615562068.0</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.975147e+08</td>\n",
       "      <td>\"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>853718221</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.5185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>615562488.0</td>\n",
       "      <td>golden</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.889236e+08</td>\n",
       "      <td>@Zhugstubble You heard me bitch but any way I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  853718217     True      golden                  86               NaN   \n",
       "1  853718218     True      golden                  92               NaN   \n",
       "2  853718219     True      golden                  86               NaN   \n",
       "3  853718220     True      golden                  98               NaN   \n",
       "4  853718221     True      golden                  88               NaN   \n",
       "\n",
       "                 does_this_tweet_contain_hate_speech  \\\n",
       "0  The tweet uses offensive language but not hate...   \n",
       "1                     The tweet contains hate speech   \n",
       "2                     The tweet contains hate speech   \n",
       "3                     The tweet contains hate speech   \n",
       "4  The tweet uses offensive language but not hate...   \n",
       "\n",
       "   does_this_tweet_contain_hate_speech:confidence  _created_at orig__golden  \\\n",
       "0                                          0.6013          NaN         True   \n",
       "1                                          0.7227          NaN         True   \n",
       "2                                          0.5229          NaN         True   \n",
       "3                                          0.5184          NaN         True   \n",
       "4                                          0.5185          NaN         True   \n",
       "\n",
       "   orig__last_judgment_at  orig__trusted_judgments  orig__unit_id  \\\n",
       "0                     NaN                      0.0    615561535.0   \n",
       "1                     NaN                      0.0    615561723.0   \n",
       "2                     NaN                      0.0    615562039.0   \n",
       "3                     NaN                      0.0    615562068.0   \n",
       "4                     NaN                      0.0    615562488.0   \n",
       "\n",
       "  orig__unit_state  _updated_at orig_does_this_tweet_contain_hate_speech  \\\n",
       "0           golden          NaN           The tweet contains hate speech   \n",
       "1           golden          NaN           The tweet contains hate speech   \n",
       "2           golden          NaN           The tweet contains hate speech   \n",
       "3           golden          NaN           The tweet contains hate speech   \n",
       "4           golden          NaN           The tweet contains hate speech   \n",
       "\n",
       "            does_this_tweet_contain_hate_speech_gold  \\\n",
       "0  The tweet contains hate speech\\nThe tweet uses...   \n",
       "1  The tweet contains hate speech\\nThe tweet uses...   \n",
       "2  The tweet contains hate speech\\nThe tweet uses...   \n",
       "3  The tweet contains hate speech\\nThe tweet uses...   \n",
       "4  The tweet contains hate speech\\nThe tweet uses...   \n",
       "\n",
       "   does_this_tweet_contain_hate_speech_gold_reason  \\\n",
       "0                                              NaN   \n",
       "1                                              NaN   \n",
       "2                                              NaN   \n",
       "3                                              NaN   \n",
       "4                                              NaN   \n",
       "\n",
       "   does_this_tweet_contain_hate_speechconfidence      tweet_id  \\\n",
       "0                                            1.0  1.666196e+09   \n",
       "1                                            1.0  4.295121e+08   \n",
       "2                                            1.0  3.956238e+08   \n",
       "3                                            1.0  4.975147e+08   \n",
       "4                                            1.0  5.889236e+08   \n",
       "\n",
       "                                          tweet_text  \n",
       "0       Warning: penny boards will make you a faggot  \n",
       "1                                         Fuck dykes  \n",
       "2  @sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...  \n",
       "3  \"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...  \n",
       "4  @Zhugstubble You heard me bitch but any way I'...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14509, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['golden', 'finalized'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_unit_state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_created_at'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([True, nan], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['orig__golden'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['_golden', '_unit_state', '_trusted_judgments', '_last_judgment_at','_created_at','orig__unit_id',\n",
    "         'orig__golden', 'orig__trusted_judgments','orig__last_judgment_at', 'orig__unit_state','_updated_at','does_this_tweet_contain_hate_speech_gold_reason',\n",
    "        'tweet_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14509"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_unit_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>does_this_tweet_contain_hate_speech</th>\n",
       "      <th>does_this_tweet_contain_hate_speech:confidence</th>\n",
       "      <th>orig_does_this_tweet_contain_hate_speech</th>\n",
       "      <th>does_this_tweet_contain_hate_speech_gold</th>\n",
       "      <th>does_this_tweet_contain_hate_speechconfidence</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853718217</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Warning: penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>853718218</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7227</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>853718219</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853718220</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5184</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>853718221</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.5185</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@Zhugstubble You heard me bitch but any way I'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id                does_this_tweet_contain_hate_speech  \\\n",
       "0  853718217  The tweet uses offensive language but not hate...   \n",
       "1  853718218                     The tweet contains hate speech   \n",
       "2  853718219                     The tweet contains hate speech   \n",
       "3  853718220                     The tweet contains hate speech   \n",
       "4  853718221  The tweet uses offensive language but not hate...   \n",
       "\n",
       "   does_this_tweet_contain_hate_speech:confidence  \\\n",
       "0                                          0.6013   \n",
       "1                                          0.7227   \n",
       "2                                          0.5229   \n",
       "3                                          0.5184   \n",
       "4                                          0.5185   \n",
       "\n",
       "  orig_does_this_tweet_contain_hate_speech  \\\n",
       "0           The tweet contains hate speech   \n",
       "1           The tweet contains hate speech   \n",
       "2           The tweet contains hate speech   \n",
       "3           The tweet contains hate speech   \n",
       "4           The tweet contains hate speech   \n",
       "\n",
       "            does_this_tweet_contain_hate_speech_gold  \\\n",
       "0  The tweet contains hate speech\\nThe tweet uses...   \n",
       "1  The tweet contains hate speech\\nThe tweet uses...   \n",
       "2  The tweet contains hate speech\\nThe tweet uses...   \n",
       "3  The tweet contains hate speech\\nThe tweet uses...   \n",
       "4  The tweet contains hate speech\\nThe tweet uses...   \n",
       "\n",
       "   does_this_tweet_contain_hate_speechconfidence  \\\n",
       "0                                            1.0   \n",
       "1                                            1.0   \n",
       "2                                            1.0   \n",
       "3                                            1.0   \n",
       "4                                            1.0   \n",
       "\n",
       "                                          tweet_text  \n",
       "0       Warning: penny boards will make you a faggot  \n",
       "1                                         Fuck dykes  \n",
       "2  @sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...  \n",
       "3  \"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...  \n",
       "4  @Zhugstubble You heard me bitch but any way I'...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>does_this_tweet_contain_hate_speech</th>\n",
       "      <th>does_this_tweet_contain_hate_speech:confidence</th>\n",
       "      <th>orig_does_this_tweet_contain_hate_speech</th>\n",
       "      <th>does_this_tweet_contain_hate_speech_gold</th>\n",
       "      <th>does_this_tweet_contain_hate_speechconfidence</th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>853718217</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6013</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Warning: penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>853718218</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7227</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>853718219</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5229</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853718220</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5184</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>853718221</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.5185</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@Zhugstubble You heard me bitch but any way I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>853718223</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @ivanrabago_: @_WhitePonyJr_ looking like f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>853718224</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5619</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Well I thought you knew actually RT @KingHorse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>853718225</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6419</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@Stonisnipezz I know. It was a joke, faggot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>853718226</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6407</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'm tired of people saying I look like my brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>853718227</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7619</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>#VoteBlue2014 Yeah. CUZ 8 million people in fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>853718228</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5606</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@AndreBerto word is you use roids, stupid hypo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>853718232</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7620</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>They used to tie both ends of a niggers legs t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>853718233</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.6833</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Good night fags and fagettes (that's the femal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>853718234</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.5204</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I cant stand no crybaby ass nigga. If you gonn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>853718235</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.6410</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @nerdkiller669: @ShinSnipes no you're a nigger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>853718236</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6419</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>\"I'll fuck you til you love me faggot\" - Mike ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>853718237</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.7608</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@OxbloodStomper @PalePixie88 @SlaveCatcher88 w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>853718238</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.9221</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I fucking hate you niggers bruh... http://t.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>853718242</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.9204</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Also, Happy Armistice Day, anachronists!\\n\\n&amp;#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>853718244</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.8388</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @BitchPlsComedy: apparently &amp;#8220;bae&amp;#822...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>853718252</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.8391</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @kelter1: The fact that Kim Kardashian's ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>853718254</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.5209</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@LastProtestants @stomponato @delschilling You...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>853718257</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sitting alone watching White Chicks, no pants,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>853718261</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.9620</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @skythedon: Huge ass, small waist &amp;amp; oka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>853718265</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @KayciMalynn: I just can't help but to hate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>853718266</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @HBCUfessions: You females overlook us geek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>853718267</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.8021</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @macktology101: \"Sunday funday\" hoe quotes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>853718269</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.9226</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @JoeBudden: Young, attractive, successful, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>853718270</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.5593</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>The tweet contains hate speech\\nThe tweet uses...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RT @opticzodiac: @JerKzTheGreaT @Erupts @BajaO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>853718272</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.9192</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@KristyT @aliciafiasco @luciouskitty \"you know...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14479</th>\n",
       "      <td>853733873</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I can't deal with a slow internet connection. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>853733874</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mental Health Practitioner  - Coon Rapids Day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14481</th>\n",
       "      <td>853733875</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mental Health Practitioner  - Coon Rapids Day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14482</th>\n",
       "      <td>853733876</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.6683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mental Health Practitioner  - Coon Rapids Day ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14483</th>\n",
       "      <td>853733877</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.6684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ben Carson thinks it's okay to have A confeder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14484</th>\n",
       "      <td>853733878</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#YodaSpeakASaying \\n\\nKills, captivity does.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14485</th>\n",
       "      <td>853733879</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.3351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>diamond keep actin up ima go to her job and be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14486</th>\n",
       "      <td>853733880</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.6615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cries. I'm turning into a weeb-coon https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14487</th>\n",
       "      <td>853733881</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saw this gorgeous 4 1/2 year old Maine Coon to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14488</th>\n",
       "      <td>853733882</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jet Stream the Maine Coon riding out last nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14489</th>\n",
       "      <td>853733883</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@LenaOverFlowers I'm not even a coon girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14490</th>\n",
       "      <td>853733884</td>\n",
       "      <td>The tweet contains hate speech</td>\n",
       "      <td>0.6633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>From what I've seen men and women coon at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14491</th>\n",
       "      <td>853733885</td>\n",
       "      <td>The tweet uses offensive language but not hate...</td>\n",
       "      <td>0.6616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"BW,take care of me! pls buy my coon ass somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14492</th>\n",
       "      <td>853733886</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Interesting data on quantifying value of the I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14493</th>\n",
       "      <td>853733887</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aryan_twr60 thank you_ÃÂ«ÃÃ_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14494</th>\n",
       "      <td>853733888</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@sardesairajdeep @DwaipayanM if he is Aryan th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14495</th>\n",
       "      <td>853733889</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@sardesairajdeep sir if he is Aryan then #mall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14496</th>\n",
       "      <td>853733890</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radio Nordfront interview a Russian patriot an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14497</th>\n",
       "      <td>853733891</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@nupursanon hope u check ur fb messages, all m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14498</th>\n",
       "      <td>853733892</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WowÃ_Ã__A Jewish man having his nose measured ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14499</th>\n",
       "      <td>853733893</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>oldpicsarchive: A Jewish man having his nose m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14500</th>\n",
       "      <td>853733894</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.6684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A Jewish man having his nose measured during A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14501</th>\n",
       "      <td>853733895</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AO - aryan huy miss you na!! Happy that i'll b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14502</th>\n",
       "      <td>853733896</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@AryanTimsina Aryan, what is the version of Fifa?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14503</th>\n",
       "      <td>853733897</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>measured during Aryan race determination tests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14504</th>\n",
       "      <td>853733898</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.3418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm sorry. Did I offend your white supremacist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14505</th>\n",
       "      <td>853733899</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>0.6804</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@tradethecycles Caucasian euro aryan whatever....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14506</th>\n",
       "      <td>853733900</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@yadavakhilesh sir a patient named aryan khan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14507</th>\n",
       "      <td>853733901</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Iamshivachari  Happy birthday bro _ÃÂ«ÃÃ_ \\nHa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14508</th>\n",
       "      <td>853733902</td>\n",
       "      <td>The tweet is not offensive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Hajrahkiayni Aryan Kapoor is such a cute name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14476 rows Ã 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _unit_id                does_this_tweet_contain_hate_speech  \\\n",
       "0      853718217  The tweet uses offensive language but not hate...   \n",
       "1      853718218                     The tweet contains hate speech   \n",
       "2      853718219                     The tweet contains hate speech   \n",
       "3      853718220                     The tweet contains hate speech   \n",
       "4      853718221  The tweet uses offensive language but not hate...   \n",
       "6      853718223                     The tweet contains hate speech   \n",
       "7      853718224                     The tweet contains hate speech   \n",
       "8      853718225  The tweet uses offensive language but not hate...   \n",
       "9      853718226  The tweet uses offensive language but not hate...   \n",
       "10     853718227                     The tweet contains hate speech   \n",
       "11     853718228                     The tweet contains hate speech   \n",
       "15     853718232                     The tweet contains hate speech   \n",
       "16     853718233                     The tweet contains hate speech   \n",
       "17     853718234  The tweet uses offensive language but not hate...   \n",
       "18     853718235                     The tweet contains hate speech   \n",
       "19     853718236  The tweet uses offensive language but not hate...   \n",
       "20     853718237                     The tweet contains hate speech   \n",
       "21     853718238                     The tweet contains hate speech   \n",
       "25     853718242                         The tweet is not offensive   \n",
       "27     853718244                         The tweet is not offensive   \n",
       "35     853718252                         The tweet is not offensive   \n",
       "37     853718254                         The tweet is not offensive   \n",
       "40     853718257                         The tweet is not offensive   \n",
       "44     853718261  The tweet uses offensive language but not hate...   \n",
       "48     853718265  The tweet uses offensive language but not hate...   \n",
       "49     853718266  The tweet uses offensive language but not hate...   \n",
       "50     853718267  The tweet uses offensive language but not hate...   \n",
       "52     853718269  The tweet uses offensive language but not hate...   \n",
       "53     853718270                     The tweet contains hate speech   \n",
       "55     853718272  The tweet uses offensive language but not hate...   \n",
       "...          ...                                                ...   \n",
       "14479  853733873                         The tweet is not offensive   \n",
       "14480  853733874                         The tweet is not offensive   \n",
       "14481  853733875                         The tweet is not offensive   \n",
       "14482  853733876                         The tweet is not offensive   \n",
       "14483  853733877                     The tweet contains hate speech   \n",
       "14484  853733878                         The tweet is not offensive   \n",
       "14485  853733879                         The tweet is not offensive   \n",
       "14486  853733880                         The tweet is not offensive   \n",
       "14487  853733881                         The tweet is not offensive   \n",
       "14488  853733882                         The tweet is not offensive   \n",
       "14489  853733883  The tweet uses offensive language but not hate...   \n",
       "14490  853733884                     The tweet contains hate speech   \n",
       "14491  853733885  The tweet uses offensive language but not hate...   \n",
       "14492  853733886                         The tweet is not offensive   \n",
       "14493  853733887                         The tweet is not offensive   \n",
       "14494  853733888                         The tweet is not offensive   \n",
       "14495  853733889                         The tweet is not offensive   \n",
       "14496  853733890                         The tweet is not offensive   \n",
       "14497  853733891                         The tweet is not offensive   \n",
       "14498  853733892                         The tweet is not offensive   \n",
       "14499  853733893                         The tweet is not offensive   \n",
       "14500  853733894                         The tweet is not offensive   \n",
       "14501  853733895                         The tweet is not offensive   \n",
       "14502  853733896                         The tweet is not offensive   \n",
       "14503  853733897                         The tweet is not offensive   \n",
       "14504  853733898                         The tweet is not offensive   \n",
       "14505  853733899                         The tweet is not offensive   \n",
       "14506  853733900                         The tweet is not offensive   \n",
       "14507  853733901                         The tweet is not offensive   \n",
       "14508  853733902                         The tweet is not offensive   \n",
       "\n",
       "       does_this_tweet_contain_hate_speech:confidence  \\\n",
       "0                                              0.6013   \n",
       "1                                              0.7227   \n",
       "2                                              0.5229   \n",
       "3                                              0.5184   \n",
       "4                                              0.5185   \n",
       "6                                              0.5207   \n",
       "7                                              0.5619   \n",
       "8                                              0.6419   \n",
       "9                                              0.6407   \n",
       "10                                             0.7619   \n",
       "11                                             0.5606   \n",
       "15                                             0.7620   \n",
       "16                                             0.6833   \n",
       "17                                             0.5204   \n",
       "18                                             0.6410   \n",
       "19                                             0.6419   \n",
       "20                                             0.7608   \n",
       "21                                             0.9221   \n",
       "25                                             0.9204   \n",
       "27                                             0.8388   \n",
       "35                                             0.8391   \n",
       "37                                             0.5209   \n",
       "40                                             1.0000   \n",
       "44                                             0.9620   \n",
       "48                                             0.6837   \n",
       "49                                             0.9608   \n",
       "50                                             0.8021   \n",
       "52                                             0.9226   \n",
       "53                                             0.5593   \n",
       "55                                             0.9192   \n",
       "...                                               ...   \n",
       "14479                                          1.0000   \n",
       "14480                                          0.6771   \n",
       "14481                                          1.0000   \n",
       "14482                                          0.6683   \n",
       "14483                                          0.6684   \n",
       "14484                                          1.0000   \n",
       "14485                                          0.3351   \n",
       "14486                                          0.6615   \n",
       "14487                                          1.0000   \n",
       "14488                                          1.0000   \n",
       "14489                                          1.0000   \n",
       "14490                                          0.6633   \n",
       "14491                                          0.6616   \n",
       "14492                                          1.0000   \n",
       "14493                                          1.0000   \n",
       "14494                                          0.6579   \n",
       "14495                                          1.0000   \n",
       "14496                                          1.0000   \n",
       "14497                                          1.0000   \n",
       "14498                                          1.0000   \n",
       "14499                                          1.0000   \n",
       "14500                                          0.6684   \n",
       "14501                                          1.0000   \n",
       "14502                                          1.0000   \n",
       "14503                                          1.0000   \n",
       "14504                                          0.3418   \n",
       "14505                                          0.6804   \n",
       "14506                                          1.0000   \n",
       "14507                                          1.0000   \n",
       "14508                                          1.0000   \n",
       "\n",
       "                orig_does_this_tweet_contain_hate_speech  \\\n",
       "0                         The tweet contains hate speech   \n",
       "1                         The tweet contains hate speech   \n",
       "2                         The tweet contains hate speech   \n",
       "3                         The tweet contains hate speech   \n",
       "4                         The tweet contains hate speech   \n",
       "6                         The tweet contains hate speech   \n",
       "7                         The tweet contains hate speech   \n",
       "8                         The tweet contains hate speech   \n",
       "9                         The tweet contains hate speech   \n",
       "10                        The tweet contains hate speech   \n",
       "11                        The tweet contains hate speech   \n",
       "15                        The tweet contains hate speech   \n",
       "16                        The tweet contains hate speech   \n",
       "17                        The tweet contains hate speech   \n",
       "18                        The tweet contains hate speech   \n",
       "19                        The tweet contains hate speech   \n",
       "20                        The tweet contains hate speech   \n",
       "21                        The tweet contains hate speech   \n",
       "25                            The tweet is not offensive   \n",
       "27                            The tweet is not offensive   \n",
       "35                            The tweet is not offensive   \n",
       "37                            The tweet is not offensive   \n",
       "40                            The tweet is not offensive   \n",
       "44     The tweet uses offensive language but not hate...   \n",
       "48     The tweet uses offensive language but not hate...   \n",
       "49     The tweet uses offensive language but not hate...   \n",
       "50     The tweet uses offensive language but not hate...   \n",
       "52     The tweet uses offensive language but not hate...   \n",
       "53     The tweet uses offensive language but not hate...   \n",
       "55     The tweet uses offensive language but not hate...   \n",
       "...                                                  ...   \n",
       "14479                                                NaN   \n",
       "14480                                                NaN   \n",
       "14481                                                NaN   \n",
       "14482                                                NaN   \n",
       "14483                                                NaN   \n",
       "14484                                                NaN   \n",
       "14485                                                NaN   \n",
       "14486                                                NaN   \n",
       "14487                                                NaN   \n",
       "14488                                                NaN   \n",
       "14489                                                NaN   \n",
       "14490                                                NaN   \n",
       "14491                                                NaN   \n",
       "14492                                                NaN   \n",
       "14493                                                NaN   \n",
       "14494                                                NaN   \n",
       "14495                                                NaN   \n",
       "14496                                                NaN   \n",
       "14497                                                NaN   \n",
       "14498                                                NaN   \n",
       "14499                                                NaN   \n",
       "14500                                                NaN   \n",
       "14501                                                NaN   \n",
       "14502                                                NaN   \n",
       "14503                                                NaN   \n",
       "14504                                                NaN   \n",
       "14505                                                NaN   \n",
       "14506                                                NaN   \n",
       "14507                                                NaN   \n",
       "14508                                                NaN   \n",
       "\n",
       "                does_this_tweet_contain_hate_speech_gold  \\\n",
       "0      The tweet contains hate speech\\nThe tweet uses...   \n",
       "1      The tweet contains hate speech\\nThe tweet uses...   \n",
       "2      The tweet contains hate speech\\nThe tweet uses...   \n",
       "3      The tweet contains hate speech\\nThe tweet uses...   \n",
       "4      The tweet contains hate speech\\nThe tweet uses...   \n",
       "6      The tweet contains hate speech\\nThe tweet uses...   \n",
       "7      The tweet contains hate speech\\nThe tweet uses...   \n",
       "8      The tweet contains hate speech\\nThe tweet uses...   \n",
       "9      The tweet contains hate speech\\nThe tweet uses...   \n",
       "10     The tweet contains hate speech\\nThe tweet uses...   \n",
       "11     The tweet contains hate speech\\nThe tweet uses...   \n",
       "15     The tweet contains hate speech\\nThe tweet uses...   \n",
       "16     The tweet contains hate speech\\nThe tweet uses...   \n",
       "17     The tweet contains hate speech\\nThe tweet uses...   \n",
       "18     The tweet contains hate speech\\nThe tweet uses...   \n",
       "19     The tweet contains hate speech\\nThe tweet uses...   \n",
       "20     The tweet contains hate speech\\nThe tweet uses...   \n",
       "21     The tweet contains hate speech\\nThe tweet uses...   \n",
       "25     The tweet uses offensive language but not hate...   \n",
       "27     The tweet uses offensive language but not hate...   \n",
       "35     The tweet uses offensive language but not hate...   \n",
       "37     The tweet uses offensive language but not hate...   \n",
       "40     The tweet uses offensive language but not hate...   \n",
       "44     The tweet contains hate speech\\nThe tweet uses...   \n",
       "48     The tweet contains hate speech\\nThe tweet uses...   \n",
       "49     The tweet uses offensive language but not hate...   \n",
       "50     The tweet uses offensive language but not hate...   \n",
       "52     The tweet uses offensive language but not hate...   \n",
       "53     The tweet contains hate speech\\nThe tweet uses...   \n",
       "55     The tweet uses offensive language but not hate...   \n",
       "...                                                  ...   \n",
       "14479                                                NaN   \n",
       "14480                                                NaN   \n",
       "14481                                                NaN   \n",
       "14482                                                NaN   \n",
       "14483                                                NaN   \n",
       "14484                                                NaN   \n",
       "14485                                                NaN   \n",
       "14486                                                NaN   \n",
       "14487                                                NaN   \n",
       "14488                                                NaN   \n",
       "14489                                                NaN   \n",
       "14490                                                NaN   \n",
       "14491                                                NaN   \n",
       "14492                                                NaN   \n",
       "14493                                                NaN   \n",
       "14494                                                NaN   \n",
       "14495                                                NaN   \n",
       "14496                                                NaN   \n",
       "14497                                                NaN   \n",
       "14498                                                NaN   \n",
       "14499                                                NaN   \n",
       "14500                                                NaN   \n",
       "14501                                                NaN   \n",
       "14502                                                NaN   \n",
       "14503                                                NaN   \n",
       "14504                                                NaN   \n",
       "14505                                                NaN   \n",
       "14506                                                NaN   \n",
       "14507                                                NaN   \n",
       "14508                                                NaN   \n",
       "\n",
       "       does_this_tweet_contain_hate_speechconfidence  \\\n",
       "0                                                1.0   \n",
       "1                                                1.0   \n",
       "2                                                1.0   \n",
       "3                                                1.0   \n",
       "4                                                1.0   \n",
       "6                                                1.0   \n",
       "7                                                1.0   \n",
       "8                                                1.0   \n",
       "9                                                1.0   \n",
       "10                                               1.0   \n",
       "11                                               1.0   \n",
       "15                                               1.0   \n",
       "16                                               1.0   \n",
       "17                                               1.0   \n",
       "18                                               1.0   \n",
       "19                                               1.0   \n",
       "20                                               1.0   \n",
       "21                                               1.0   \n",
       "25                                               1.0   \n",
       "27                                               1.0   \n",
       "35                                               1.0   \n",
       "37                                               1.0   \n",
       "40                                               1.0   \n",
       "44                                               1.0   \n",
       "48                                               1.0   \n",
       "49                                               1.0   \n",
       "50                                               1.0   \n",
       "52                                               1.0   \n",
       "53                                               1.0   \n",
       "55                                               1.0   \n",
       "...                                              ...   \n",
       "14479                                            NaN   \n",
       "14480                                            NaN   \n",
       "14481                                            NaN   \n",
       "14482                                            NaN   \n",
       "14483                                            NaN   \n",
       "14484                                            NaN   \n",
       "14485                                            NaN   \n",
       "14486                                            NaN   \n",
       "14487                                            NaN   \n",
       "14488                                            NaN   \n",
       "14489                                            NaN   \n",
       "14490                                            NaN   \n",
       "14491                                            NaN   \n",
       "14492                                            NaN   \n",
       "14493                                            NaN   \n",
       "14494                                            NaN   \n",
       "14495                                            NaN   \n",
       "14496                                            NaN   \n",
       "14497                                            NaN   \n",
       "14498                                            NaN   \n",
       "14499                                            NaN   \n",
       "14500                                            NaN   \n",
       "14501                                            NaN   \n",
       "14502                                            NaN   \n",
       "14503                                            NaN   \n",
       "14504                                            NaN   \n",
       "14505                                            NaN   \n",
       "14506                                            NaN   \n",
       "14507                                            NaN   \n",
       "14508                                            NaN   \n",
       "\n",
       "                                              tweet_text  \n",
       "0           Warning: penny boards will make you a faggot  \n",
       "1                                             Fuck dykes  \n",
       "2      @sizzurp__ @ILIKECATS74 @yoPapi_chulo @brandon...  \n",
       "3      \"@jayswaggkillah: \"@JacklynAnnn: @jayswaggkill...  \n",
       "4      @Zhugstubble You heard me bitch but any way I'...  \n",
       "6      RT @ivanrabago_: @_WhitePonyJr_ looking like f...  \n",
       "7      Well I thought you knew actually RT @KingHorse...  \n",
       "8           @Stonisnipezz I know. It was a joke, faggot.  \n",
       "9      I'm tired of people saying I look like my brot...  \n",
       "10     #VoteBlue2014 Yeah. CUZ 8 million people in fa...  \n",
       "11     @AndreBerto word is you use roids, stupid hypo...  \n",
       "15     They used to tie both ends of a niggers legs t...  \n",
       "16     Good night fags and fagettes (that's the femal...  \n",
       "17     I cant stand no crybaby ass nigga. If you gonn...  \n",
       "18     RT @nerdkiller669: @ShinSnipes no you're a nigger  \n",
       "19     \"I'll fuck you til you love me faggot\" - Mike ...  \n",
       "20     @OxbloodStomper @PalePixie88 @SlaveCatcher88 w...  \n",
       "21     I fucking hate you niggers bruh... http://t.co...  \n",
       "25     Also, Happy Armistice Day, anachronists!\\n\\n&#...  \n",
       "27     RT @BitchPlsComedy: apparently &#8220;bae&#822...  \n",
       "35     RT @kelter1: The fact that Kim Kardashian's ap...  \n",
       "37     @LastProtestants @stomponato @delschilling You...  \n",
       "40     Sitting alone watching White Chicks, no pants,...  \n",
       "44     RT @skythedon: Huge ass, small waist &amp; oka...  \n",
       "48     RT @KayciMalynn: I just can't help but to hate...  \n",
       "49     RT @HBCUfessions: You females overlook us geek...  \n",
       "50         RT @macktology101: \"Sunday funday\" hoe quotes  \n",
       "52     RT @JoeBudden: Young, attractive, successful, ...  \n",
       "53     RT @opticzodiac: @JerKzTheGreaT @Erupts @BajaO...  \n",
       "55     @KristyT @aliciafiasco @luciouskitty \"you know...  \n",
       "...                                                  ...  \n",
       "14479  I can't deal with a slow internet connection. ...  \n",
       "14480  Mental Health Practitioner  - Coon Rapids Day ...  \n",
       "14481  Mental Health Practitioner  - Coon Rapids Day ...  \n",
       "14482  Mental Health Practitioner  - Coon Rapids Day ...  \n",
       "14483  Ben Carson thinks it's okay to have A confeder...  \n",
       "14484  #YodaSpeakASaying \\n\\nKills, captivity does.\\n...  \n",
       "14485  diamond keep actin up ima go to her job and be...  \n",
       "14486  Cries. I'm turning into a weeb-coon https://t....  \n",
       "14487  Saw this gorgeous 4 1/2 year old Maine Coon to...  \n",
       "14488  Jet Stream the Maine Coon riding out last nigh...  \n",
       "14489          @LenaOverFlowers I'm not even a coon girl  \n",
       "14490  From what I've seen men and women coon at the ...  \n",
       "14491  \"BW,take care of me! pls buy my coon ass somet...  \n",
       "14492  Interesting data on quantifying value of the I...  \n",
       "14493                       @aryan_twr60 thank you_ÃÂ«ÃÃ_  \n",
       "14494  @sardesairajdeep @DwaipayanM if he is Aryan th...  \n",
       "14495  @sardesairajdeep sir if he is Aryan then #mall...  \n",
       "14496  Radio Nordfront interview a Russian patriot an...  \n",
       "14497  @nupursanon hope u check ur fb messages, all m...  \n",
       "14498  WowÃ_Ã__A Jewish man having his nose measured ...  \n",
       "14499  oldpicsarchive: A Jewish man having his nose m...  \n",
       "14500  A Jewish man having his nose measured during A...  \n",
       "14501  AO - aryan huy miss you na!! Happy that i'll b...  \n",
       "14502  @AryanTimsina Aryan, what is the version of Fifa?  \n",
       "14503  measured during Aryan race determination tests...  \n",
       "14504  I'm sorry. Did I offend your white supremacist...  \n",
       "14505  @tradethecycles Caucasian euro aryan whatever....  \n",
       "14506  @yadavakhilesh sir a patient named aryan khan ...  \n",
       "14507  @Iamshivachari  Happy birthday bro _ÃÂ«ÃÃ_ \\nHa...  \n",
       "14508  @Hajrahkiayni Aryan Kapoor is such a cute name...  \n",
       "\n",
       "[14476 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['does_this_tweet_contain_hate_speech']!=df['does_this_tweet_contain_hate_speech_gold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14442"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['orig_does_this_tweet_contain_hate_speech'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14442"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['does_this_tweet_contain_hate_speech_gold'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The tweet uses offensive language but not hate speech',\n",
       "       'The tweet contains hate speech', 'The tweet is not offensive'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['does_this_tweet_contain_hate_speech'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['orig_does_this_tweet_contain_hate_speech', 'does_this_tweet_contain_hate_speech_gold',\n",
    "         'does_this_tweet_contain_hate_speechconfidence'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[['does_this_tweet_contain_hate_speech','tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df1['does_this_tweet_contain_hate_speech']=df1['does_this_tweet_contain_hate_speech'].map({'The tweet is not offensive':0,\n",
    "                                                'The tweet uses offensive language but not hate speech':1,\n",
    "                                               'The tweet contains hate speech':2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like both training and test data is sorted by time. Let's shuffle them first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>warning penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>at least i dont look like jefree starr faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>is a fag jackie jealous neeeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>you heard me bitch but any way i m back th tex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                              tweet\n",
       "0         1        warning penny boards will make you a faggot\n",
       "1         2                                         fuck dykes\n",
       "2         2      at least i dont look like jefree starr faggot\n",
       "3         2                     is a fag jackie jealous neeeee\n",
       "4         1  you heard me bitch but any way i m back th tex..."
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns=[\"sentiment\", 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>warning penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>at least i dont look like jefree starr faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>is a fag jackie jealous neeeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>you heard me bitch but any way i m back th tex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                              tweet\n",
       "0         1        warning penny boards will make you a faggot\n",
       "1         2                                         fuck dykes\n",
       "2         2      at least i dont look like jefree starr faggot\n",
       "3         2                     is a fag jackie jealous neeeee\n",
       "4         1  you heard me bitch but any way i m back th tex..."
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tweet\n",
       "sentiment       \n",
       "0           7274\n",
       "1           4836\n",
       "2           2399"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       "  array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       "  array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       " array([2399. , 2886.5, 3374. , 3861.5, 4349. , 4836.5, 5324. , 5811.5,\n",
       "        6299. , 6786.5, 7274. ]),\n",
       " <a list of 3 Lists of Patches objects>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "labels=['2399','4836','7274']\n",
    "#plt.xticks((np.arange(0,3, step=1),labels)\n",
    "plt.hist(df1.groupby('sentiment').count())\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code clean up\n",
    "\n",
    "We will use the following function to cleanup the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df1[\"sentiment\"] = df1[\"sentiment\"].apply(lambda y: str(y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet, remove_stopwords=False):\n",
    "    # Remove HTML tags\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    \n",
    "    # Remove \"@...\" mentions\n",
    "    tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", tweet)\n",
    "\n",
    "    # Remove URLs that start with http, https or just www\n",
    "    tweet = re.sub(\"https?://[^ ]+\",\"\",tweet)\n",
    "    tweet = re.sub(\"www.[^ ]+\",\"\",tweet)\n",
    "\n",
    "    # Decode and replace unicode unrecognisable special characters with u\"\\ufffd\"\n",
    "    try:\n",
    "        tweet = tweet.decode(\"utf-8-sig\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # now replace u\"\\ufffd\" with \"?\"\n",
    "    tweet.replace(u\"\\ufffd\", \"?\")\n",
    "\n",
    "    # TODO: Keep or replace emoticons and exclamations, etc. with text\n",
    "    \n",
    "    # Remove non-letters\n",
    "    tweet = re.sub(\"[^a-zA-Z]\",\" \", tweet)\n",
    "    \n",
    "    # Convert text to lower case and tokenize them\n",
    "    words = WordPunctTokenizer().tokenize(tweet.lower())\n",
    "\n",
    "    # Remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in set(stopwords.words(\"english\"))]\n",
    "\n",
    "    # Join back the list of words\n",
    "    cleaned_tweet = (\" \".join(words)).strip()\n",
    "\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the above function on an example with all the patterns that it removes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ll say i have a dream come and see\n"
     ]
    }
   ],
   "source": [
    "test_example = \"@my_friend http://www.mywebsite.com/index.html?start=1 I\\xef\\xbf\\xbdll\" + \\\n",
    "                \" say &quot;I have a dream&quot; :)\" + \\\n",
    "                \"come and see www.mywebsite.com/my_contact.html\"\n",
    "print(clean_tweet(test_example, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using only the text of the tweet and the sentiment fields, so let's get rid of the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : 2019-01-02 22:12:18\n",
      "End : 2019-01-02 22:12:23\n",
      "This step took 5.315 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"Start : {0}\".format(start.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "# The following code is identical to the above loop, but is more fancy\n",
    "df1[\"tweet\"] = df1[\"tweet\"].apply(lambda x: clean_tweet(x))\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"End : {0}\".format(end.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "print(\"This step took {0:.3f} seconds\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to be perfectly balanced for training, but we have an extra level in test set. For now we will just ignore the level=2 which corresponds to neutral sentiment.\n",
    "\n",
    "Let's rename 2 and 4 as 1 in the above data, so that we have binary target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>warning penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>at least i dont look like jefree starr faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>is a fag jackie jealous neeeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>you heard me bitch but any way i m back th tex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                              tweet\n",
       "0         1        warning penny boards will make you a faggot\n",
       "1         2                                         fuck dykes\n",
       "2         2      at least i dont look like jefree starr faggot\n",
       "3         2                     is a fag jackie jealous neeeee\n",
       "4         1  you heard me bitch but any way i m back th tex..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=df1['tweet']\n",
    "y=df1['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.02, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14218,), (14218,), (291,), (291,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the above function on an example with all the patterns that it removes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the results so that we can use them in the future, or start from here in future iterations to avoid the huge learning time above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tweets = \"hate_tweets.csv\"\n",
    "df1.to_csv(hate_tweets, index=False, quoting=3, \n",
    "                quotechar='', escapechar='\\\\', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn Random Forest Classifier\n",
    "\n",
    "For this model we will use the df_train and df_test data we created and wrote to their respective files above. We can directly use them since they are already in memory, but just for the sake of modularity and completeness, we will start from scratch and read them from the files into pandas dataframes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop the rows with empty tweets, but let's first check the sizes of the dataframes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we dropped a couple of thousand rows from df_train_new. Those were the tweets that became empty strings after the cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14218x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 88521 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bag of words from the cleaned up reviews\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = stopwords.words(\"english\"),   \\\n",
    "                             max_features = 5000)\n",
    "\n",
    "# Fits the model and learn the vocabulary; then transforms the data into feature vectors\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest classifier with 100 trees\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the training right now, but because we are not using spark for this model, it will take some time (approximately 6 minutes). Let's record the start and end time and compute the total run time of this step (feel free to change the CUSTOM_TRAINING_SET_SIZE to a lower number to reduce runtime, e.g. 100000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : 2019-01-02 15:43:56\n",
      "End : 2019-01-02 15:44:16\n",
      "This step took 19.813 seconds\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"Start : {0}\".format(start.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "# Fit the Random Forest classifier to the training set, using the bag of words as\n",
    "# features and the sentiment labels as the response variable\n",
    "rfc.fit(X_train_features, y_train)\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"End : {0}\".format(end.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "print(\"This step took {0:.3f} seconds\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first calculate the accuracy on the exact subset of the training set we used for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score=0.9642706428470952\n"
     ]
    }
   ],
   "source": [
    "X_train_features = vectorizer.transform(X_train)\n",
    "Y_train_pred = rfc.predict(X_train_features)\n",
    "acc_score = accuracy_score(y_train, Y_train_pred)\n",
    "print(\"accuracy_score={}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " on the independent test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score=0.7628865979381443\n"
     ]
    }
   ],
   "source": [
    "# transform and predict the test set\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "Y_test_pred = rfc.predict(X_test_features)\n",
    "acc_score = accuracy_score(y_test, Y_test_pred)\n",
    "print(\"accuracy_score={}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC=0.9346128360469519\n"
     ]
    }
   ],
   "source": [
    "Yhat_rf = rfc.predict_proba(X_test_features)\n",
    "fpr, tpr, threshold = roc_curve(y_test.apply(lambda y:0 if y=='0' else 1), Yhat_rf[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"AUC={}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Learn Logistic Regression with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>warning penny boards will make you a faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>fuck dykes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>at least i dont look like jefree starr faggot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>is a fag jackie jealous neeeee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>you heard me bitch but any way i m back th tex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentiment                                              tweet\n",
       "0         1        warning penny boards will make you a faggot\n",
       "1         2                                         fuck dykes\n",
       "2         2      at least i dont look like jefree starr faggot\n",
       "3         2                     is a fag jackie jealous neeeee\n",
       "4         1  you heard me bitch but any way i m back th tex..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : 2019-01-02 15:44:17\n",
      "End : 2019-01-02 15:44:23\n",
      "This step took 5.857 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"Start : {0}\".format(start.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "# The following code is identical to the above loop, but is more fancy\n",
    "df1[\"tweet\"] = df1[\"tweet\"].apply(lambda x: clean_tweet(x))\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"End : {0}\".format(end.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "print(\"This step took {0:.3f} seconds\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=df1['tweet']\n",
    "y=df1['sentiment'].astype(int)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE_NUMBERS = True\n",
    "if USE_NUMBERS:\n",
    "    Y_train = Y_train.apply(lambda y: 0 if y==\"0\" else 1)\n",
    "    Y_valid = Y_valid.apply(lambda y: 0 if y==\"0\" else 1)\n",
    "    Y_test = Y_test.apply(lambda y: 0 if y==\"0\" else 1)\n",
    "else:\n",
    "    Y_train = Y_train.apply(lambda y: \"Neg\" if y==\"0\" else \"Pos\")\n",
    "    Y_valid = Y_valid.apply(lambda y: \"Neg\" if y==\"0\" else \"Pos\")\n",
    "    Y_test = Y_test.apply(lambda y: \"Neg\" if y==\"0\" else \"Pos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2902,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the input for the Logistic Regression model, create and fit a tf-idf vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='replace',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.6, max_features=100000, min_df=5,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvec = TfidfVectorizer(max_features=NUM_WORDS,ngram_range=(1, 3), decode_error='replace', max_df=0.6, min_df=5)\n",
    "tvec.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform the training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tvec.transform(X_train)\n",
    "X_test_tfidf = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now fit a Logistic Regression model to the tf-idf encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_with_tfidf = LogisticRegression()\n",
    "lr_with_tfidf.fit(X_train_tfidf,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is this models accuracy score on the validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the accuracy score of the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7715368711233632"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_with_tfidf.score(X_test_tfidf,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the accuracy of the Logistic Regression model was much better than the Random Forest Model and it also took a franction of the time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the prediction probabilities for the test set (which we will need while creating the ROC curve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.98      0.91      1459\n",
      "          1       0.70      0.68      0.69       953\n",
      "          2       0.55      0.32      0.41       490\n",
      "\n",
      "avg / total       0.75      0.77      0.75      2902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_preds=lr_with_tfidf.predict(X_test_tfidf)\n",
    "report = classification_report( Y_test, y_preds )\n",
    "print(report)\n",
    "#It turns out that Logistic Regression model does better to predict offensive language that hate speech. It is relevant\n",
    "#with our data which consist more offensive tweets and ofefnsive tweets are wrongly labeled as 'hate speech' by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_lr = lr_with_tfidf.predict_proba(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data for modeling on Spark\n",
    "\n",
    "Let's load the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(hate_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('index', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(sentiment=1, tweet='warning penny boards will make you a faggot', index=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark_df_train, spark_df_valid, spark_df_test) = spark_df.randomSplit([0.80, 0.05, 0.05], seed = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12938, 785, 786)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_train.count(), spark_df_valid.count(), spark_df_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with tf-idf on Spark (model #1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Note</B>: The following models are from the following website. You may find more detailed models for Twitter Sentiment Analysis there:\n",
    "https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pyspark's CountVectorizer to be used in the following examples\n",
    "# The import is here because we used similar modules from scikit-learn above\n",
    "from pyspark.ml.feature import CountVectorizer, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : 2019-01-02 17:04:35\n",
      "End : 2019-01-02 17:04:49\n",
      "This step took 14.023 seconds\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"Start : {0}\".format(start.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"tweet\", outputCol=\"words\")\n",
    "hashtf = HashingTF(numFeatures=100000, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "label_stringIdx = StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx])\n",
    "\n",
    "# tf-idf and conversions\n",
    "pipelineFit = pipeline.fit(spark_df_train)\n",
    "spark_df_train_transformed = pipelineFit.transform(spark_df_train)\n",
    "spark_df_valid_transformed = pipelineFit.transform(spark_df_valid)\n",
    "spark_df_test_transformed = pipelineFit.transform(spark_df_test)\n",
    "\n",
    "# modeling\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(spark_df_train_transformed)\n",
    "validation_predictions = lrModel.transform(spark_df_valid_transformed)\n",
    "test_predictions = lrModel.transform(spark_df_test_transformed)\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"End : {0}\".format(end.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "print(\"This step took {0:.3f} seconds\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7031847133757961"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_matches = validation_predictions.filter(validation_predictions.label == validation_predictions.prediction).count()\n",
    "validation_set_accuracy =  number_of_matches / float(validation_predictions.count())\n",
    "validation_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446416426466334"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC (Area Under the Curve metric)\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7048346055979644"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_matches = test_predictions.filter(test_predictions.label == test_predictions.prediction).count()\n",
    "test_set_accuracy =  number_of_matches / float(test_predictions.count())\n",
    "test_set_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result is on par with our previous Random Forest model, but a bit worse than the similar Logistic Regression model using tf-idf. The impressive part was that the whole run was done in less than 2 minutes due to spark leveraging parallelism using multiple cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with N-gram on Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will extract around 6000 features from unigrams, bigrams and trigrams; hence 18000 features in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 6000\n",
    "\n",
    "def build_ngrams(inputCol=[\"tweet\",\"sentiment\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"tweet\", outputCol=\"words\")]\n",
    "\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=VOCAB_SIZE,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    \n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=2) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"features\"\n",
    "    )] #Spark does not automatically combine features from different n-grams, so I had to use VectorAssembler in the pipeline, \n",
    "       #to combine the features I get from each n-gram.\n",
    "    \n",
    "    label_stringIdx = [StringIndexer(inputCol = \"sentiment\", outputCol = \"label\")]\n",
    "    \n",
    "    lr = [LogisticRegression(maxIter=100)]\n",
    "    \n",
    "    return Pipeline(stages = tokenizer + ngrams + cv + idf + assembler + label_stringIdx + lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : 2019-01-02 21:21:30\n",
      "End : 2019-01-02 21:22:05\n",
      "This step took 35.151 seconds\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"Start : {0}\".format(start.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "ngram_pipelineFit = build_ngrams().fit(spark_df_train)\n",
    "\n",
    "validation_predictions = ngram_pipelineFit.transform(spark_df_valid)\n",
    "test_predictions = ngram_pipelineFit.transform(spark_df_test)\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"End : {0}\".format(end.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "print(\"This step took {0:.3f} seconds\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7235668789808917"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_matches = validation_predictions.filter(validation_predictions.label == validation_predictions.prediction).count()\n",
    "validation_set_accuracy =  number_of_matches / float(validation_predictions.count())\n",
    "validation_set_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8648119959840666"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC (Area Under the Curve metric)\n",
    "evaluator.evaluate(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7404580152671756"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_matches = test_predictions.filter(test_predictions.label == test_predictions.prediction).count()\n",
    "test_set_accuracy =  number_of_matches / float(test_predictions.count())\n",
    "test_set_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w Feature Engineering, Credits for Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "stopwords=stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "stopwords.extend(other_exclusions)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    #parsed_text = parsed_text.code(\"utf-8\", errors='ignore')\n",
    "    return parsed_text\n",
    "\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    #tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #Convert to lower case, split into individual words:\n",
    "    tweet= word_tokenize(tweet.lower())\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet]\n",
    "    return tokens\n",
    "\n",
    "def basic_tokenize(tweet):\n",
    "    \"\"\"Same as tokenize but without the stemming\"\"\"\n",
    "    #tweet=word_tokenize(tweet.lower())\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z.,!?]*\", tweet.lower())).strip()\n",
    "    return tweet.split()\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=tokenize,\n",
    "    preprocessor=preprocess,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=stopwords, #We do better when we keep stopwords\n",
    "    use_idf=True,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=10000,\n",
    "    min_df=5,\n",
    "    max_df=0.501\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=df1.tweet\n",
    "#Construct tfidf matrix and get relevant scores\n",
    "tfidf = vectorizer.fit_transform(tweets).toarray()\n",
    "vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\n",
    "idf_vals = vectorizer.idf_\n",
    "idf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('aap', 0), ('abandon', 1), ('abil', 2), ('abl', 3), ('abort', 4), ('abov', 5), ('absolut', 6), ('abt', 7), ('abus', 8), ('acc', 9), ('accent', 10), ('accept', 11), ('accid', 12), ('accomplish', 13), ('accord', 14), ('account', 15), ('account tri', 16), ('accur', 17), ('accus', 18), ('achiev', 19), ('across', 20), ('across field', 21), ('across field bag', 22), ('act', 23), ('act like', 24), ('act like faggot', 25), ('act like fuck', 26), ('action', 27), ('activ', 28), ('actor', 29), ('actual', 30), ('ad', 31), ('ad video', 32), ('ad video playlist', 33), ('add', 34), ('addict', 35), ('address', 36), ('adel', 37), ('admit', 38), ('adopt', 39), ('ador', 40), ('adult', 41), ('advantag', 42), ('advic', 43), ('af', 44), ('afford', 45), ('afraid', 46), ('africa', 47), ('african', 48), ('age', 49), ('agenc', 50), ('ago', 51), ('agre', 52), ('ah', 53), ('ahead', 54), ('ahh', 55), ('aid', 56), ('aim', 57), ('aint', 58), ('aint makin', 59), ('aint makin nois', 60), ('air', 61), ('air horn', 62), ('air horn bo', 63), ('airport', 64), ('ak', 65), ('ak stop', 66), ('ak stop major', 67), ('aka', 68), ('akbar', 69), ('akbar ran', 70), ('al', 71), ('alan', 72), ('alarm', 73), ('album', 74), ('alcohol', 75), ('alex', 76), ('alexfromtarget', 77), ('alexfromtarget vote', 78), ('alexfromtarget vote sosonellen', 79), ('alik', 80), ('aliv', 81), ('allah', 82), ('allah akbar', 83), ('allah doe', 84), ('allahu', 85), ('allahu akbar', 86), ('allahu akbar ran', 87), ('allegedli', 88), ('alley', 89), ('allow', 90), ('almost', 91), ('alon', 92), ('along', 93), ('alreadi', 94), ('alreadi know', 95), ('alright', 96), ('also', 97), ('altern', 98), ('although', 99), ('alway', 100), ('alway blame', 101), ('alway crack', 102), ('alway fuck', 103), ('alway fuck everyth', 104), ('alway gon', 105), ('alway gon na', 106), ('alway kindli', 107), ('alway kindli follow', 108), ('alway mind', 109), ('alway mind follow', 110), ('alway right', 111), ('alway riski', 112), ('alway riski ea', 113), ('amateur', 114), ('amaz', 115), ('amaz pleas', 116), ('amaz pleas subscrib', 117), ('amazon', 118), ('america', 119), ('america beach', 120), ('america beach ilovethebeach', 121), ('american', 122), ('amiabl', 123), ('amiabl day', 124), ('amiabl day christma', 125), ('amid', 126), ('amid econom', 127), ('amid econom recoveri', 128), ('amount', 129), ('anal', 130), ('android', 131), ('android tech', 132), ('android tech gift', 133), ('angel', 134), ('anger', 135), ('angri', 136), ('ani', 137), ('ani form', 138), ('ani form alway', 139), ('ani nigga', 140), ('ani right', 141), ('ani way', 142), ('anim', 143), ('announc', 144), ('annoy', 145), ('anoth', 146), ('anoth nigga', 147), ('anoth one', 148), ('answer', 149), ('anti', 150), ('anxieti', 151), ('anybodi', 152), ('anymor', 153), ('anyon', 154), ('anyon els', 155), ('anyth', 156), ('anytim', 157), ('anyway', 158), ('anywher', 159), ('apart', 160), ('ape', 161), ('apolog', 162), ('apologis', 163), ('app', 164), ('appar', 165), ('appear', 166), ('appl', 167), ('appl iphon', 168), ('appl iphon plu', 169), ('appli', 170), ('appli job', 171), ('appreci', 172), ('approv', 173), ('approv demolit', 174), ('approv demolit two', 175), ('arab', 176), ('area', 177), ('arent', 178), ('argu', 179), ('argument', 180), ('arm', 181), ('armi', 182), ('armor', 183), ('armour', 184), ('armour info', 185), ('armour info gather', 186), ('around', 187), ('around say', 188), ('around tell', 189), ('around tv', 190), ('around tv year', 191), ('arrest', 192), ('arriv', 193), ('ars', 194), ('arsen', 195), ('art', 196), ('articl', 197), ('artist', 198), ('aryan', 199), ('asf', 200), ('ash', 201), ('ashi', 202), ('ashton', 203), ('asian', 204), ('ask', 205), ('ask could', 206), ('ask one', 207), ('ask one time', 208), ('ask question', 209), ('ask wa', 210), ('ask want', 211), ('ask whi', 212), ('askpcatt', 213), ('askpcatt mani', 214), ('askpcatt mani nigger', 215), ('ass', 216), ('ass back', 217), ('ass bitch', 218), ('ass bitch made', 219), ('ass faggot', 220), ('ass friend', 221), ('ass fuck', 222), ('ass like', 223), ('ass nigga', 224), ('ass nigger', 225), ('ass pussi', 226), ('ass shit', 227), ('ass somewher', 228), ('ass wa', 229), ('ass white', 230), ('ass white boy', 231), ('ass white girl', 232), ('ass white guy', 233), ('ass white peopl', 234), ('assassin', 235), ('assembl', 236), ('asshol', 237), ('associ', 238), ('assum', 239), ('ate', 240), ('atheist', 241), ('atleast', 242), ('attack', 243), ('attack left', 244), ('attack left say', 245), ('attempt', 246), ('attent', 247), ('attitud', 248), ('attract', 249), ('auspol', 250), ('australia', 251), ('authent', 252), ('auto', 253), ('automat', 254), ('avail', 255), ('avi', 256), ('avoid', 257), ('aw', 258), ('awak', 259), ('awaken', 260), ('award', 261), ('away', 262), ('awe', 263), ('awesom', 264), ('awkward', 265), ('aye', 266), ('b', 267), ('babe', 268), ('babi', 269), ('back', 270), ('back via', 271), ('background', 272), ('bad', 273), ('bad guy', 274), ('bad guy mouth', 275), ('badli', 276), ('bae', 277), ('bag', 278), ('bag eye', 279), ('bag head', 280), ('bag head thi', 281), ('bag head yogi', 282), ('bake', 283), ('balanc', 284), ('ball', 285), ('ban', 286), ('band', 287), ('bang', 288), ('bank', 289), ('bar', 290), ('bar full', 291), ('bar full white', 292), ('barbar', 293), ('bare', 294), ('base', 295), ('basebal', 296), ('bash', 297), ('basic', 298), ('basketbal', 299), ('bass', 300), ('bass btsm', 301), ('bass btsm x', 302), ('bastard', 303), ('bat', 304), ('bath', 305), ('bathroom', 306), ('battl', 307), ('bay', 308), ('bb', 309), ('bbi', 310), ('bc', 311), ('bday', 312), ('beach', 313), ('beach central', 314), ('beach central america', 315), ('beach ilovethebeach', 316), ('beach ilovethebeach centralamerica', 317), ('bead', 318), ('bear', 319), ('beat', 320), ('beauti', 321), ('becam', 322), ('becaus', 323), ('becaus faggot', 324), ('becaus fuck', 325), ('becaus like', 326), ('becaus wa', 327), ('becaus white', 328), ('beckham', 329), ('becom', 330), ('bed', 331), ('bee', 332), ('beef', 333), ('beef nigga', 334), ('beer', 335), ('befor', 336), ('befor christma', 337), ('befor go', 338), ('befor wa', 339), ('beg', 340), ('begin', 341), ('begun', 342), ('begun whip', 343), ('behavior', 344), ('behavior could', 345), ('behavior could secret', 346), ('behind', 347), ('bein', 348), ('believ', 349), ('bell', 350), ('belong', 351), ('belt', 352), ('ben', 353), ('bend', 354), ('bend blvd', 355), ('benefit', 356), ('berni', 357), ('berra', 358), ('berra yogiberrarip', 359), ('besid', 360), ('best', 361), ('best car', 362), ('best car safeti', 363), ('best friend', 364), ('best gift', 365), ('best gift alway', 366), ('bestfriend', 367), ('bet', 368), ('better', 369), ('better fag', 370), ('beyond', 371), ('bf', 372), ('bi', 373), ('bibl', 374), ('bid', 375), ('bieber', 376), ('big', 377), ('big bend', 378), ('big bend blvd', 379), ('big boy', 380), ('big deal', 381), ('bigger', 382), ('biggest', 383), ('biggest faggot', 384), ('biggest faggot ever', 385), ('biggest faggot know', 386), ('biggest faggot twitter', 387), ('bigot', 388), ('bilawalinfantkil', 389), ('bill', 390), ('bin', 391), ('bio', 392), ('bird', 393), ('birth', 394), ('birth control', 395), ('birthday', 396), ('birthday biggest', 397), ('birthday biggest faggot', 398), ('bit', 399), ('bitch', 400), ('bitch ass', 401), ('bitch fuck', 402), ('bitch gucci', 403), ('bitch gucci flip', 404), ('bitch like', 405), ('bitch lol', 406), ('bitch made', 407), ('bitch made ass', 408), ('bitch made nigga', 409), ('bitch make', 410), ('bitch nigga', 411), ('bitch nigga fuck', 412), ('bitch nigga hi', 413), ('bitch shoot', 414), ('bitch shoot nigga', 415), ('bitch whip', 416), ('bitch whip slave', 417), ('bitter', 418), ('bjp', 419), ('black', 420), ('black dyke', 421), ('black male', 422), ('black op', 423), ('black op funni', 424), ('black peopl', 425), ('black person', 426), ('black woman', 427), ('black women', 428), ('blame', 429), ('blame america', 430), ('blast', 431), ('blast chink', 432), ('blast chink remain', 433), ('bless', 434), ('blind', 435), ('block', 436), ('block becaus', 437), ('block faggot', 438), ('blog', 439), ('blond', 440), ('blood', 441), ('bloodi', 442), ('blow', 443), ('blue', 444), ('blue collar', 445), ('blue collar could', 446), ('blvd', 447), ('bo', 448), ('board', 449), ('bob', 450), ('bodi', 451), ('boi', 452), ('bomb', 453), ('boo', 454), ('boob', 455), ('book', 456), ('boot', 457), ('booti', 458), ('bore', 459), ('born', 460), ('borrow', 461), ('boss', 462), ('bother', 463), ('bottl', 464), ('bottom', 465), ('bottom remix', 466), ('bought', 467), ('bounc', 468), ('bout', 469), ('bout get', 470), ('bout go', 471), ('bout thi', 472), ('bout u', 473), ('bowl', 474), ('box', 475), ('box day', 476), ('boy', 477), ('boyfriend', 478), ('brain', 479), ('brand', 480), ('brand new', 481), ('brave', 482), ('bread', 483), ('break', 484), ('breakfast', 485), ('breath', 486), ('brick', 487), ('bridg', 488), ('bright', 489), ('brilliant', 490), ('bring', 491), ('british', 492), ('bro', 493), ('broad', 494), ('broke', 495), ('broke af', 496), ('broken', 497), ('brother', 498), ('brother got', 499), ('brother got pregnant', 500), ('brought', 501), ('brown', 502), ('bruce', 503), ('bruh', 504), ('brutal', 505), ('bs', 506), ('bt', 507), ('btsm', 508), ('btsm x', 509), ('btsm x lektriqu', 510), ('btw', 511), ('bu', 512), ('bu driver', 513), ('bu driver school', 514), ('bu driver shortag', 515), ('bu driver unemploy', 516), ('buck', 517), ('buddi', 518), ('bug', 519), ('build', 520), ('built', 521), ('bull', 522), ('bull dyke', 523), ('bulli', 524), ('bullshit', 525), ('bum', 526), ('bump', 527), ('bun', 528), ('bunch', 529), ('bunch fag', 530), ('bunni', 531), ('burger', 532), ('buri', 533), ('burn', 534), ('bush', 535), ('busi', 536), ('bustin', 537), ('bustin hoe', 538), ('butt', 539), ('butt ugli', 540), ('butter', 541), ('button', 542), ('buy', 543), ('bye', 544), ('c', 545), ('cab', 546), ('cake', 547), ('california', 548), ('call', 549), ('call bitch', 550), ('call bitch made', 551), ('call black', 552), ('call dumb', 553), ('call dumb nigger', 554), ('call fag', 555), ('call faggot', 556), ('call faggot say', 557), ('call fuck', 558), ('call gay', 559), ('call nigga', 560), ('call nigga got', 561), ('call nigger', 562), ('call obj', 563), ('call odel', 564), ('call odel faggot', 565), ('call porch', 566), ('call porch monkey', 567), ('call redneck', 568), ('call redneck white', 569), ('call sand', 570), ('call sand nigger', 571), ('call sarah', 572), ('call sarah palin', 573), ('call u', 574), ('call us', 575), ('call white', 576), ('callin', 577), ('calm', 578), ('cam', 579), ('came', 580), ('camel', 581), ('camera', 582), ('cameron', 583), ('camp', 584), ('campaign', 585), ('canadian', 586), ('cancer', 587), ('candi', 588), ('candid', 589), ('cant', 590), ('cant even', 591), ('cant stand', 592), ('cap', 593), ('captur', 594), ('car', 595), ('car safeti', 596), ('car safeti devic', 597), ('card', 598), ('cardin', 599), ('care', 600), ('career', 601), ('carey', 602), ('carol', 603), ('carri', 604), ('carti', 605), ('cartoon', 606), ('case', 607), ('cash', 608), ('casino', 609), ('cast', 610), ('castl', 611), ('cat', 612), ('catch', 613), ('catch hand', 614), ('catch terrorist', 615), ('catch terrorist pol', 616), ('caught', 617), ('caus', 618), ('caus faggot', 619), ('caus got', 620), ('celebr', 621), ('cell', 622), ('center', 623), ('central', 624), ('central america', 625), ('central america beach', 626), ('centralamerica', 627), ('centuri', 628), ('certain', 629), ('chain', 630), ('challeng', 631), ('champ', 632), ('champion', 633), ('chanc', 634), ('chang', 635), ('chang coupl', 636), ('chang coupl million', 637), ('chang life', 638), ('chang mind', 639), ('channel', 640), ('charact', 641), ('charg', 642), ('charli', 643), ('chase', 644), ('chat', 645), ('cheap', 646), ('cheat', 647), ('check', 648), ('check new', 649), ('check thi', 650), ('check top', 651), ('check top beach', 652), ('check ultim', 653), ('check ultim holiday', 654), ('cheer', 655), ('chick', 656), ('chicken', 657), ('child', 658), ('child porn', 659), ('child porn law', 660), ('children', 661), ('chill', 662), ('china', 663), ('chines', 664), ('chink', 665), ('chink armor', 666), ('chink armour', 667), ('chink eye', 668), ('chink hi', 669), ('chink remain', 670), ('chink remain karnataka', 671), ('chip', 672), ('chipotl', 673), ('chocol', 674), ('choic', 675), ('choke', 676), ('choos', 677), ('chri', 678), ('chri brown', 679), ('christ', 680), ('christian', 681), ('christma', 682), ('christma carol', 683), ('christma day', 684), ('christma day follow', 685), ('christma dinner', 686), ('christma eve', 687), ('christma eve eve', 688), ('christma gift', 689), ('christma happi', 690), ('christma may', 691), ('christma may pleas', 692), ('christma movi', 693), ('christma present', 694), ('christma tree', 695), ('christma xma', 696), ('christmasweek', 697), ('chuck', 698), ('chucker', 699), ('church', 700), ('church street', 701), ('church street blast', 702), ('cigarett', 703), ('circl', 704), ('citi', 705), ('citi soon', 706), ('citi soon tag', 707), ('citizen', 708), ('civil', 709), ('claim', 710), ('class', 711), ('classic', 712), ('clau', 713), ('clean', 714), ('clear', 715), ('clearli', 716), ('cleveland', 717), ('click', 718), ('click stereotyp', 719), ('click stereotyp come', 720), ('clinton', 721), ('clip', 722), ('close', 723), ('closer', 724), ('closet', 725), ('closur', 726), ('closur need', 727), ('closur need outrag', 728), ('cloth', 729), ('cloud', 730), ('clown', 731), ('club', 732), ('co', 733), ('coach', 734), ('cock', 735), ('cod', 736), ('code', 737), ('coffe', 738), ('cold', 739), ('collar', 740), ('collar could', 741), ('collar could chang', 742), ('collect', 743), ('colleg', 744), ('colombia', 745), ('color', 746), ('colour', 747), ('columbia', 748), ('com', 749), ('combin', 750), ('come', 751), ('come back', 752), ('come closet', 753), ('come get', 754), ('come home', 755), ('come justwaitonit', 756), ('come like', 757), ('comeback', 758), ('comfort', 759), ('comin', 760), ('comment', 761), ('comment punch', 762), ('comment punch kick', 763), ('commerci', 764), ('commit', 765), ('common', 766), ('commun', 767), ('compani', 768), ('compar', 769), ('competit', 770), ('complain', 771), ('complet', 772), ('complex', 773), ('con', 774), ('concept', 775), ('concern', 776), ('concert', 777), ('condemn', 778), ('confeder', 779), ('confirm', 780), ('confus', 781), ('congrat', 782), ('congratul', 783), ('congression', 784), ('connect', 785), ('conserv', 786), ('consid', 787), ('consol', 788), ('constantli', 789), ('constitut', 790), ('content', 791), ('context', 792), ('continu', 793), ('control', 794), ('control system', 795), ('convers', 796), ('convict', 797), ('convinc', 798), ('cook', 799), ('cooki', 800), ('cool', 801), ('coon', 802), ('coon ass', 803), ('coon rapid', 804), ('coon shit', 805), ('cop', 806), ('cop dmz', 807), ('copi', 808), ('corner', 809), ('corni', 810), ('corps', 811), ('correct', 812), ('corrupt', 813), ('cost', 814), ('cotton', 815), ('couch', 816), ('could', 817), ('could chang', 818), ('could chang coupl', 819), ('could get', 820), ('could secret', 821), ('could secret likabl', 822), ('could see', 823), ('count', 824), ('counti', 825), ('countri', 826), ('coupl', 827), ('coupl million', 828), ('coupl million dollar', 829), ('cours', 830), ('court', 831), ('court approv', 832), ('court approv demolit', 833), ('court doctor', 834), ('court doctor right', 835), ('cousin', 836), ('cover', 837), ('cover case', 838), ('cover ear', 839), ('cover mouth', 840), ('cow', 841), ('coward', 842), ('coz', 843), ('cpl', 844), ('cpl holder', 845), ('cpl holder shoot', 846), ('crack', 847), ('crack everi', 848), ('crack everi time', 849), ('cracker', 850), ('crap', 851), ('crash', 852), ('crawl', 853), ('crazi', 854), ('cream', 855), ('creat', 856), ('creatur', 857), ('credit', 858), ('creep', 859), ('creepi', 860), ('crew', 861), ('cri', 862), ('crime', 863), ('crimin', 864), ('crimin reagan', 865), ('crimin reagan made', 866), ('crisi', 867), ('critic', 868), ('cross', 869), ('crowd', 870), ('crown', 871), ('crude', 872), ('cruel', 873), ('crush', 874), ('cruz', 875), ('ct', 876), ('cuffin', 877), ('cuffin bitch', 878), ('cuffin hoe', 879), ('cuffin hoe bustin', 880), ('cuffin hoe u', 881), ('cultur', 882), ('cum', 883), ('cummi', 884), ('cummi four', 885), ('cummi four mani', 886), ('cummi three', 887), ('cummi three cummi', 888), ('cummi two', 889), ('cummi two cummi', 890), ('cunt', 891), ('cunt better', 892), ('cunt better fag', 893), ('cup', 894), ('cure', 895), ('curiou', 896), ('current', 897), ('curri', 898), ('curv', 899), ('cuss', 900), ('custom', 901), ('cut', 902), ('cute', 903), ('cuz', 904), ('da', 905), ('dad', 906), ('daddi', 907), ('daili', 908), ('dam', 909), ('damag', 910), ('damn', 911), ('dan', 912), ('danc', 913), ('danger', 914), ('dare', 915), ('dark', 916), ('dat', 917), ('dat nigga', 918), ('dat nigga hi', 919), ('data', 920), ('date', 921), ('daughter', 922), ('dave', 923), ('david', 924), ('dawg', 925), ('day', 926), ('day befor', 927), ('day christma', 928), ('day christma may', 929), ('day congression', 930), ('day follow', 931), ('day follow would', 932), ('day full', 933), ('day like', 934), ('day look', 935), ('day pal', 936), ('day pal x', 937), ('day stat', 938), ('day stat use', 939), ('day work', 940), ('dc', 941), ('de', 942), ('dead', 943), ('deal', 944), ('dear', 945), ('death', 946), ('debat', 947), ('dec', 948), ('decemb', 949), ('decent', 950), ('decentr', 951), ('decentr power', 952), ('decentr power zen', 953), ('decid', 954), ('decis', 955), ('deck', 956), ('declar', 957), ('decor', 958), ('deep', 959), ('def', 960), ('defeat', 961), ('defend', 962), ('defens', 963), ('defin', 964), ('definit', 965), ('definit white', 966), ('definit white trash', 967), ('degre', 968), ('delay', 969), ('delet', 970), ('delet thi', 971), ('deliv', 972), ('deliveri', 973), ('delusion', 974), ('dem', 975), ('demand', 976), ('democrat', 977), ('demolit', 978), ('demolit two', 979), ('demolit two terrorist', 980), ('demon', 981), ('den', 982), ('deni', 983), ('denver', 984), ('depart', 985), ('depend', 986), ('depress', 987), ('der', 988), ('descript', 989), ('deserv', 990), ('deserv kill', 991), ('deserv love', 992), ('deserv love happi', 993), ('design', 994), ('desk', 995), ('desper', 996), ('desper bu', 997), ('desper bu driver', 998), ('destroy', 999), ('detail', 1000), ('determin', 1001), ('detroit', 1002), ('develop', 1003), ('devic', 1004), ('devic rear', 1005), ('devic rear view', 1006), ('devil', 1007), ('di', 1008), ('di song', 1009), ('di song wan', 1010), ('diamond', 1011), ('diamond engag', 1012), ('dick', 1013), ('dick ha', 1014), ('dick ha flop', 1015), ('dick like', 1016), ('dick van', 1017), ('dick van dyke', 1018), ('didnt', 1019), ('die', 1020), ('die fag', 1021), ('die hard', 1022), ('differ', 1023), ('difficult', 1024), ('dinner', 1025), ('direct', 1026), ('director', 1027), ('dirti', 1028), ('dirti bitch', 1029), ('dirti fuck', 1030), ('dirti girl', 1031), ('dirti look', 1032), ('dirti mind', 1033), ('disagre', 1034), ('disappoint', 1035), ('discuss', 1036), ('diseas', 1037), ('disgrac', 1038), ('disgust', 1039), ('dish', 1040), ('dislik', 1041), ('disrespect', 1042), ('district', 1043), ('district desper', 1044), ('district desper bu', 1045), ('disturb', 1046), ('divorc', 1047), ('dj', 1048), ('dm', 1049), ('dmz', 1050), ('dnt', 1051), ('doctor', 1052), ('doctor right', 1053), ('doctor right interrog', 1054), ('doe', 1055), ('doe anyon', 1056), ('doe look', 1057), ('doe make', 1058), ('doe thi', 1059), ('doesnt', 1060), ('dog', 1061), ('doin', 1062), ('doll', 1063), ('dollar', 1064), ('domin', 1065), ('donald', 1066), ('donald trump', 1067), ('donat', 1068), ('done', 1069), ('dont', 1070), ('dont even', 1071), ('dont fuck', 1072), ('dont get', 1073), ('dont know', 1074), ('dont want', 1075), ('door', 1076), ('dope', 1077), ('doubl', 1078), ('doubt', 1079), ('download', 1080), ('dr', 1081), ('draft', 1082), ('drag', 1083), ('drake', 1084), ('drama', 1085), ('draw', 1086), ('dread', 1087), ('dream', 1088), ('dress', 1089), ('dress like', 1090), ('dri', 1091), ('drink', 1092), ('drive', 1093), ('driver', 1094), ('driver school', 1095), ('driver school bu', 1096), ('driver shortag', 1097), ('driver unemploy', 1098), ('driver unemploy high', 1099), ('drop', 1100), ('drug', 1101), ('drum', 1102), ('drum bass', 1103), ('drum bass btsm', 1104), ('drunk', 1105), ('duck', 1106), ('dude', 1107), ('dude faggot', 1108), ('dude hate', 1109), ('dude hate faggot', 1110), ('due', 1111), ('dumb', 1112), ('dumb ass', 1113), ('dumb fuck', 1114), ('dumb monkey', 1115), ('dumb nigger', 1116), ('dumbass', 1117), ('dummi', 1118), ('dunno', 1119), ('dure', 1120), ('dure holiday', 1121), ('dye', 1122), ('dyke', 1123), ('dyke bitch', 1124), ('dyke lesbian', 1125), ('dyke lesbian want', 1126), ('e', 1127), ('ea', 1128), ('eagl', 1129), ('ear', 1130), ('earli', 1131), ('earlier', 1132), ('earn', 1133), ('earth', 1134), ('easi', 1135), ('easili', 1136), ('easter', 1137), ('easter bunni', 1138), ('eat', 1139), ('econom', 1140), ('econom recoveri', 1141), ('econom recoveri school', 1142), ('ed', 1143), ('edg', 1144), ('edit', 1145), ('educ', 1146), ('effect', 1147), ('effort', 1148), ('egg', 1149), ('egg white', 1150), ('ego', 1151), ('egypt', 1152), ('eh', 1153), ('either', 1154), ('elect', 1155), ('electr', 1156), ('elf', 1157), ('eli', 1158), ('els', 1159), ('em', 1160), ('email', 1161), ('embarrass', 1162), ('embrac', 1163), ('emerg', 1164), ('emo', 1165), ('emoji', 1166), ('emot', 1167), ('empir', 1168), ('employ', 1169), ('employe', 1170), ('empti', 1171), ('end', 1172), ('end career', 1173), ('end like', 1174), ('enemi', 1175), ('energi', 1176), ('enforc', 1177), ('engag', 1178), ('engin', 1179), ('english', 1180), ('enjoy', 1181), ('enough', 1182), ('enter', 1183), ('entertain', 1184), ('entir', 1185), ('entrepreneur', 1186), ('entri', 1187), ('ep', 1188), ('episod', 1189), ('epitom', 1190), ('equal', 1191), ('equal opportun', 1192), ('equal opportun employ', 1193), ('er', 1194), ('eras', 1195), ('eric', 1196), ('escap', 1197), ('especi', 1198), ('estat', 1199), ('etc', 1200), ('europ', 1201), ('eve', 1202), ('eve eve', 1203), ('even', 1204), ('even fuck', 1205), ('even know', 1206), ('even though', 1207), ('eventu', 1208), ('ever', 1209), ('ever feel', 1210), ('ever feel like', 1211), ('ever met', 1212), ('ever seen', 1213), ('everi', 1214), ('everi day', 1215), ('everi night', 1216), ('everi right', 1217), ('everi singl', 1218), ('everi smile', 1219), ('everi tear', 1220), ('everi time', 1221), ('everi time see', 1222), ('everi white', 1223), ('everi year', 1224), ('everybodi', 1225), ('everyday', 1226), ('everyon', 1227), ('everyon deserv', 1228), ('everyon ha', 1229), ('everyon know', 1230), ('everyon want', 1231), ('everyon want like', 1232), ('everyth', 1233), ('everyth deserv', 1234), ('everyth deserv love', 1235), ('everytim', 1236), ('everywher', 1237), ('evid', 1238), ('evil', 1239), ('ew', 1240), ('ex', 1241), ('exactli', 1242), ('exam', 1243), ('exampl', 1244), ('exampl life', 1245), ('exampl life love', 1246), ('excel', 1247), ('except', 1248), ('excit', 1249), ('exclus', 1250), ('excus', 1251), ('execut', 1252), ('exist', 1253), ('expect', 1254), ('expens', 1255), ('experi', 1256), ('expert', 1257), ('explain', 1258), ('expos', 1259), ('express', 1260), ('exquisit', 1261), ('exquisit stay', 1262), ('exquisit stay well', 1263), ('extra', 1264), ('extrem', 1265), ('extremist', 1266), ('eye', 1267), ('eye bad', 1268), ('eye bad guy', 1269), ('eyebrow', 1270), ('f', 1271), ('fab', 1272), ('fabul', 1273), ('face', 1274), ('facebook', 1275), ('fact', 1276), ('fade', 1277), ('fag', 1278), ('fag fag', 1279), ('fag fuck', 1280), ('fag get', 1281), ('fag got', 1282), ('fag like', 1283), ('fag lol', 1284), ('fag love', 1285), ('fag one', 1286), ('fag say', 1287), ('fag shit', 1288), ('fag thi', 1289), ('fag u', 1290), ('fag want', 1291), ('faggot', 1292), ('faggot ass', 1293), ('faggot ass nigga', 1294), ('faggot ass somewher', 1295), ('faggot bitch', 1296), ('faggot block', 1297), ('faggot come', 1298), ('faggot ever', 1299), ('faggot ever seen', 1300), ('faggot friend', 1301), ('faggot fuck', 1302), ('faggot get', 1303), ('faggot go', 1304), ('faggot know', 1305), ('faggot life', 1306), ('faggot like', 1307), ('faggot lol', 1308), ('faggot look', 1309), ('faggot love', 1310), ('faggot make', 1311), ('faggot need', 1312), ('faggot never', 1313), ('faggot real', 1314), ('faggot say', 1315), ('faggot smh', 1316), ('faggot take', 1317), ('faggot thi', 1318), ('faggot think', 1319), ('faggot tho', 1320), ('faggot tri', 1321), ('faggot tweet', 1322), ('faggot twitter', 1323), ('faggot u', 1324), ('faggot use', 1325), ('faggot wa', 1326), ('fail', 1327), ('fair', 1328), ('fairi', 1329), ('faith', 1330), ('fake', 1331), ('fake spoiler', 1332), ('fake spoiler air', 1333), ('fall', 1334), ('fam', 1335), ('famili', 1336), ('famili merri', 1337), ('famili merri xma', 1338), ('famili weloveyouashton', 1339), ('famou', 1340), ('fan', 1341), ('fan like', 1342), ('fan run', 1343), ('fan run nake', 1344), ('fan show', 1345), ('fandom', 1346), ('fantasi', 1347), ('fantast', 1348), ('far', 1349), ('farm', 1350), ('fashion', 1351), ('fast', 1352), ('faster', 1353), ('fat', 1354), ('fat ass', 1355), ('fat bitch', 1356), ('fat peopl', 1357), ('fat ugli', 1358), ('father', 1359), ('fault', 1360), ('fav', 1361), ('fave', 1362), ('favor', 1363), ('favorit', 1364), ('favour', 1365), ('favourit', 1366), ('faze', 1367), ('fb', 1368), ('fear', 1369), ('feat', 1370), ('featur', 1371), ('fed', 1372), ('feder', 1373), ('feder govern', 1374), ('feder govern feder', 1375), ('feder govern said', 1376), ('feed', 1377), ('feedli', 1378), ('feel', 1379), ('feel bad', 1380), ('feel better', 1381), ('feel like', 1382), ('feel like faggot', 1383), ('feel like need', 1384), ('feel like true', 1385), ('feel sorri', 1386), ('feel sorri bitch', 1387), ('feet', 1388), ('fell', 1389), ('fellow', 1390), ('felt', 1391), ('felt like', 1392), ('femal', 1393), ('femal killer', 1394), ('femal nigga', 1395), ('femal version', 1396), ('femin', 1397), ('feminist', 1398), ('festiv', 1399), ('festivu', 1400), ('fg', 1401), ('fg feder', 1402), ('fg feder govern', 1403), ('field', 1404), ('field bag', 1405), ('field bag head', 1406), ('fifa', 1407), ('fight', 1408), ('fight faggot', 1409), ('fighter', 1410), ('figur', 1411), ('file', 1412), ('fill', 1413), ('film', 1414), ('filter', 1415), ('filthi', 1416), ('final', 1417), ('final begun', 1418), ('final begun whip', 1419), ('find', 1420), ('find someon', 1421), ('fine', 1422), ('fine ass', 1423), ('fine ass white', 1424), ('finger', 1425), ('finish', 1426), ('finna', 1427), ('fire', 1428), ('first', 1429), ('first time', 1430), ('fish', 1431), ('fit', 1432), ('five', 1433), ('fix', 1434), ('fl', 1435), ('flag', 1436), ('flash', 1437), ('flat', 1438), ('fli', 1439), ('flight', 1440), ('flip', 1441), ('flip flop', 1442), ('flip wallet', 1443), ('float', 1444), ('flood', 1445), ('floor', 1446), ('flop', 1447), ('flop around', 1448), ('flop around tv', 1449), ('florida', 1450), ('flow', 1451), ('flower', 1452), ('fml', 1453), ('focu', 1454), ('foh', 1455), ('folk', 1456), ('follow', 1457), ('follow back', 1458), ('follow faggot', 1459), ('follow last', 1460), ('follow last day', 1461), ('follow love', 1462), ('follow love day', 1463), ('follow love xx', 1464), ('follow make', 1465), ('follow make exquisit', 1466), ('follow pleas', 1467), ('follow pleas x', 1468), ('follow wish', 1469), ('follow wish famili', 1470), ('follow would', 1471), ('follow would best', 1472), ('food', 1473), ('fool', 1474), ('footbal', 1475), ('forc', 1476), ('forc awaken', 1477), ('forehead', 1478), ('foreign', 1479), ('forev', 1480), ('forev follow', 1481), ('forev follow pleas', 1482), ('forex', 1483), ('forex invest', 1484), ('forex invest money', 1485), ('forget', 1486), ('forgiv', 1487), ('forgot', 1488), ('form', 1489), ('form alway', 1490), ('form alway riski', 1491), ('forth', 1492), ('forward', 1493), ('found', 1494), ('four', 1495), ('four mani', 1496), ('four mani nigger', 1497), ('four vine', 1498), ('four vine funni', 1499), ('fox', 1500), ('fr', 1501), ('frame', 1502), ('frank', 1503), ('freak', 1504), ('free', 1505), ('freedom', 1506), ('french', 1507), ('fresh', 1508), ('fri', 1509), ('friend', 1510), ('friend fuck', 1511), ('friendship', 1512), ('frog', 1513), ('front', 1514), ('ft', 1515), ('fuck', 1516), ('fuck bitch', 1517), ('fuck bitch gucci', 1518), ('fuck bitch nigga', 1519), ('fuck boy', 1520), ('fuck christma', 1521), ('fuck cunt', 1522), ('fuck cunt better', 1523), ('fuck dumb', 1524), ('fuck everyth', 1525), ('fuck fag', 1526), ('fuck faggot', 1527), ('fuck faggot fuck', 1528), ('fuck feel', 1529), ('fuck feel sorri', 1530), ('fuck fuck', 1531), ('fuck gay', 1532), ('fuck get', 1533), ('fuck gift', 1534), ('fuck go', 1535), ('fuck hate', 1536), ('fuck hate faggot', 1537), ('fuck hate peopl', 1538), ('fuck joke', 1539), ('fuck kill', 1540), ('fuck lame', 1541), ('fuck lame nigga', 1542), ('fuck life', 1543), ('fuck like', 1544), ('fuck loser', 1545), ('fuck nigga', 1546), ('fuck nigger', 1547), ('fuck outta', 1548), ('fuck peopl', 1549), ('fuck queer', 1550), ('fuck retard', 1551), ('fuck shit', 1552), ('fuck stupid', 1553), ('fuck thi', 1554), ('fuck thing', 1555), ('fuck think', 1556), ('fuck u', 1557), ('fuck u talk', 1558), ('fuck want', 1559), ('fuck white', 1560), ('fuck wit', 1561), ('fuck wit di', 1562), ('fuck wit fuck', 1563), ('fuck wit lame', 1564), ('fuck wit nigga', 1565), ('fuck ya', 1566), ('fuckboy', 1567), ('fucker', 1568), ('fuckin', 1569), ('fuckin bitch', 1570), ('fuckin faggot', 1571), ('fuckin hoe', 1572), ('fuckin seriu', 1573), ('fuckin wit', 1574), ('fuckin wit lame', 1575), ('full', 1576), ('full white', 1577), ('full white girl', 1578), ('full white lie', 1579), ('full white peopl', 1580), ('full white trash', 1581), ('fulli', 1582), ('fun', 1583), ('fund', 1584), ('funni', 1585), ('funni fake', 1586), ('funni fake spoiler', 1587), ('funni vine', 1588), ('funni vine mani', 1589), ('funniest', 1590), ('futur', 1591), ('fw', 1592), ('fw lame', 1593), ('fw lame nigga', 1594), ('g', 1595), ('ga', 1596), ('gaal', 1597), ('gain', 1598), ('game', 1599), ('gang', 1600), ('garbag', 1601), ('gate', 1602), ('gather', 1603), ('gather hit', 1604), ('gather hit shortag', 1605), ('gave', 1606), ('gay', 1607), ('gay ass', 1608), ('gay faggot', 1609), ('gay marriag', 1610), ('gear', 1611), ('gender', 1612), ('gener', 1613), ('genuin', 1614), ('georg', 1615), ('german', 1616), ('get', 1617), ('get ass', 1618), ('get back', 1619), ('get better', 1620), ('get call', 1621), ('get christma', 1622), ('get drunk', 1623), ('get fade', 1624), ('get faggot', 1625), ('get free', 1626), ('get fuck', 1627), ('get fuck outta', 1628), ('get hi', 1629), ('get hit', 1630), ('get job', 1631), ('get job mcdonald', 1632), ('get kill', 1633), ('get life', 1634), ('get like', 1635), ('get mad', 1636), ('get marri', 1637), ('get money', 1638), ('get new', 1639), ('get nigger', 1640), ('get nigger ass', 1641), ('get one', 1642), ('get rape', 1643), ('get readi', 1644), ('get realli', 1645), ('get rid', 1646), ('get see', 1647), ('get shit', 1648), ('get shot', 1649), ('get thi', 1650), ('get u', 1651), ('get ur', 1652), ('get white', 1653), ('gettin', 1654), ('gf', 1655), ('ghost', 1656), ('giant', 1657), ('gif', 1658), ('gift', 1659), ('gift alway', 1660), ('gift alway kindli', 1661), ('gift card', 1662), ('gift guid', 1663), ('gift guid ton', 1664), ('gift idea', 1665), ('gift idea christma', 1666), ('ginger', 1667), ('girl', 1668), ('girl got', 1669), ('girl like', 1670), ('girl like girl', 1671), ('girl near', 1672), ('girlfriend', 1673), ('give', 1674), ('give ani', 1675), ('give fuck', 1676), ('give money', 1677), ('give one', 1678), ('give one chanc', 1679), ('give shit', 1680), ('give us', 1681), ('giveaway', 1682), ('giveaway via', 1683), ('giveaway via r', 1684), ('given', 1685), ('glad', 1686), ('glass', 1687), ('glo', 1688), ('go', 1689), ('go around', 1690), ('go around say', 1691), ('go around tell', 1692), ('go back', 1693), ('go bed', 1694), ('go fuck', 1695), ('go get', 1696), ('go head', 1697), ('go home', 1698), ('go kill', 1699), ('go look', 1700), ('go make', 1701), ('go right', 1702), ('go right head', 1703), ('go school', 1704), ('go sleep', 1705), ('go start', 1706), ('goal', 1707), ('goat', 1708), ('god', 1709), ('god bless', 1710), ('god damn', 1711), ('god everi', 1712), ('god everi smile', 1713), ('god hate', 1714), ('goe', 1715), ('goin', 1716), ('gold', 1717), ('gon', 1718), ('gon na', 1719), ('gon na call', 1720), ('gon na end', 1721), ('gon na get', 1722), ('gon na go', 1723), ('gon na lie', 1724), ('gone', 1725), ('gone talk', 1726), ('good', 1727), ('good day', 1728), ('good job', 1729), ('good luck', 1730), ('good night', 1731), ('good thing', 1732), ('goodby', 1733), ('goodi', 1734), ('goodnight', 1735), ('goofi', 1736), ('googl', 1737), ('goon', 1738), ('gop', 1739), ('got', 1740), ('got ask', 1741), ('got back', 1742), ('got back via', 1743), ('got call', 1744), ('got call faggot', 1745), ('got fuck', 1746), ('got hi', 1747), ('got job', 1748), ('got mad', 1749), ('got marri', 1750), ('got one', 1751), ('got pregnant', 1752), ('got pregnant year', 1753), ('got rape', 1754), ('got ta', 1755), ('got ta get', 1756), ('gotten', 1757), ('gov', 1758), ('gov right', 1759), ('gov right reduc', 1760), ('govern', 1761), ('govern feder', 1762), ('govern feder govern', 1763), ('govern said', 1764), ('govern said yesterday', 1765), ('governor', 1766), ('governor right', 1767), ('governor right reduc', 1768), ('govt', 1769), ('grab', 1770), ('grade', 1771), ('graduat', 1772), ('grain', 1773), ('grandma', 1774), ('grass', 1775), ('grate', 1776), ('gray', 1777), ('great', 1778), ('great job', 1779), ('great mind', 1780), ('great mind think', 1781), ('green', 1782), ('greet', 1783), ('greg', 1784), ('grew', 1785), ('grey', 1786), ('grievanc', 1787), ('groceri', 1788), ('gross', 1789), ('ground', 1790), ('group', 1791), ('grow', 1792), ('grow account', 1793), ('grow account tri', 1794), ('grown', 1795), ('growth', 1796), ('gtfo', 1797), ('gtg', 1798), ('guard', 1799), ('gucci', 1800), ('gucci flip', 1801), ('gucci flip flop', 1802), ('guess', 1803), ('guid', 1804), ('guid ton', 1805), ('guid ton android', 1806), ('guilti', 1807), ('guitar', 1808), ('gun', 1809), ('gun control', 1810), ('gun industri', 1811), ('guy', 1812), ('guy faggot', 1813), ('guy mouth', 1814), ('guy mouth full', 1815), ('guy wa', 1816), ('gym', 1817), ('h', 1818), ('h christma', 1819), ('h christma day', 1820), ('ha', 1821), ('ha ani', 1822), ('ha ani right', 1823), ('ha everi', 1824), ('ha everi right', 1825), ('ha final', 1826), ('ha final begun', 1827), ('ha flop', 1828), ('ha flop around', 1829), ('ha ha', 1830), ('ha one', 1831), ('ha right', 1832), ('hack', 1833), ('haha', 1834), ('hahah', 1835), ('hahaha', 1836), ('hahahaha', 1837), ('hair', 1838), ('half', 1839), ('han', 1840), ('hand', 1841), ('handl', 1842), ('handsom', 1843), ('hang', 1844), ('hannah', 1845), ('happen', 1846), ('happi', 1847), ('happi alway', 1848), ('happi alway mind', 1849), ('happi bday', 1850), ('happi birthday', 1851), ('happi holiday', 1852), ('harass', 1853), ('hard', 1854), ('harder', 1855), ('harri', 1856), ('harsh', 1857), ('harvey', 1858), ('hashtag', 1859), ('hat', 1860), ('hate', 1861), ('hate bitch', 1862), ('hate bitch made', 1863), ('hate fag', 1864), ('hate faggot', 1865), ('hate faggot like', 1866), ('hate fat', 1867), ('hate fat bitch', 1868), ('hate fat peopl', 1869), ('hate fuck', 1870), ('hate much', 1871), ('hate nigga', 1872), ('hate nigger', 1873), ('hate peopl', 1874), ('hate thi', 1875), ('hate u', 1876), ('hate white', 1877), ('hater', 1878), ('hater call', 1879), ('hater call nigga', 1880), ('hatr', 1881), ('head', 1882), ('head ak', 1883), ('head ak stop', 1884), ('head thi', 1885), ('head yogi', 1886), ('head yogi berra', 1887), ('header', 1888), ('health', 1889), ('healthi', 1890), ('hear', 1891), ('heard', 1892), ('heart', 1893), ('heartless', 1894), ('heat', 1895), ('heaven', 1896), ('heavi', 1897), ('hehe', 1898), ('held', 1899), ('hell', 1900), ('hella', 1901), ('hello', 1902), ('hello h', 1903), ('hello h christma', 1904), ('help', 1905), ('help us', 1906), ('hermion', 1907), ('hero', 1908), ('hey', 1909), ('hi', 1910), ('hi ass', 1911), ('hi bitch', 1912), ('hi bitch nigga', 1913), ('hi faggot', 1914), ('hi faggot ass', 1915), ('hi famili', 1916), ('hi friend', 1917), ('hi girl', 1918), ('hi hand', 1919), ('hi music', 1920), ('hi name', 1921), ('hi nigger', 1922), ('hi nigger ass', 1923), ('hi privaci', 1924), ('hi shorti', 1925), ('hi shorti bitch', 1926), ('hidden', 1927), ('hide', 1928), ('high', 1929), ('high court', 1930), ('high court approv', 1931), ('high school', 1932), ('high school district', 1933), ('high strung', 1934), ('higher', 1935), ('highli', 1936), ('hijab', 1937), ('hilari', 1938), ('hill', 1939), ('hillari', 1940), ('hillari clinton', 1941), ('hip', 1942), ('hip hop', 1943), ('hire', 1944), ('hispan', 1945), ('histori', 1946), ('hit', 1947), ('hit femal', 1948), ('hit like', 1949), ('hit shortag', 1950), ('hitler', 1951), ('hittin', 1952), ('hmm', 1953), ('ho', 1954), ('hobbi', 1955), ('hoe', 1956), ('hoe bustin', 1957), ('hoe bustin hoe', 1958), ('hoe u', 1959), ('hoe u know', 1960), ('hold', 1961), ('holder', 1962), ('holder shoot', 1963), ('holder shoot towel', 1964), ('hole', 1965), ('holi', 1966), ('holi shit', 1967), ('holiday', 1968), ('holiday tech', 1969), ('holiday tech gift', 1970), ('hollywood', 1971), ('home', 1972), ('homeless', 1973), ('homi', 1974), ('homo', 1975), ('homophob', 1976), ('honestli', 1977), ('honey', 1978), ('honor', 1979), ('hood', 1980), ('hood nigga', 1981), ('hook', 1982), ('hoop', 1983), ('hop', 1984), ('hope', 1985), ('hope get', 1986), ('hope great', 1987), ('hope u', 1988), ('hope u get', 1989), ('horn', 1990), ('horn bo', 1991), ('horni', 1992), ('horribl', 1993), ('hors', 1994), ('hospit', 1995), ('host', 1996), ('hot', 1997), ('hour', 1998), ('hous', 1999), ('hous christma', 2000), ('hous speaker', 2001), ('hous speaker decentr', 2002), ('howev', 2003), ('hr', 2004), ('hug', 2005), ('huge', 2006), ('huh', 2007), ('hula', 2008), ('hula hoop', 2009), ('human', 2010), ('humor', 2011), ('hundr', 2012), ('hundr thousand', 2013), ('hundr thousand tortur', 2014), ('hungri', 2015), ('hunt', 2016), ('hurt', 2017), ('husband', 2018), ('hv', 2019), ('hype', 2020), ('hypocrit', 2021), ('ian', 2022), ('ice', 2023), ('id', 2024), ('idc', 2025), ('idea', 2026), ('idea christma', 2027), ('idea christma xma', 2028), ('ideal', 2029), ('ident', 2030), ('idiot', 2031), ('idk', 2032), ('idk whi', 2033), ('idol', 2034), ('ig', 2035), ('ignor', 2036), ('ikon', 2037), ('ikr', 2038), ('ili', 2039), ('ill', 2040), ('illeg', 2041), ('ilovethebeach', 2042), ('ilovethebeach centralamerica', 2043), ('im', 2044), ('im alway', 2045), ('im alway gon', 2046), ('im done', 2047), ('im fuck', 2048), ('im gon', 2049), ('im gon na', 2050), ('im sorri', 2051), ('im sure', 2052), ('ima', 2053), ('imag', 2054), ('imagin', 2055), ('imagin cpl', 2056), ('imagin cpl holder', 2057), ('imma', 2058), ('immigr', 2059), ('imo', 2060), ('import', 2061), ('imposs', 2062), ('impress', 2063), ('improv', 2064), ('inbr', 2065), ('inc', 2066), ('inch', 2067), ('includ', 2068), ('incred', 2069), ('india', 2070), ('indian', 2071), ('indoctrin', 2072), ('indoctrin lead', 2073), ('indoctrin lead school', 2074), ('industri', 2075), ('inferior', 2076), ('info', 2077), ('info gather', 2078), ('info gather hit', 2079), ('inform', 2080), ('injuri', 2081), ('inmat', 2082), ('inner', 2083), ('innoc', 2084), ('innoc peopl', 2085), ('insecur', 2086), ('insid', 2087), ('inspir', 2088), ('instagram', 2089), ('instead', 2090), ('insult', 2091), ('intel', 2092), ('intel armour', 2093), ('intel armour info', 2094), ('intent', 2095), ('interest', 2096), ('internet', 2097), ('interrog', 2098), ('interrog patient', 2099), ('interrog patient gun', 2100), ('interview', 2101), ('introduc', 2102), ('invad', 2103), ('invas', 2104), ('invest', 2105), ('invest money', 2106), ('invest money ani', 2107), ('investig', 2108), ('investor', 2109), ('invit', 2110), ('involv', 2111), ('ion', 2112), ('iphon', 2113), ('iphon plu', 2114), ('iphon plu bid', 2115), ('iran', 2116), ('ireland', 2117), ('iron', 2118), ('irrelev', 2119), ('isi', 2120), ('islam', 2121), ('island', 2122), ('isnt', 2123), ('israel', 2124), ('issu', 2125), ('item', 2126), ('ive', 2127), ('j', 2128), ('jack', 2129), ('jail', 2130), ('jam', 2131), ('jame', 2132), ('jan', 2133), ('januari', 2134), ('japan', 2135), ('jay', 2136), ('jealou', 2137), ('jean', 2138), ('jedi', 2139), ('jeep', 2140), ('jersey', 2141), ('jess', 2142), ('jesu', 2143), ('jesu christ', 2144), ('jet', 2145), ('jew', 2146), ('jewelri', 2147), ('jewish', 2148), ('jihadist', 2149), ('jim', 2150), ('jimmi', 2151), ('jk', 2152), ('jo', 2153), ('job', 2154), ('job get', 2155), ('job mcdonald', 2156), ('job work', 2157), ('joe', 2158), ('joey', 2159), ('john', 2160), ('johnni', 2161), ('join', 2162), ('joke', 2163), ('jordan', 2164), ('jose', 2165), ('josh', 2166), ('josh norman', 2167), ('js', 2168), ('ju', 2169), ('judg', 2170), ('judg bitch', 2171), ('judg bitch nigga', 2172), ('juic', 2173), ('juici', 2174), ('jump', 2175), ('jungl', 2176), ('jungl bunni', 2177), ('justic', 2178), ('justifi', 2179), ('justin', 2180), ('justin bieber', 2181), ('justwaitonit', 2182), ('k', 2183), ('k gold', 2184), ('k yellow', 2185), ('k yellow gold', 2186), ('karma', 2187), ('karnataka', 2188), ('karnataka polic', 2189), ('karnataka polic intel', 2190), ('kati', 2191), ('kd', 2192), ('keep', 2193), ('keep nigger', 2194), ('kelli', 2195), ('kentucki', 2196), ('kept', 2197), ('kettl', 2198), ('kevin', 2199), ('kevin gate', 2200), ('key', 2201), ('keyboard', 2202), ('khan', 2203), ('kick', 2204), ('kick scratch', 2205), ('kick scratch stay', 2206), ('kid', 2207), ('kid get', 2208), ('kid like', 2209), ('kidnap', 2210), ('kike', 2211), ('kill', 2212), ('kill faggot', 2213), ('kill hi', 2214), ('kill kill', 2215), ('kill nigger', 2216), ('kill stor', 2217), ('kill thi', 2218), ('kill white', 2219), ('killer', 2220), ('kim', 2221), ('kind', 2222), ('kinda', 2223), ('kinda wan', 2224), ('kinda wan na', 2225), ('kindli', 2226), ('kindli follow', 2227), ('kindli follow love', 2228), ('king', 2229), ('kirtiazadsuspend', 2230), ('kiss', 2231), ('kitchen', 2232), ('knee', 2233), ('knew', 2234), ('knew wa', 2235), ('knew would', 2236), ('kno', 2237), ('knock', 2238), ('knot', 2239), ('know', 2240), ('know better', 2241), ('know fuck', 2242), ('know get', 2243), ('know go', 2244), ('know got', 2245), ('know im', 2246), ('know like', 2247), ('know look', 2248), ('know men', 2249), ('know men women', 2250), ('know right', 2251), ('know thi', 2252), ('know u', 2253), ('know wa', 2254), ('know whi', 2255), ('know word', 2256), ('known', 2257), ('ko', 2258), ('korean', 2259), ('ky', 2260), ('kylo', 2261), ('kylo ren', 2262), ('l', 2263), ('la', 2264), ('la vega', 2265), ('lack', 2266), ('lad', 2267), ('ladi', 2268), ('lag', 2269), ('laid', 2270), ('laker', 2271), ('lame', 2272), ('lame ass', 2273), ('lame ass nigga', 2274), ('lame bitch', 2275), ('lame nigga', 2276), ('lame nigga aint', 2277), ('lame nigga lame', 2278), ('lame nigga make', 2279), ('lame nigga makin', 2280), ('land', 2281), ('lane', 2282), ('languag', 2283), ('larg', 2284), ('larri', 2285), ('last', 2286), ('last day', 2287), ('last day congression', 2288), ('last day stat', 2289), ('last minut', 2290), ('last night', 2291), ('last time', 2292), ('last week', 2293), ('last year', 2294), ('late', 2295), ('later', 2296), ('latest', 2297), ('latino', 2298), ('laugh', 2299), ('launch', 2300), ('lauren', 2301), ('law', 2302), ('law model', 2303), ('law model track', 2304), ('lawyer', 2305), ('lay', 2306), ('lazi', 2307), ('lead', 2308), ('lead school', 2309), ('lead school closur', 2310), ('leader', 2311), ('leadership', 2312), ('leagu', 2313), ('leak', 2314), ('learn', 2315), ('least', 2316), ('leather', 2317), ('leather flip', 2318), ('leather flip wallet', 2319), ('leav', 2320), ('leav alon', 2321), ('led', 2322), ('left', 2323), ('left say', 2324), ('leg', 2325), ('leg stand', 2326), ('legal', 2327), ('legit', 2328), ('lektriqu', 2329), ('lektriqu religion', 2330), ('lektriqu religion muzzi', 2331), ('length', 2332), ('lesbian', 2333), ('lesbian want', 2334), ('lesbian want meet', 2335), ('less', 2336), ('lesson', 2337), ('let', 2338), ('let get', 2339), ('let go', 2340), ('let know', 2341), ('let play', 2342), ('let see', 2343), ('let us', 2344), ('letter', 2345), ('level', 2346), ('liar', 2347), ('lib', 2348), ('liber', 2349), ('lick', 2350), ('lie', 2351), ('life', 2352), ('life full', 2353), ('life lol', 2354), ('life love', 2355), ('life love forev', 2356), ('life sinc', 2357), ('life sinc quit', 2358), ('lifestyl', 2359), ('light', 2360), ('likabl', 2361), ('likabl everyon', 2362), ('likabl everyon want', 2363), ('like', 2364), ('like bitch', 2365), ('like bitch made', 2366), ('like dirti', 2367), ('like dyke', 2368), ('like ever', 2369), ('like everi', 2370), ('like fag', 2371), ('like faggot', 2372), ('like faggot smh', 2373), ('like fuck', 2374), ('like fuck queer', 2375), ('like fuck wit', 2376), ('like gay', 2377), ('like get', 2378), ('like girl', 2379), ('like girl near', 2380), ('like go', 2381), ('like gon', 2382), ('like gon na', 2383), ('like guy', 2384), ('like hi', 2385), ('like kill', 2386), ('like know', 2387), ('like life', 2388), ('like like', 2389), ('like littl', 2390), ('like need', 2391), ('like need go', 2392), ('like nigger', 2393), ('like one', 2394), ('like quora', 2395), ('like quora length', 2396), ('like real', 2397), ('like realli', 2398), ('like rest', 2399), ('like right', 2400), ('like sad', 2401), ('like sad eye', 2402), ('like said', 2403), ('like say', 2404), ('like see', 2405), ('like shit', 2406), ('like thi', 2407), ('like time', 2408), ('like true', 2409), ('like u', 2410), ('like ugli', 2411), ('like video', 2412), ('like video black', 2413), ('like video mani', 2414), ('like video thi', 2415), ('like wa', 2416), ('like white', 2417), ('like white trash', 2418), ('like work', 2419), ('like would', 2420), ('lil', 2421), ('lil bitch', 2422), ('lil boy', 2423), ('limit', 2424), ('line', 2425), ('link', 2426), ('lip', 2427), ('liquor', 2428), ('list', 2429), ('listen', 2430), ('listen faggot', 2431), ('listen hous', 2432), ('listen hous speaker', 2433), ('listen nigger', 2434), ('listen nigger music', 2435), ('lit', 2436), ('liter', 2437), ('liter biggest', 2438), ('liter biggest faggot', 2439), ('littl', 2440), ('littl bitch', 2441), ('littl faggot', 2442), ('littl girl', 2443), ('littl sister', 2444), ('littl thing', 2445), ('live', 2446), ('live near', 2447), ('live room', 2448), ('live trailer', 2449), ('live trailer park', 2450), ('lmao', 2451), ('lmaoo', 2452), ('lmaoooo', 2453), ('lmaooooo', 2454), ('lmfao', 2455), ('lo', 2456), ('lo angel', 2457), ('load', 2458), ('local', 2459), ('lock', 2460), ('logic', 2461), ('lol', 2462), ('lol dude', 2463), ('lol dude hate', 2464), ('lol fuck', 2465), ('lol like', 2466), ('london', 2467), ('long', 2468), ('long time', 2469), ('longer', 2470), ('look', 2471), ('look alik', 2472), ('look better', 2473), ('look cute', 2474), ('look forward', 2475), ('look fuck', 2476), ('look good', 2477), ('look happi', 2478), ('look hi', 2479), ('look job', 2480), ('look like', 2481), ('look like faggot', 2482), ('look like need', 2483), ('look thi', 2484), ('lookin', 2485), ('loos', 2486), ('lord', 2487), ('lose', 2488), ('loser', 2489), ('loser fag', 2490), ('loss', 2491), ('lost', 2492), ('lot', 2493), ('loud', 2494), ('loui', 2495), ('love', 2496), ('love day', 2497), ('love day pal', 2498), ('love fag', 2499), ('love forev', 2500), ('love forev follow', 2501), ('love happi', 2502), ('love happi alway', 2503), ('love life', 2504), ('love much', 2505), ('love nigger', 2506), ('love overcom', 2507), ('love overcom obstacl', 2508), ('love peopl', 2509), ('love someon', 2510), ('love thi', 2511), ('love u', 2512), ('love xx', 2513), ('love ya', 2514), ('lover', 2515), ('lovin', 2516), ('low', 2517), ('lower', 2518), ('lowkey', 2519), ('lrt', 2520), ('luca', 2521), ('luck', 2522), ('lucki', 2523), ('luke', 2524), ('lunch', 2525), ('lvg', 2526), ('lyric', 2527), ('mac', 2528), ('machin', 2529), ('mad', 2530), ('mad caus', 2531), ('made', 2532), ('made ass', 2533), ('made ass nigga', 2534), ('made nigga', 2535), ('made nigga like', 2536), ('made nigga would', 2537), ('made savag', 2538), ('made savag terrorist', 2539), ('made white', 2540), ('magazin', 2541), ('maggot', 2542), ('magic', 2543), ('mail', 2544), ('main', 2545), ('maintain', 2546), ('major', 2547), ('major attack', 2548), ('major attack left', 2549), ('make', 2550), ('make ani', 2551), ('make excus', 2552), ('make exquisit', 2553), ('make exquisit stay', 2554), ('make feel', 2555), ('make feel like', 2556), ('make last', 2557), ('make last day', 2558), ('make life', 2559), ('make look', 2560), ('make nois', 2561), ('make sens', 2562), ('make sure', 2563), ('make us', 2564), ('maker', 2565), ('makeup', 2566), ('makin', 2567), ('makin nois', 2568), ('male', 2569), ('mall', 2570), ('mama', 2571), ('man', 2572), ('man like', 2573), ('man rape', 2574), ('man said', 2575), ('manag', 2576), ('mani', 2577), ('mani nigger', 2578), ('mani nigger store', 2579), ('mani peopl', 2580), ('mani time', 2581), ('manufactur', 2582), ('map', 2583), ('mari', 2584), ('mariah', 2585), ('mariah carey', 2586), ('mario', 2587), ('mark', 2588), ('market', 2589), ('marri', 2590), ('marri man', 2591), ('marri money', 2592), ('marriag', 2593), ('mass', 2594), ('master', 2595), ('match', 2596), ('mate', 2597), ('math', 2598), ('matter', 2599), ('matur', 2600), ('max', 2601), ('may', 2602), ('may pleas', 2603), ('may pleas follow', 2604), ('mayb', 2605), ('mcdonald', 2606), ('meal', 2607), ('mean', 2608), ('mean know', 2609), ('mean nigger', 2610), ('meant', 2611), ('measur', 2612), ('meat', 2613), ('media', 2614), ('medic', 2615), ('meet', 2616), ('meet girl', 2617), ('meet girl like', 2618), ('melt', 2619), ('member', 2620), ('meme', 2621), ('memori', 2622), ('men', 2623), ('men get', 2624), ('men get rape', 2625), ('men rape', 2626), ('men women', 2627), ('men women fan', 2628), ('mental', 2629), ('mention', 2630), ('merci', 2631), ('merri', 2632), ('merri christma', 2633), ('merri xma', 2634), ('merri xma christmasweek', 2635), ('mess', 2636), ('messag', 2637), ('messi', 2638), ('met', 2639), ('metal', 2640), ('mexican', 2641), ('mexico', 2642), ('mf', 2643), ('mic', 2644), ('michael', 2645), ('mid', 2646), ('middl', 2647), ('middl class', 2648), ('midget', 2649), ('might', 2650), ('mike', 2651), ('mikey', 2652), ('mile', 2653), ('militari', 2654), ('milk', 2655), ('million', 2656), ('million dollar', 2657), ('min', 2658), ('mind', 2659), ('mind follow', 2660), ('mind follow love', 2661), ('mind think', 2662), ('mind think alik', 2663), ('mine', 2664), ('mini', 2665), ('minimum', 2666), ('minimum wage', 2667), ('minimum wage feder', 2668), ('minimum wage fg', 2669), ('minimum wage ngige', 2670), ('minor', 2671), ('minut', 2672), ('miracl', 2673), ('mirror', 2674), ('mirror cop', 2675), ('mirror cop dmz', 2676), ('miss', 2677), ('mission', 2678), ('mistak', 2679), ('mix', 2680), ('mo', 2681), ('mobil', 2682), ('mock', 2683), ('model', 2684), ('model track', 2685), ('model track jihadist', 2686), ('modern', 2687), ('mom', 2688), ('mom ask', 2689), ('moment', 2690), ('momma', 2691), ('mommi', 2692), ('monday', 2693), ('money', 2694), ('money ani', 2695), ('money ani form', 2696), ('money fuck', 2697), ('monkey', 2698), ('monster', 2699), ('month', 2700), ('mood', 2701), ('moon', 2702), ('moral', 2703), ('morn', 2704), ('moron', 2705), ('mostli', 2706), ('mother', 2707), ('mother fucker', 2708), ('motherfuck', 2709), ('motiv', 2710), ('mountain', 2711), ('mouth', 2712), ('mouth full', 2713), ('mouth full white', 2714), ('move', 2715), ('movi', 2716), ('mp', 2717), ('mr', 2718), ('msg', 2719), ('mt', 2720), ('much', 2721), ('much better', 2722), ('mufc', 2723), ('muhammad', 2724), ('multi', 2725), ('multipl', 2726), ('mum', 2727), ('murder', 2728), ('murder white', 2729), ('muscl', 2730), ('music', 2731), ('muslim', 2732), ('muslim famili', 2733), ('muslim women', 2734), ('must', 2735), ('mutil', 2736), ('mutil corps', 2737), ('muzzi', 2738), ('muzzi indoctrin', 2739), ('muzzi indoctrin lead', 2740), ('muzzi remix', 2741), ('n', 2742), ('n minimum', 2743), ('n minimum wage', 2744), ('na', 2745), ('na call', 2746), ('na end', 2747), ('na fuck', 2748), ('na get', 2749), ('na go', 2750), ('na kill', 2751), ('na know', 2752), ('na lie', 2753), ('na see', 2754), ('na see perform', 2755), ('nah', 2756), ('nail', 2757), ('nake', 2758), ('nake across', 2759), ('nake across field', 2760), ('name', 2761), ('nasti', 2762), ('nation', 2763), ('nation park', 2764), ('nativ', 2765), ('natur', 2766), ('naughti', 2767), ('nazi', 2768), ('nba', 2769), ('nd', 2770), ('ne', 2771), ('near', 2772), ('necessari', 2773), ('neck', 2774), ('necklac', 2775), ('need', 2776), ('need fag', 2777), ('need find', 2778), ('need get', 2779), ('need go', 2780), ('need help', 2781), ('need kill', 2782), ('need know', 2783), ('need new', 2784), ('need outrag', 2785), ('need outrag tbyg', 2786), ('need stop', 2787), ('needa', 2788), ('neg', 2789), ('negro', 2790), ('neighbor', 2791), ('neighbour', 2792), ('neither', 2793), ('nerd', 2794), ('net', 2795), ('network', 2796), ('neva', 2797), ('never', 2798), ('never chang', 2799), ('never forget', 2800), ('never get', 2801), ('never knew', 2802), ('never leav', 2803), ('never said', 2804), ('never said wa', 2805), ('never seen', 2806), ('new', 2807), ('new follow', 2808), ('new follow last', 2809), ('new orlean', 2810), ('new white', 2811), ('new white bitch', 2812), ('new year', 2813), ('new york', 2814), ('news', 2815), ('newton', 2816), ('newwav', 2817), ('newwav click', 2818), ('newwav click stereotyp', 2819), ('next', 2820), ('next time', 2821), ('next year', 2822), ('nfl', 2823), ('nfl player', 2824), ('ngige', 2825), ('ngige via', 2826), ('nh', 2827), ('nice', 2828), ('nice peopl', 2829), ('nich', 2830), ('nick', 2831), ('niec', 2832), ('nigga', 2833), ('nigga aint', 2834), ('nigga aint makin', 2835), ('nigga ask', 2836), ('nigga bitch', 2837), ('nigga came', 2838), ('nigga cop', 2839), ('nigga cuffin', 2840), ('nigga cuffin hoe', 2841), ('nigga even', 2842), ('nigga faggot', 2843), ('nigga fuck', 2844), ('nigga fuck wit', 2845), ('nigga get', 2846), ('nigga got', 2847), ('nigga got back', 2848), ('nigga hi', 2849), ('nigga hi bitch', 2850), ('nigga hi music', 2851), ('nigga hi shorti', 2852), ('nigga lame', 2853), ('nigga lame bitch', 2854), ('nigga like', 2855), ('nigga like thi', 2856), ('nigga mad', 2857), ('nigga make', 2858), ('nigga make nois', 2859), ('nigga makin', 2860), ('nigga makin nois', 2861), ('nigga put', 2862), ('nigga real', 2863), ('nigga realli', 2864), ('nigga shit', 2865), ('nigga thi', 2866), ('nigga thi bitch', 2867), ('nigga w', 2868), ('nigga would', 2869), ('nigger', 2870), ('nigger ass', 2871), ('nigger bitch', 2872), ('nigger die', 2873), ('nigger doe', 2874), ('nigger even', 2875), ('nigger faggot', 2876), ('nigger fuck', 2877), ('nigger get', 2878), ('nigger got', 2879), ('nigger hate', 2880), ('nigger jew', 2881), ('nigger look', 2882), ('nigger monkey', 2883), ('nigger music', 2884), ('nigger need', 2885), ('nigger nigger', 2886), ('nigger one', 2887), ('nigger racist', 2888), ('nigger spic', 2889), ('nigger store', 2890), ('nigger store alexfromtarget', 2891), ('nigger store jk', 2892), ('nigger store start', 2893), ('nigger store ur', 2894), ('nigger store vine', 2895), ('nigger thi', 2896), ('nigger u', 2897), ('nigger would', 2898), ('night', 2899), ('nightmar', 2900), ('nobodi', 2901), ('nobodi want', 2902), ('nois', 2903), ('non', 2904), ('non white', 2905), ('none', 2906), ('nonsens', 2907), ('noodl', 2908), ('nope', 2909), ('normal', 2910), ('norman', 2911), ('north', 2912), ('norway', 2913), ('norway teach', 2914), ('nose', 2915), ('note', 2916), ('noth', 2917), ('noth say', 2918), ('noth wrong', 2919), ('notic', 2920), ('notmyabuela', 2921), ('nowaday', 2922), ('nowher', 2923), ('nowplay', 2924), ('np', 2925), ('nsfw', 2926), ('nude', 2927), ('number', 2928), ('nurs', 2929), ('nut', 2930), ('ny', 2931), ('nyc', 2932), ('obama', 2933), ('obj', 2934), ('obj faggot', 2935), ('obsess', 2936), ('obstacl', 2937), ('obstacl exampl', 2938), ('obstacl exampl life', 2939), ('obviou', 2940), ('obvious', 2941), ('ocean', 2942), ('odd', 2943), ('odel', 2944), ('odel beckham', 2945), ('odel faggot', 2946), ('offend', 2947), ('offens', 2948), ('offer', 2949), ('offic', 2950), ('offici', 2951), ('often', 2952), ('oh', 2953), ('oh god', 2954), ('oh yeah', 2955), ('oil', 2956), ('ok', 2957), ('okay', 2958), ('ol', 2959), ('old', 2960), ('old girl', 2961), ('old man', 2962), ('old school', 2963), ('ole', 2964), ('omfg', 2965), ('omg', 2966), ('onc', 2967), ('one', 2968), ('one anoth', 2969), ('one best', 2970), ('one chanc', 2971), ('one cummi', 2972), ('one cummi two', 2973), ('one day', 2974), ('one ever', 2975), ('one faggot', 2976), ('one fuck', 2977), ('one go', 2978), ('one ha', 2979), ('one like', 2980), ('one night', 2981), ('one peopl', 2982), ('one person', 2983), ('one reason', 2984), ('one sinc', 2985), ('one thing', 2986), ('one think', 2987), ('one time', 2988), ('one want', 2989), ('one white', 2990), ('onli', 2991), ('onli faggot', 2992), ('onli get', 2993), ('onli one', 2994), ('onli thing', 2995), ('onli way', 2996), ('onlin', 2997), ('op', 2998), ('op faggot', 2999), ('op funni', 3000), ('op funni fake', 3001), ('open', 3002), ('openli', 3003), ('oper', 3004), ('opinion', 3005), ('opp', 3006), ('opportun', 3007), ('opportun employ', 3008), ('opposit', 3009), ('oppress', 3010), ('option', 3011), ('orang', 3012), ('order', 3013), ('organ', 3014), ('origin', 3015), ('orlean', 3016), ('outlet', 3017), ('outrag', 3018), ('outrag muzzi', 3019), ('outrag muzzi indoctrin', 3020), ('outrag tbyg', 3021), ('outsid', 3022), ('outsid fag', 3023), ('outsid hous', 3024), ('outsid stadium', 3025), ('outta', 3026), ('outta faggot', 3027), ('overcom', 3028), ('overcom obstacl', 3029), ('overcom obstacl exampl', 3030), ('overr', 3031), ('owe', 3032), ('owner', 3033), ('oxygen', 3034), ('p', 3035), ('pa', 3036), ('pack', 3037), ('pack fag', 3038), ('packag', 3039), ('packet', 3040), ('page', 3041), ('paid', 3042), ('pain', 3043), ('paint', 3044), ('pair', 3045), ('pak', 3046), ('pakistan', 3047), ('pal', 3048), ('pal x', 3049), ('palin', 3050), ('palin teabagg', 3051), ('panda', 3052), ('pandora', 3053), ('pant', 3054), ('panther', 3055), ('panther call', 3056), ('paparazzi', 3057), ('paper', 3058), ('paper bag', 3059), ('paper bag head', 3060), ('paper towel', 3061), ('parent', 3062), ('parent outrag', 3063), ('parent outrag muzzi', 3064), ('park', 3065), ('park boy', 3066), ('part', 3067), ('parti', 3068), ('partner', 3069), ('pass', 3070), ('passion', 3071), ('past', 3072), ('pathet', 3073), ('patient', 3074), ('patient gun', 3075), ('patient gun industri', 3076), ('patriot', 3077), ('paul', 3078), ('pay', 3079), ('pay attent', 3080), ('pay bill', 3081), ('pc', 3082), ('peac', 3083), ('pedophil', 3084), ('pee', 3085), ('peep', 3086), ('pendant', 3087), ('peni', 3088), ('peopl', 3089), ('peopl act', 3090), ('peopl alway', 3091), ('peopl call', 3092), ('peopl faggot', 3093), ('peopl fuck', 3094), ('peopl get', 3095), ('peopl get kill', 3096), ('peopl kill', 3097), ('peopl know', 3098), ('peopl life', 3099), ('peopl like', 3100), ('peopl like thi', 3101), ('peopl live', 3102), ('peopl need', 3103), ('peopl right', 3104), ('peopl say', 3105), ('peopl see', 3106), ('peopl think', 3107), ('peopl white', 3108), ('peopl world', 3109), ('per', 3110), ('perfect', 3111), ('perform', 3112), ('perform citi', 3113), ('perform citi soon', 3114), ('perhap', 3115), ('period', 3116), ('person', 3117), ('person kill', 3118), ('person like', 3119), ('person live', 3120), ('person want', 3121), ('pet', 3122), ('peter', 3123), ('philli', 3124), ('phone', 3125), ('phoni', 3126), ('photo', 3127), ('physic', 3128), ('pic', 3129), ('pick', 3130), ('pick one', 3131), ('pictur', 3132), ('pie', 3133), ('piec', 3134), ('piec shit', 3135), ('piec white', 3136), ('piec white trash', 3137), ('pig', 3138), ('pill', 3139), ('pillow', 3140), ('pink', 3141), ('piss', 3142), ('piti', 3143), ('pizza', 3144), ('pjnet', 3145), ('pl', 3146), ('place', 3147), ('plain', 3148), ('plan', 3149), ('plane', 3150), ('planet', 3151), ('plastic', 3152), ('plastic bag', 3153), ('play', 3154), ('player', 3155), ('player call', 3156), ('playlist', 3157), ('playlist amaz', 3158), ('playlist amaz pleas', 3159), ('playoff', 3160), ('pleas', 3161), ('pleas come', 3162), ('pleas follow', 3163), ('pleas follow make', 3164), ('pleas help', 3165), ('pleas let', 3166), ('pleas like', 3167), ('pleas like video', 3168), ('pleas stop', 3169), ('pleas subscrib', 3170), ('pleas x', 3171), ('pleasant', 3172), ('pleasur', 3173), ('plenti', 3174), ('plot', 3175), ('plu', 3176), ('plu bid', 3177), ('pm', 3178), ('po', 3179), ('poc', 3180), ('pocket', 3181), ('point', 3182), ('pol', 3183), ('pol see', 3184), ('pol see child', 3185), ('pole', 3186), ('polic', 3187), ('polic intel', 3188), ('polic intel armour', 3189), ('polici', 3190), ('polit', 3191), ('politician', 3192), ('poll', 3193), ('pool', 3194), ('poor', 3195), ('pop', 3196), ('porch', 3197), ('porch monkey', 3198), ('porn', 3199), ('porn law', 3200), ('porn law model', 3201), ('pose', 3202), ('posit', 3203), ('possibl', 3204), ('post', 3205), ('potato', 3206), ('potenti', 3207), ('pound', 3208), ('pour', 3209), ('power', 3210), ('power zen', 3211), ('power zen sausag', 3212), ('ppl', 3213), ('practic', 3214), ('pray', 3215), ('prayer', 3216), ('pre', 3217), ('preciou', 3218), ('prefer', 3219), ('pregnant', 3220), ('pregnant year', 3221), ('pregnant year reveal', 3222), ('present', 3223), ('presid', 3224), ('press', 3225), ('pretend', 3226), ('pretti', 3227), ('pretti fuck', 3228), ('pretti much', 3229), ('prevent', 3230), ('price', 3231), ('priceless', 3232), ('pride', 3233), ('princ', 3234), ('princess', 3235), ('print', 3236), ('prison', 3237), ('privaci', 3238), ('privaci right', 3239), ('privat', 3240), ('privileg', 3241), ('prize', 3242), ('pro', 3243), ('prob', 3244), ('probabl', 3245), ('problem', 3246), ('process', 3247), ('prod', 3248), ('produc', 3249), ('product', 3250), ('professor', 3251), ('profil', 3252), ('profit', 3253), ('program', 3254), ('project', 3255), ('prolli', 3256), ('promis', 3257), ('promot', 3258), ('proof', 3259), ('proper', 3260), ('properti', 3261), ('propos', 3262), ('protect', 3263), ('protest', 3264), ('proud', 3265), ('prove', 3266), ('provid', 3267), ('ps', 3268), ('public', 3269), ('pull', 3270), ('punch', 3271), ('punch kick', 3272), ('punch kick scratch', 3273), ('punish', 3274), ('punk', 3275), ('purchas', 3276), ('pure', 3277), ('purpos', 3278), ('push', 3279), ('pussi', 3280), ('pussi ass', 3281), ('pussi faggot', 3282), ('put', 3283), ('put bag', 3284), ('put bag head', 3285), ('put hand', 3286), ('put thi', 3287), ('q', 3288), ('qb', 3289), ('qualiti', 3290), ('que', 3291), ('queen', 3292), ('queer', 3293), ('question', 3294), ('quick', 3295), ('quit', 3296), ('quit life', 3297), ('quit life lol', 3298), ('quora', 3299), ('quora length', 3300), ('quot', 3301), ('r', 3302), ('r kelli', 3303), ('r u', 3304), ('rabbit', 3305), ('race', 3306), ('racism', 3307), ('racist', 3308), ('rack', 3309), ('radic', 3310), ('radic islam', 3311), ('radio', 3312), ('rain', 3313), ('rais', 3314), ('ran', 3315), ('random', 3316), ('rang', 3317), ('rap', 3318), ('rape', 3319), ('rape brother', 3320), ('rape brother got', 3321), ('rape kill', 3322), ('rape murder', 3323), ('rape women', 3324), ('rapid', 3325), ('rapist', 3326), ('rapper', 3327), ('rare', 3328), ('rat', 3329), ('rate', 3330), ('rather', 3331), ('rd', 3332), ('reach', 3333), ('react', 3334), ('read', 3335), ('read book', 3336), ('read thi', 3337), ('read thi book', 3338), ('readi', 3339), ('reagan', 3340), ('reagan made', 3341), ('reagan made savag', 3342), ('real', 3343), ('real life', 3344), ('real nigga', 3345), ('real shit', 3346), ('realis', 3347), ('realiti', 3348), ('realiz', 3349), ('realli', 3350), ('realli fuck', 3351), ('realli hate', 3352), ('realli need', 3353), ('realli realli', 3354), ('realli want', 3355), ('rear', 3356), ('rear view', 3357), ('rear view mirror', 3358), ('reason', 3359), ('reason whi', 3360), ('receiv', 3361), ('recent', 3362), ('recommend', 3363), ('record', 3364), ('recoveri', 3365), ('recoveri school', 3366), ('recoveri school district', 3367), ('recruit', 3368), ('red', 3369), ('redneck', 3370), ('redneck white', 3371), ('redneck white trash', 3372), ('reduc', 3373), ('reduc minimum', 3374), ('reduc minimum wage', 3375), ('reduc n', 3376), ('reduc n minimum', 3377), ('refer', 3378), ('refuge', 3379), ('refus', 3380), ('regardless', 3381), ('regist', 3382), ('regret', 3383), ('reh', 3384), ('reindeer', 3385), ('relationship', 3386), ('relax', 3387), ('releas', 3388), ('religi', 3389), ('religion', 3390), ('religion muzzi', 3391), ('religion muzzi remix', 3392), ('remain', 3393), ('remain karnataka', 3394), ('remain karnataka polic', 3395), ('rememb', 3396), ('remind', 3397), ('remix', 3398), ('remov', 3399), ('ren', 3400), ('rent', 3401), ('replac', 3402), ('repli', 3403), ('report', 3404), ('repres', 3405), ('republican', 3406), ('request', 3407), ('requir', 3408), ('rescu', 3409), ('research', 3410), ('resid', 3411), ('resourc', 3412), ('respect', 3413), ('respond', 3414), ('respons', 3415), ('rest', 3416), ('result', 3417), ('retail', 3418), ('retard', 3419), ('return', 3420), ('retweet', 3421), ('retweet pleas', 3422), ('retweet pleas like', 3423), ('reveal', 3424), ('reveal stori', 3425), ('review', 3426), ('rey', 3427), ('rice', 3428), ('rich', 3429), ('rid', 3430), ('ride', 3431), ('ridicul', 3432), ('right', 3433), ('right angri', 3434), ('right blame', 3435), ('right complain', 3436), ('right head', 3437), ('right interrog', 3438), ('right interrog patient', 3439), ('right invad', 3440), ('right judg', 3441), ('right lane', 3442), ('right mad', 3443), ('right reduc', 3444), ('right reduc minimum', 3445), ('right reduc n', 3446), ('right say', 3447), ('right talk', 3448), ('right tell', 3449), ('right thing', 3450), ('right way', 3451), ('right wing', 3452), ('right wrong', 3453), ('ring', 3454), ('ring bell', 3455), ('ring k', 3456), ('ringer', 3457), ('rip', 3458), ('riski', 3459), ('riski ea', 3460), ('river', 3461), ('rn', 3462), ('road', 3463), ('roast', 3464), ('rob', 3465), ('robert', 3466), ('rock', 3467), ('rocket', 3468), ('roll', 3469), ('rondo', 3470), ('room', 3471), ('room full', 3472), ('rose', 3473), ('rough', 3474), ('round', 3475), ('rs', 3476), ('rude', 3477), ('ruin', 3478), ('rule', 3479), ('rule law', 3480), ('rumor', 3481), ('run', 3482), ('run low', 3483), ('run nake', 3484), ('run nake across', 3485), ('run time', 3486), ('rush', 3487), ('russian', 3488), ('ryan', 3489), ('sa', 3490), ('sack', 3491), ('sad', 3492), ('sad eye', 3493), ('sad eye bad', 3494), ('sadli', 3495), ('safe', 3496), ('safeti', 3497), ('safeti devic', 3498), ('safeti devic rear', 3499), ('said', 3500), ('said im', 3501), ('said thi', 3502), ('said wa', 3503), ('said yesterday', 3504), ('saint', 3505), ('saint fan', 3506), ('sake', 3507), ('sale', 3508), ('sam', 3509), ('san', 3510), ('sand', 3511), ('sand nigger', 3512), ('sander', 3513), ('sandrabland', 3514), ('santa', 3515), ('santa clau', 3516), ('sarah', 3517), ('sarah palin', 3518), ('sarah palin teabagg', 3519), ('sarcasm', 3520), ('sat', 3521), ('satan', 3522), ('saudi', 3523), ('sausag', 3524), ('sausag make', 3525), ('sausag make last', 3526), ('savag', 3527), ('savag terrorist', 3528), ('savag terrorist war', 3529), ('save', 3530), ('saw', 3531), ('say', 3532), ('say anyth', 3533), ('say fag', 3534), ('say faggot', 3535), ('say fuck', 3536), ('say hate', 3537), ('say im', 3538), ('say nigga', 3539), ('say nigger', 3540), ('say say', 3541), ('say someth', 3542), ('say thi', 3543), ('say u', 3544), ('say wa', 3545), ('scandal', 3546), ('scare', 3547), ('scare white', 3548), ('scari', 3549), ('scene', 3550), ('schlong', 3551), ('school', 3552), ('school bu', 3553), ('school bu driver', 3554), ('school closur', 3555), ('school closur need', 3556), ('school district', 3557), ('school district desper', 3558), ('scienc', 3559), ('score', 3560), ('scott', 3561), ('scratch', 3562), ('scratch stay', 3563), ('scratch stay kill', 3564), ('scream', 3565), ('screen', 3566), ('screenshot', 3567), ('screw', 3568), ('scrub', 3569), ('scumbag', 3570), ('search', 3571), ('season', 3572), ('seat', 3573), ('second', 3574), ('secret', 3575), ('secret likabl', 3576), ('secret likabl everyon', 3577), ('secretli', 3578), ('section', 3579), ('secur', 3580), ('see', 3581), ('see child', 3582), ('see child porn', 3583), ('see faggot', 3584), ('see peopl', 3585), ('see perform', 3586), ('see perform citi', 3587), ('see thi', 3588), ('see u', 3589), ('see whi', 3590), ('see white', 3591), ('seek', 3592), ('seem', 3593), ('seem like', 3594), ('seen', 3595), ('select', 3596), ('self', 3597), ('self made', 3598), ('self made nigga', 3599), ('selfi', 3600), ('selfish', 3601), ('sell', 3602), ('send', 3603), ('senior', 3604), ('sens', 3605), ('sensit', 3606), ('sent', 3607), ('separ', 3608), ('seri', 3609), ('seriou', 3610), ('serious', 3611), ('seriu', 3612), ('serv', 3613), ('servic', 3614), ('session', 3615), ('set', 3616), ('set fire', 3617), ('settl', 3618), ('sever', 3619), ('sex', 3620), ('sexi', 3621), ('sexist', 3622), ('sexual', 3623), ('sh', 3624), ('shadi', 3625), ('shake', 3626), ('shall', 3627), ('shame', 3628), ('shape', 3629), ('share', 3630), ('shave', 3631), ('shawti', 3632), ('sheep', 3633), ('shift', 3634), ('ship', 3635), ('shirt', 3636), ('shit', 3637), ('shit fuck', 3638), ('shit get', 3639), ('shit go', 3640), ('shit like', 3641), ('shit make', 3642), ('shit talk', 3643), ('shit u', 3644), ('shit wa', 3645), ('shitti', 3646), ('shock', 3647), ('shoe', 3648), ('shone', 3649), ('shoot', 3650), ('shoot dat', 3651), ('shoot dat nigga', 3652), ('shoot nigga', 3653), ('shoot nigga hi', 3654), ('shoot towel', 3655), ('shoot towel head', 3656), ('shoot unarm', 3657), ('shop', 3658), ('short', 3659), ('shortag', 3660), ('shorti', 3661), ('shorti bitch', 3662), ('shot', 3663), ('shoulder', 3664), ('shout', 3665), ('shout allahu', 3666), ('shout allahu akbar', 3667), ('show', 3668), ('shower', 3669), ('shut', 3670), ('shut fuck', 3671), ('shut fuck faggot', 3672), ('shut fuck nigger', 3673), ('shut nigger', 3674), ('shut nigger get', 3675), ('si', 3676), ('sick', 3677), ('side', 3678), ('sight', 3679), ('sign', 3680), ('silenc', 3681), ('silent', 3682), ('silk', 3683), ('silli', 3684), ('silver', 3685), ('simpl', 3686), ('sin', 3687), ('sinc', 3688), ('sinc quit', 3689), ('sinc quit life', 3690), ('sing', 3691), ('singl', 3692), ('sir', 3693), ('sissi', 3694), ('sister', 3695), ('sit', 3696), ('site', 3697), ('situat', 3698), ('six', 3699), ('size', 3700), ('skill', 3701), ('skin', 3702), ('skinni', 3703), ('skirt', 3704), ('sky', 3705), ('skype', 3706), ('slag', 3707), ('slam', 3708), ('slander', 3709), ('slang', 3710), ('slang term', 3711), ('slap', 3712), ('slaughter', 3713), ('slave', 3714), ('slave ship', 3715), ('slay', 3716), ('sleep', 3717), ('sleev', 3718), ('slide', 3719), ('slip', 3720), ('slow', 3721), ('slur', 3722), ('slut', 3723), ('smack', 3724), ('small', 3725), ('smart', 3726), ('smash', 3727), ('smell', 3728), ('smell fag', 3729), ('smell like', 3730), ('smfh', 3731), ('smh', 3732), ('smile', 3733), ('smith', 3734), ('smoke', 3735), ('snake', 3736), ('snap', 3737), ('snapchat', 3738), ('sneak', 3739), ('snow', 3740), ('soak', 3741), ('social', 3742), ('social media', 3743), ('societi', 3744), ('sock', 3745), ('soft', 3746), ('sold', 3747), ('solid', 3748), ('solo', 3749), ('solut', 3750), ('somebodi', 3751), ('somebodi get', 3752), ('somehow', 3753), ('someon', 3754), ('someon call', 3755), ('someon els', 3756), ('someon get', 3757), ('someon love', 3758), ('someth', 3759), ('someth wrong', 3760), ('sometim', 3761), ('somewher', 3762), ('son', 3763), ('son bitch', 3764), ('song', 3765), ('song wan', 3766), ('song wan na', 3767), ('soon', 3768), ('soon tag', 3769), ('soon tag promot', 3770), ('sooo', 3771), ('soooo', 3772), ('sorri', 3773), ('sorri bitch', 3774), ('sorri bitch shoot', 3775), ('sort', 3776), ('sosonellen', 3777), ('sosonellen four', 3778), ('sosonellen four vine', 3779), ('soul', 3780), ('sound', 3781), ('sound like', 3782), ('sound like faggot', 3783), ('sound like need', 3784), ('soundcloud', 3785), ('sourc', 3786), ('south', 3787), ('southern', 3788), ('space', 3789), ('spam', 3790), ('spanish', 3791), ('speak', 3792), ('speaker', 3793), ('speaker decentr', 3794), ('speaker decentr power', 3795), ('spear', 3796), ('spear chucker', 3797), ('special', 3798), ('specif', 3799), ('speed', 3800), ('spell', 3801), ('spend', 3802), ('spent', 3803), ('spic', 3804), ('spin', 3805), ('spirit', 3806), ('spit', 3807), ('spoil', 3808), ('spoiler', 3809), ('spoiler air', 3810), ('spoiler air horn', 3811), ('spoke', 3812), ('spoken', 3813), ('spoken like', 3814), ('spoken like true', 3815), ('sport', 3816), ('spot', 3817), ('spread', 3818), ('spread word', 3819), ('squad', 3820), ('squar', 3821), ('st', 3822), ('stab', 3823), ('stack', 3824), ('stadium', 3825), ('staff', 3826), ('stage', 3827), ('stalk', 3828), ('stan', 3829), ('stand', 3830), ('stand bitch', 3831), ('stand bitch made', 3832), ('stand shit', 3833), ('star', 3834), ('star war', 3835), ('starbuck', 3836), ('stare', 3837), ('start', 3838), ('start bottom', 3839), ('start bottom remix', 3840), ('start get', 3841), ('start new', 3842), ('start shit', 3843), ('stat', 3844), ('stat use', 3845), ('stat use grow', 3846), ('state', 3847), ('state governor', 3848), ('state governor right', 3849), ('statement', 3850), ('station', 3851), ('statist', 3852), ('statu', 3853), ('stay', 3854), ('stay kill', 3855), ('stay kill stor', 3856), ('stay well', 3857), ('stay well pal', 3858), ('steal', 3859), ('steam', 3860), ('steer', 3861), ('step', 3862), ('stereotyp', 3863), ('stereotyp come', 3864), ('stereotyp come justwaitonit', 3865), ('sterl', 3866), ('sterl silver', 3867), ('steve', 3868), ('steve harvey', 3869), ('steve hater', 3870), ('steve hater call', 3871), ('steveharvey', 3872), ('stfu', 3873), ('stick', 3874), ('still', 3875), ('still believ', 3876), ('still crack', 3877), ('still fag', 3878), ('still faggot', 3879), ('still faggot tho', 3880), ('still fuck', 3881), ('still get', 3882), ('still know', 3883), ('stink', 3884), ('stitch', 3885), ('stock', 3886), ('stole', 3887), ('stomach', 3888), ('stone', 3889), ('stop', 3890), ('stop act', 3891), ('stop act like', 3892), ('stop coon', 3893), ('stop coon shit', 3894), ('stop cuffin', 3895), ('stop cuffin hoe', 3896), ('stop faggot', 3897), ('stop major', 3898), ('stop major attack', 3899), ('stop say', 3900), ('stop sell', 3901), ('stor', 3902), ('store', 3903), ('store alexfromtarget', 3904), ('store alexfromtarget vote', 3905), ('store jk', 3906), ('store start', 3907), ('store start bottom', 3908), ('store ur', 3909), ('store ur life', 3910), ('store vine', 3911), ('store vine funni', 3912), ('stori', 3913), ('straight', 3914), ('stream', 3915), ('street', 3916), ('street blast', 3917), ('street blast chink', 3918), ('street nigga', 3919), ('strength', 3920), ('stress', 3921), ('strike', 3922), ('strip', 3923), ('stripper', 3924), ('strong', 3925), ('struggl', 3926), ('strung', 3927), ('stubborn', 3928), ('stuck', 3929), ('stud', 3930), ('student', 3931), ('studi', 3932), ('studio', 3933), ('stuff', 3934), ('stupid', 3935), ('stupid fuck', 3936), ('stupid nigger', 3937), ('style', 3938), ('submit', 3939), ('subscrib', 3940), ('subway', 3941), ('success', 3942), ('suck', 3943), ('suck dick', 3944), ('sucker', 3945), ('suddenli', 3946), ('sugar', 3947), ('suggest', 3948), ('suit', 3949), ('sum', 3950), ('summer', 3951), ('sun', 3952), ('sunday', 3953), ('super', 3954), ('superior', 3955), ('suppli', 3956), ('support', 3957), ('suppos', 3958), ('supremacist', 3959), ('sure', 3960), ('surpris', 3961), ('surviv', 3962), ('suspect', 3963), ('suspend', 3964), ('swear', 3965), ('swear fuck', 3966), ('sweat', 3967), ('sweater', 3968), ('sweden', 3969), ('sweep', 3970), ('sweet', 3971), ('sweeti', 3972), ('swing', 3973), ('switch', 3974), ('symbol', 3975), ('syria', 3976), ('system', 3977), ('ta', 3978), ('ta get', 3979), ('tabl', 3980), ('tag', 3981), ('tag promot', 3982), ('taiji', 3983), ('take', 3984), ('take back', 3985), ('take care', 3986), ('taken', 3987), ('talent', 3988), ('talk', 3989), ('talk abt', 3990), ('talk bout', 3991), ('talk hi', 3992), ('talk like', 3993), ('talk real', 3994), ('talk shit', 3995), ('talkin', 3996), ('tall', 3997), ('tampa', 3998), ('tape', 3999), ('target', 4000), ('tast', 4001), ('tax', 4002), ('taylor', 4003), ('tbh', 4004), ('tbyg', 4005), ('tcot', 4006), ('tea', 4007), ('teabagg', 4008), ('teach', 4009), ('teacher', 4010), ('team', 4011), ('tear', 4012), ('teas', 4013), ('teas tv', 4014), ('teas tv ha', 4015), ('tech', 4016), ('tech gift', 4017), ('tech gift guid', 4018), ('tech gift idea', 4019), ('ted', 4020), ('ted cruz', 4021), ('teen', 4022), ('teenag', 4023), ('teeth', 4024), ('tell', 4025), ('tell peopl', 4026), ('tell u', 4027), ('tell u bout', 4028), ('tell us', 4029), ('temper', 4030), ('ten', 4031), ('term', 4032), ('terri', 4033), ('terribl', 4034), ('terror', 4035), ('terrorist', 4036), ('terrorist attack', 4037), ('terrorist home', 4038), ('terrorist pol', 4039), ('terrorist pol see', 4040), ('terrorist war', 4041), ('terrorist war hundr', 4042), ('test', 4043), ('texa', 4044), ('text', 4045), ('text faggot', 4046), ('tf', 4047), ('th', 4048), ('tha', 4049), ('thank', 4050), ('thank everyth', 4051), ('thank everyth deserv', 4052), ('thank follow', 4053), ('thank follow wish', 4054), ('thank god', 4055), ('thank god everi', 4056), ('thank u', 4057), ('theater', 4058), ('theme', 4059), ('themselv', 4060), ('theori', 4061), ('theyr', 4062), ('thi', 4063), ('thi behavior', 4064), ('thi behavior could', 4065), ('thi bitch', 4066), ('thi bitch made', 4067), ('thi book', 4068), ('thi boy', 4069), ('thi christma', 4070), ('thi coon', 4071), ('thi coon shit', 4072), ('thi countri', 4073), ('thi dude', 4074), ('thi dumb', 4075), ('thi faggot', 4076), ('thi fuck', 4077), ('thi fuckin', 4078), ('thi game', 4079), ('thi girl', 4080), ('thi guy', 4081), ('thi happen', 4082), ('thi job', 4083), ('thi kid', 4084), ('thi littl', 4085), ('thi man', 4086), ('thi morn', 4087), ('thi new', 4088), ('thi newwav', 4089), ('thi newwav click', 4090), ('thi nigga', 4091), ('thi nigga hi', 4092), ('thi nigger', 4093), ('thi one', 4094), ('thi pictur', 4095), ('thi place', 4096), ('thi planet', 4097), ('thi playlist', 4098), ('thi playlist amaz', 4099), ('thi shit', 4100), ('thi show', 4101), ('thi time', 4102), ('thi tweet', 4103), ('thi video', 4104), ('thi wa', 4105), ('thi way', 4106), ('thi week', 4107), ('thi whi', 4108), ('thi white', 4109), ('thi white bitch', 4110), ('thi whole', 4111), ('thi world', 4112), ('thi year', 4113), ('thick', 4114), ('thigh', 4115), ('thing', 4116), ('thing go', 4117), ('thing like', 4118), ('thing say', 4119), ('think', 4120), ('think alik', 4121), ('think could', 4122), ('think go', 4123), ('think need', 4124), ('think peopl', 4125), ('think thi', 4126), ('think wa', 4127), ('third', 4128), ('tho', 4129), ('tho lol', 4130), ('thoma', 4131), ('thot', 4132), ('though', 4133), ('thought', 4134), ('thought wa', 4135), ('thousand', 4136), ('thousand tortur', 4137), ('thousand tortur mutil', 4138), ('thread', 4139), ('threat', 4140), ('threaten', 4141), ('three', 4142), ('three cummi', 4143), ('three cummi four', 4144), ('three year', 4145), ('throw', 4146), ('thrown', 4147), ('thru', 4148), ('tht', 4149), ('thug', 4150), ('thumb', 4151), ('thx', 4152), ('ti', 4153), ('ticket', 4154), ('tie', 4155), ('tie dye', 4156), ('tie knot', 4157), ('til', 4158), ('till', 4159), ('time', 4160), ('time bitch', 4161), ('time bitch made', 4162), ('time get', 4163), ('time see', 4164), ('timelin', 4165), ('tini', 4166), ('tip', 4167), ('tire', 4168), ('tit', 4169), ('titl', 4170), ('tl', 4171), ('today', 4172), ('toe', 4173), ('togeth', 4174), ('told', 4175), ('told us', 4176), ('told wa', 4177), ('tom', 4178), ('tomorrow', 4179), ('ton', 4180), ('ton android', 4181), ('ton android tech', 4182), ('tone', 4183), ('tonight', 4184), ('took', 4185), ('took hi', 4186), ('tool', 4187), ('top', 4188), ('top beach', 4189), ('top beach central', 4190), ('tori', 4191), ('tortur', 4192), ('tortur mutil', 4193), ('tortur mutil corps', 4194), ('total', 4195), ('touch', 4196), ('tour', 4197), ('toward', 4198), ('towel', 4199), ('towel head', 4200), ('towel head ak', 4201), ('town', 4202), ('toy', 4203), ('track', 4204), ('track jihadist', 4205), ('trade', 4206), ('trade forex', 4207), ('trade forex invest', 4208), ('tradit', 4209), ('traffic', 4210), ('trailer', 4211), ('trailer park', 4212), ('trailer park boy', 4213), ('train', 4214), ('traitor', 4215), ('transgend', 4216), ('translat', 4217), ('trap', 4218), ('trash', 4219), ('trash blue', 4220), ('trash blue collar', 4221), ('travel', 4222), ('treat', 4223), ('treat like', 4224), ('treatment', 4225), ('tree', 4226), ('tri', 4227), ('tri get', 4228), ('tri get job', 4229), ('tri hard', 4230), ('tri make', 4231), ('tri tell', 4232), ('trick', 4233), ('trip', 4234), ('troll', 4235), ('troubl', 4236), ('troy', 4237), ('truck', 4238), ('true', 4239), ('truli', 4240), ('truli amiabl', 4241), ('truli amiabl day', 4242), ('trump', 4243), ('trump support', 4244), ('trust', 4245), ('truth', 4246), ('tryna', 4247), ('tryna get', 4248), ('ts', 4249), ('tuesday', 4250), ('tumblr', 4251), ('tune', 4252), ('turkey', 4253), ('turn', 4254), ('turn around', 4255), ('tv', 4256), ('tv ha', 4257), ('tv ha final', 4258), ('tv year', 4259), ('tv year year', 4260), ('twat', 4261), ('tweet', 4262), ('tweet thi', 4263), ('twice', 4264), ('twin', 4265), ('twist', 4266), ('twitter', 4267), ('two', 4268), ('two cummi', 4269), ('two cummi three', 4270), ('two terrorist', 4271), ('two terrorist home', 4272), ('ty', 4273), ('tyler', 4274), ('type', 4275), ('typic', 4276), ('tyson', 4277), ('u', 4278), ('u alway', 4279), ('u bout', 4280), ('u bout get', 4281), ('u dumb', 4282), ('u fag', 4283), ('u faggot', 4284), ('u fuck', 4285), ('u fuckin', 4286), ('u fuckin seriu', 4287), ('u fuckin wit', 4288), ('u get', 4289), ('u go', 4290), ('u got', 4291), ('u kno', 4292), ('u know', 4293), ('u look', 4294), ('u never', 4295), ('u nigga', 4296), ('u r', 4297), ('u right', 4298), ('u say', 4299), ('u still', 4300), ('u talk', 4301), ('u talk bout', 4302), ('u think', 4303), ('u want', 4304), ('ugh', 4305), ('ugli', 4306), ('ugli ass', 4307), ('ugli dyke', 4308), ('uh', 4309), ('uk', 4310), ('ultim', 4311), ('ultim holiday', 4312), ('ultim holiday tech', 4313), ('um', 4314), ('un', 4315), ('unarm', 4316), ('unblock', 4317), ('uncl', 4318), ('uncomfort', 4319), ('understand', 4320), ('understood', 4321), ('uneduc', 4322), ('unemploy', 4323), ('unemploy high', 4324), ('unemploy high school', 4325), ('unfollow', 4326), ('uniform', 4327), ('uniqu', 4328), ('unit', 4329), ('univers', 4330), ('unless', 4331), ('updat', 4332), ('upon', 4333), ('upset', 4334), ('ur', 4335), ('ur fag', 4336), ('ur faggot', 4337), ('ur fuck', 4338), ('ur fuck queer', 4339), ('ur life', 4340), ('ur life sinc', 4341), ('ur nigger', 4342), ('ur nigger ass', 4343), ('ur still', 4344), ('us', 4345), ('us know', 4346), ('us nigger', 4347), ('usa', 4348), ('use', 4349), ('use grow', 4350), ('use grow account', 4351), ('use word', 4352), ('user', 4353), ('usual', 4354), ('v', 4355), ('va', 4356), ('valu', 4357), ('vampir', 4358), ('van', 4359), ('van dyke', 4360), ('van gaal', 4361), ('vape', 4362), ('vega', 4363), ('vehicl', 4364), ('veri', 4365), ('veri good', 4366), ('version', 4367), ('veteran', 4368), ('via', 4369), ('via r', 4370), ('vibe', 4371), ('vice', 4372), ('victim', 4373), ('vid', 4374), ('video', 4375), ('video black', 4376), ('video black op', 4377), ('video mani', 4378), ('video mani nigger', 4379), ('video playlist', 4380), ('video thi', 4381), ('video thi playlist', 4382), ('view', 4383), ('view mirror', 4384), ('view mirror cop', 4385), ('vine', 4386), ('vine funni', 4387), ('vine funni vine', 4388), ('vine mani', 4389), ('vine mani nigger', 4390), ('vintag', 4391), ('virgin', 4392), ('virginia', 4393), ('virtual', 4394), ('visa', 4395), ('visibl', 4396), ('vision', 4397), ('visit', 4398), ('voic', 4399), ('vote', 4400), ('vote sosonellen', 4401), ('vote sosonellen four', 4402), ('voter', 4403), ('vow', 4404), ('vs', 4405), ('w', 4406), ('wa', 4407), ('wa born', 4408), ('wa call', 4409), ('wa faggot', 4410), ('wa first', 4411), ('wa forc', 4412), ('wa fuck', 4413), ('wa fun', 4414), ('wa go', 4415), ('wa gon', 4416), ('wa gon na', 4417), ('wa good', 4418), ('wa like', 4419), ('wa littl', 4420), ('wa look', 4421), ('wa marri', 4422), ('wa one', 4423), ('wa onli', 4424), ('wa rape', 4425), ('wa rape brother', 4426), ('wa real', 4427), ('wa right', 4428), ('wa white', 4429), ('wa wrong', 4430), ('wack', 4431), ('wage', 4432), ('wage feder', 4433), ('wage feder govern', 4434), ('wage fg', 4435), ('wage fg feder', 4436), ('wage ngige', 4437), ('wage ngige via', 4438), ('waist', 4439), ('wait', 4440), ('wait see', 4441), ('wake', 4442), ('wake like', 4443), ('walk', 4444), ('wall', 4445), ('wallet', 4446), ('walmart', 4447), ('wan', 4448), ('wan na', 4449), ('wan na get', 4450), ('wan na go', 4451), ('wan na kill', 4452), ('wan na know', 4453), ('wan na see', 4454), ('wanker', 4455), ('want', 4456), ('want christma', 4457), ('want fuck', 4458), ('want get', 4459), ('want go', 4460), ('want kill', 4461), ('want know', 4462), ('want like', 4463), ('want like quora', 4464), ('want make', 4465), ('want meet', 4466), ('want meet girl', 4467), ('want nigger', 4468), ('want one', 4469), ('want say', 4470), ('want see', 4471), ('want take', 4472), ('want thi', 4473), ('want us', 4474), ('want white', 4475), ('war', 4476), ('war hundr', 4477), ('war hundr thousand', 4478), ('warm', 4479), ('warn', 4480), ('wash', 4481), ('washington', 4482), ('wasnt', 4483), ('wast', 4484), ('watch', 4485), ('watch back', 4486), ('watch favorit', 4487), ('watch mouth', 4488), ('water', 4489), ('wave', 4490), ('way', 4491), ('way get', 4492), ('wb', 4493), ('weak', 4494), ('wealth', 4495), ('wear', 4496), ('wear bag', 4497), ('wear bag head', 4498), ('wear paper', 4499), ('wear paper bag', 4500), ('wear plastic', 4501), ('wear plastic bag', 4502), ('weather', 4503), ('websit', 4504), ('wed', 4505), ('weed', 4506), ('week', 4507), ('weekend', 4508), ('weight', 4509), ('weird', 4510), ('welcom', 4511), ('welfar', 4512), ('well', 4513), ('well fuck', 4514), ('well pal', 4515), ('well pal x', 4516), ('weloveyouashton', 4517), ('wen', 4518), ('went', 4519), ('went bad', 4520), ('west', 4521), ('wet', 4522), ('wetback', 4523), ('whatev', 4524), ('whenev', 4525), ('whether', 4526), ('whi', 4527), ('whi doe', 4528), ('whi fuck', 4529), ('whi fuck peopl', 4530), ('whi peopl', 4531), ('whi peopl like', 4532), ('whi thi', 4533), ('whi u', 4534), ('whi would', 4535), ('whip', 4536), ('whip slave', 4537), ('whip slave ship', 4538), ('whisper', 4539), ('white', 4540), ('white ass', 4541), ('white bitch', 4542), ('white bitch whip', 4543), ('white boy', 4544), ('white chocol', 4545), ('white christma', 4546), ('white dude', 4547), ('white folk', 4548), ('white girl', 4549), ('white gold', 4550), ('white guy', 4551), ('white hous', 4552), ('white kid', 4553), ('white lie', 4554), ('white male', 4555), ('white man', 4556), ('white men', 4557), ('white one', 4558), ('white peopl', 4559), ('white peopl live', 4560), ('white peopl white', 4561), ('white person', 4562), ('white ppl', 4563), ('white privileg', 4564), ('white racist', 4565), ('white supremacist', 4566), ('white trash', 4567), ('white trash blue', 4568), ('white women', 4569), ('whitegenocid', 4570), ('whoever', 4571), ('whole', 4572), ('whoop', 4573), ('whore', 4574), ('wide', 4575), ('wife', 4576), ('wifi', 4577), ('wild', 4578), ('william', 4579), ('win', 4580), ('wind', 4581), ('wind like', 4582), ('window', 4583), ('wine', 4584), ('wing', 4585), ('winner', 4586), ('winter', 4587), ('wise', 4588), ('wish', 4589), ('wish could', 4590), ('wish famili', 4591), ('wish famili merri', 4592), ('wish wa', 4593), ('wit', 4594), ('wit di', 4595), ('wit di song', 4596), ('wit fuck', 4597), ('wit fuck wit', 4598), ('wit lame', 4599), ('wit lame nigga', 4600), ('wit nigga', 4601), ('wit nigga fuck', 4602), ('within', 4603), ('without', 4604), ('woke', 4605), ('woke like', 4606), ('woke like thi', 4607), ('wolf', 4608), ('woman', 4609), ('women', 4610), ('women fan', 4611), ('women fan run', 4612), ('women get', 4613), ('women get rape', 4614), ('women men', 4615), ('women rape', 4616), ('wonder', 4617), ('wonder whi', 4618), ('wont', 4619), ('word', 4620), ('word nigger', 4621), ('wore', 4622), ('work', 4623), ('worker', 4624), ('workout', 4625), ('world', 4626), ('world full', 4627), ('worn', 4628), ('worri', 4629), ('wors', 4630), ('worship', 4631), ('worst', 4632), ('worth', 4633), ('worthless', 4634), ('would', 4635), ('would best', 4636), ('would best gift', 4637), ('would get', 4638), ('would go', 4639), ('would kill', 4640), ('would like', 4641), ('would love', 4642), ('would say', 4643), ('would support', 4644), ('wouldnt', 4645), ('wound', 4646), ('wow', 4647), ('wrap', 4648), ('wrap present', 4649), ('write', 4650), ('write book', 4651), ('writer', 4652), ('wrong', 4653), ('wrote', 4654), ('wtf', 4655), ('wtf steve', 4656), ('wtf steve hater', 4657), ('x', 4658), ('x lektriqu', 4659), ('x lektriqu religion', 4660), ('xbox', 4661), ('xbox one', 4662), ('xd', 4663), ('xma', 4664), ('xma christmasweek', 4665), ('xx', 4666), ('xxx', 4667), ('ya', 4668), ('yall', 4669), ('yard', 4670), ('yawn', 4671), ('ye', 4672), ('yea', 4673), ('yeah', 4674), ('year', 4675), ('year ago', 4676), ('year old', 4677), ('year old girl', 4678), ('year reveal', 4679), ('year reveal stori', 4680), ('year teas', 4681), ('year teas tv', 4682), ('year year', 4683), ('year year teas', 4684), ('yell', 4685), ('yellow', 4686), ('yellow gold', 4687), ('yesterday', 4688), ('yet', 4689), ('yet still', 4690), ('yiddish', 4691), ('yo', 4692), ('yo ass', 4693), ('yogi', 4694), ('yogi berra', 4695), ('yogi berra yogiberrarip', 4696), ('yogiberrarip', 4697), ('york', 4698), ('young', 4699), ('young thug', 4700), ('younger', 4701), ('yourselv', 4702), ('youtub', 4703), ('yr', 4704), ('yr old', 4705), ('yu', 4706), ('zen', 4707), ('zen sausag', 4708), ('zen sausag make', 4709), ('zombi', 4710)])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#Get POS tags for tweets and save as a string\n",
    "#more details https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b\n",
    "tweet_tags = []\n",
    "for t in tweets:\n",
    "    tokens = basic_tokenize(preprocess(t))\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tag_list = [x[1] for x in tags]\n",
    "    #for i in range(0, len(tokens)):\n",
    "    tag_str = \" \".join(tag_list)\n",
    "    tweet_tags.append(tag_str)\n",
    "        #print(tokens[i],tag_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the TFIDF vectorizer to get a token matrix for the POS tags\n",
    "pos_vectorizer = TfidfVectorizer(\n",
    "    #vectorizer = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words=None, #We do better when we keep stopwords\n",
    "    use_idf=False,\n",
    "    smooth_idf=False,\n",
    "    norm=None, #Applies l2 norm smoothing\n",
    "    decode_error='replace',\n",
    "    max_features=5000,\n",
    "    min_df=5,\n",
    "    max_df=0.501,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct POS TF matrix and get vocab dict\n",
    "pos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\n",
    "pos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now get other features\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "from textstat.textstat import *\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "def count_twitter_objs(text_string):\n",
    "    \"\"\"\n",
    "    Accepts a text string and replaces:\n",
    "    1) urls with URLHERE\n",
    "    2) lots of whitespace with one instance\n",
    "    3) mentions with MENTIONHERE\n",
    "    4) hashtags with HASHTAGHERE\n",
    "\n",
    "    This allows us to get standardized counts of urls and mentions\n",
    "    Without caring about specific people mentioned.\n",
    "    \n",
    "    Returns counts of urls, mentions, and hashtags.\n",
    "    \"\"\"\n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', text_string)\n",
    "    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n",
    "    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n",
    "    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n",
    "\n",
    "def other_features(tweet):\n",
    "    \"\"\"This function takes a string and returns a list of features.\n",
    "    These include Sentiment scores, Text and Readability scores,\n",
    "    as well as Twitter specific features\"\"\"\n",
    "    ##SENTIMENT\n",
    "    sentiment = sentiment_analyzer.polarity_scores(tweet)\n",
    "    \n",
    "    words = preprocess(tweet) #Get text only\n",
    "    \n",
    "    syllables = textstat.syllable_count(words) #count syllables in words\n",
    "    num_chars = sum(len(w) for w in words) #num chars in words\n",
    "    num_chars_total = len(tweet)\n",
    "    num_terms = len(tweet.split())\n",
    "    num_words = len(words.split())\n",
    "    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n",
    "    num_unique_terms = len(set(words.split()))\n",
    "    \n",
    "    ###Modified FK grade, where avg words per sentence is just num words/1\n",
    "    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n",
    "    ##Modified FRE score, where sentence fixed to 1\n",
    "    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n",
    "    \n",
    "    twitter_objs = count_twitter_objs(tweet) #Count #, @, and http://\n",
    "    retweet = 0\n",
    "    if \"rt\" in words:\n",
    "        retweet = 1\n",
    "    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n",
    "                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n",
    "                twitter_objs[2], twitter_objs[1],\n",
    "                twitter_objs[0], retweet]\n",
    "    #features = pandas.DataFrame(features)\n",
    "    return features\n",
    "\n",
    "def get_feature_array(tweets):\n",
    "    feats=[]\n",
    "    for t in tweets:\n",
    "        feats.append(other_features(t))\n",
    "    return np.array(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n",
    "                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \"vader compound\", \\\n",
    "                        \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feature_array(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now join them all up\n",
    "M = np.concatenate([tfidf,pos,feats],axis=1)\n",
    "#M = np.concatenate([tfidf,pos],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14509, 8167)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally get a list of variable names\n",
    "variables = ['']*len(vocab)\n",
    "for k,v in vocab.items():\n",
    "    variables[v] = k\n",
    "\n",
    "pos_variables = ['']*len(pos_vocab)\n",
    "for k,v in pos_vocab.items():\n",
    "    pos_variables[v] = k\n",
    "\n",
    "feature_names = variables+pos_variables+other_features_names\n",
    "#feature_names = variables+pos_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "This model was found using a GridSearch with 5-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(M)\n",
    "y = df1['sentiment'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = SelectFromModel(LogisticRegression(class_weight='balanced',penalty=\"l1\",C=0.01))\n",
    "X_ = select.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(class_weight='balanced',C=0.01, penalty='l2', loss='squared_hinge',multi_class='ovr').fit(X_, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(X_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report( y, y_preds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.95      0.93      7274\n",
      "          1       0.76      0.64      0.70      4836\n",
      "          2       0.52      0.61      0.56      2399\n",
      "\n",
      "avg / total       0.80      0.79      0.79     14509\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word2Vec + Keras Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Word2Vec representation. \n",
    "\n",
    "\"Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.\" \n",
    "\n",
    "In Gensim package, you can specify whether to use CBOW or Skip-gram by passing the argument \"sg\" when implementing Word2Vec. By default (sg=0), CBOW is used. Otherwise (sg=1), skip-gram is employed. \n",
    "\n",
    "For example, let's say we have a following sentence: \"I love dogs\". CBOW model tries to predict the word \"love\" when given \"I\", \"dogs\" as inputs, on the other hand Skip-gram model tries to predict \"I\", \"dogs\" when given the word \"love\" as input.\n",
    "\n",
    "Below picture represents more formally how these two models work.\n",
    "\n",
    "<IMG SRC=\"https://cdn-images-1.medium.com/max/1600/1*6YmcrrGj1_wAmv0BQBarhw.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df.loc[perm[:train_end]]\n",
    "    validate = df.loc[perm[train_end:validate_end]]\n",
    "    test = df.loc[perm[validate_end:]]\n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = train_validate_test_split(df1)\n",
    "\n",
    "X_train = train_set[\"tweet\"]\n",
    "Y_train = train_set[\"sentiment\"]\n",
    "X_valid = valid_set[\"tweet\"]\n",
    "Y_valid = valid_set[\"sentiment\"]\n",
    "X_test = test_set[\"tweet\"]\n",
    "Y_test = test_set[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labelize each tweet with a unique id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_w2v = labelize_tweets_ug(df1[\"tweet\"], 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['your', 'a', 'dirty', 'terrorist', 'and', 'your', 'religion', 'is', 'a', 'fucking', 'joke', 'you', 'go', 'around', 'screaming', 'allah', 'akbar', 'doing', 'terrorist', 'shit', 'dirty', 'faggot'], tags=['all_5'])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_x_w2v[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of cores available in your system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cores = multiprocessing.cpu_count()\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use it to set how many workers we would like to run in parallel (feel free to use a smaller number)\n",
    "\n",
    "Now let's build the skip-gram model (note the \"sg=1\" argument in the Word2Vec call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 907432.66it/s]\n"
     ]
    }
   ],
   "source": [
    "WORD2VEC_SIZE = 100\n",
    "model_ug_sg = Word2Vec(sg=1, size=WORD2VEC_SIZE, negative=5, window=2, min_count=2, \n",
    "                       workers=num_cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x160a380a080>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ug_sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it for 30 epochs (iterations) using all the labelized inputs (all_x_w2v):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 1037138.81it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 967906.04it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 1037085.78it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 1116895.29it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 967906.04it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 1036944.41it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 1451870.61it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 967936.83it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 1319925.32it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "100%|ââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ| 14509/14509 [00:00<00:00, 967875.26it/s]\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), \n",
    "                      total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_sg.alpha -= 0.002\n",
    "    model_ug_sg.min_alpha = model_ug_sg.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the embeddings_index (word to embedding mapping, for easy lookup) using the model_ug_sg model results. Youmay append more than one model's results here (see homework questions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_sg.wv.vocab.keys():\n",
    "    embeddings_index[w] = model_ug_sg.wv[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the last \"w\" value to get the size for embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7482 word vectors of size 100\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_SIZE = len(embeddings_index[w])\n",
    "print('Found %s word vectors of size %s' % (len(embeddings_index), EMBEDDINGS_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the input tweets into sequence of word representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the the maximum number of words a tweet has in our training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH = 0\n",
    "for x in X_train:\n",
    "    x_length = len(x.split())\n",
    "    if x_length > MAX_LENGTH:\n",
    "        MAX_LENGTH = x_length\n",
    "\n",
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the maximum number of words in a tweet within the training data is 45. To be on the safe size, we will set the maximum length of the sequences to 50 (you may go higher, but that will take more space). We will pad the shorter sequences using the pad_sequences function from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data tensor: (8705, 35)\n"
     ]
    }
   ],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_seq = pad_sequences(sequences_train, maxlen=MAX_LENGTH)\n",
    "print('Shape of training data tensor:', X_train_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is now it looks like (by default, Keras places a zero-padding at the beginning of the sequence):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,  171,   18,  174,  147,  235,   71,  214,\n",
       "         290,  160],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  541,\n",
       "         203,    6,  161,   18,   11,  123,    3,  151,   12,   32,  137,\n",
       "           1,   96],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  256,\n",
       "          10,   15,   55,    8,  103, 5322, 1818,   11, 2124,    6, 2644,\n",
       "        5323,    9],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,   47, 2645,   48,  241,  154,  892,  280,\n",
       "          50,  155],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,   29,  112,\n",
       "          92, 5324,  175,  634, 1291,  679,   11,   64,   72,   61,  148,\n",
       "        5325,   89]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation data tensor: (2901, 35)\n"
     ]
    }
   ],
   "source": [
    "sequences_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "X_valid_seq = pad_sequences(sequences_valid, maxlen=MAX_LENGTH)\n",
    "print('Shape of validation data tensor:', X_valid_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and also the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data tensor: (2903, 35)\n"
     ]
    }
   ],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_seq = pad_sequences(sequences_test, maxlen=MAX_LENGTH)\n",
    "print('Shape of test data tensor:', X_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the size of the tokenizer word index:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate the embedding_matrix (the matrix of embeddings for all words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((NUM_WORDS, EMBEDDINGS_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= NUM_WORDS:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector # embedding vectors lenght is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0225263 , -0.2118109 ,  0.24931687,  0.01480665,  0.13254005,\n",
       "       -0.20635621, -0.01730832,  0.14405814, -0.09318416,  0.5671739 ,\n",
       "       -0.14281207,  0.2497136 ,  0.35093439,  0.26466158,  0.63157713,\n",
       "        0.15533064,  0.2381909 ,  0.06052995, -0.09094279, -0.29695177,\n",
       "        0.49278367, -0.19765331,  0.21745673,  0.05567987, -0.34864944,\n",
       "       -0.35937759,  0.6170122 , -0.07464511, -0.14967354, -0.17001393,\n",
       "       -0.19983515,  0.2378765 ,  0.10040831,  0.49678108, -0.0267651 ,\n",
       "       -0.28077805, -0.15237866, -0.12894255, -0.34791484, -0.14757393,\n",
       "       -0.18990237, -0.38726541, -0.02517139,  0.44125456, -0.48438194,\n",
       "        0.38597184,  0.07317051,  0.40041244,  0.26231167, -0.03688072,\n",
       "       -0.00854358, -0.21939629, -0.07347817,  0.17532299, -0.5194245 ,\n",
       "        0.04909679, -0.27985653,  0.16722509, -0.55433118, -0.02230527,\n",
       "        0.18727635,  0.18431625,  0.06054831,  0.15568703,  0.19964884,\n",
       "        0.01544949, -0.3564685 ,  0.16930853, -0.30353513,  0.65116674,\n",
       "        0.23537126,  0.27921966, -0.63488197, -0.25652677,  0.40573847,\n",
       "        0.26660827, -0.13325366, -0.89231306, -0.24141598,  0.18429209,\n",
       "        0.47004765,  0.13968273,  0.13366909,  0.09726414,  0.17787237,\n",
       "        0.40009278, -0.33791721, -0.14619155, -0.25116366, -0.17741111,\n",
       "       -0.37445527,  0.12648319,  0.50686759,  0.13036574,  0.11193072,\n",
       "        0.33161479, -0.38563427,  0.46913996, -0.34360597, -0.12648481])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to build our CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the embedding_matrix to initialize the weights of the Embedding layer called \"tweet_encoder\". We don't want to use a pre-trained one, as it would be generic and may not fit the specific (tweet data we have). Also we don't want to train one from scratch, because it may overfit our training data.\n",
    "\n",
    "We will create one dimensional convolutional layers for bigrams, trigrams and fourgrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 35, 100)      10000000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 34, 100)      20100       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 33, 100)      30100       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 32, 100)      40100       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 100)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 100)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 100)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 300)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          77056       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            257         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,167,613\n",
      "Trainable params: 10,167,613\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tweet_input = Input(shape=(MAX_LENGTH,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(NUM_WORDS, EMBEDDINGS_SIZE, weights=[embedding_matrix], input_length=MAX_LENGTH, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=100, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[tweet_input], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the training set. Please uncomment an use the original values when you have enough time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the dataset to learn from\n",
    "CUSTOM_TRAINING_SET_SIZE = 600000 # X_train_seq.shape[0]\n",
    "CUSTOM_VALIDATION_SET_SIZE = 300000 # X_valid_seq.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the training for 5 epochs (iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start : 2019-01-03 01:40:00\n",
      "Train on 8705 samples, validate on 2901 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6464/8705 [=====================>........] - ETA: 13:23 - loss: 0.7559 - acc: 0.40 - ETA: 7:18 - loss: 0.5846 - acc: 0.4375 - ETA: 5:14 - loss: 0.5092 - acc: 0.447 - ETA: 4:12 - loss: 0.4192 - acc: 0.421 - ETA: 3:35 - loss: 0.6269 - acc: 0.393 - ETA: 3:10 - loss: 0.6586 - acc: 0.406 - ETA: 2:52 - loss: 0.6529 - acc: 0.397 - ETA: 2:39 - loss: 0.6121 - acc: 0.394 - ETA: 2:28 - loss: 0.6059 - acc: 0.388 - ETA: 2:20 - loss: 0.5970 - acc: 0.415 - ETA: 2:13 - loss: 0.5957 - acc: 0.443 - ETA: 2:08 - loss: 0.6059 - acc: 0.450 - ETA: 2:03 - loss: 0.6279 - acc: 0.451 - ETA: 1:59 - loss: 0.6328 - acc: 0.466 - ETA: 1:55 - loss: 0.6280 - acc: 0.481 - ETA: 1:52 - loss: 0.6274 - acc: 0.478 - ETA: 1:49 - loss: 0.6175 - acc: 0.481 - ETA: 1:47 - loss: 0.6142 - acc: 0.482 - ETA: 1:44 - loss: 0.6230 - acc: 0.473 - ETA: 1:42 - loss: 0.6122 - acc: 0.468 - ETA: 1:40 - loss: 0.6005 - acc: 0.470 - ETA: 1:38 - loss: 0.6036 - acc: 0.468 - ETA: 1:36 - loss: 0.5906 - acc: 0.472 - ETA: 1:35 - loss: 0.5832 - acc: 0.474 - ETA: 1:33 - loss: 0.5731 - acc: 0.476 - ETA: 1:32 - loss: 0.5633 - acc: 0.485 - ETA: 1:30 - loss: 0.5507 - acc: 0.491 - ETA: 1:29 - loss: 0.5484 - acc: 0.496 - ETA: 1:28 - loss: 0.5382 - acc: 0.492 - ETA: 1:27 - loss: 0.5274 - acc: 0.492 - ETA: 1:26 - loss: 0.5131 - acc: 0.496 - ETA: 1:25 - loss: 0.5083 - acc: 0.501 - ETA: 1:24 - loss: 0.4983 - acc: 0.500 - ETA: 1:23 - loss: 0.5068 - acc: 0.497 - ETA: 1:23 - loss: 0.5020 - acc: 0.497 - ETA: 1:22 - loss: 0.4938 - acc: 0.494 - ETA: 1:21 - loss: 0.4919 - acc: 0.489 - ETA: 1:20 - loss: 0.4879 - acc: 0.491 - ETA: 1:19 - loss: 0.4744 - acc: 0.492 - ETA: 1:19 - loss: 0.4636 - acc: 0.495 - ETA: 1:18 - loss: 0.4537 - acc: 0.497 - ETA: 1:17 - loss: 0.4502 - acc: 0.503 - ETA: 1:17 - loss: 0.4408 - acc: 0.505 - ETA: 1:16 - loss: 0.4382 - acc: 0.509 - ETA: 1:15 - loss: 0.4312 - acc: 0.511 - ETA: 1:15 - loss: 0.4261 - acc: 0.512 - ETA: 1:14 - loss: 0.4184 - acc: 0.513 - ETA: 1:13 - loss: 0.4068 - acc: 0.518 - ETA: 1:13 - loss: 0.3932 - acc: 0.519 - ETA: 1:13 - loss: 0.3939 - acc: 0.520 - ETA: 1:12 - loss: 0.3765 - acc: 0.521 - ETA: 1:11 - loss: 0.3627 - acc: 0.522 - ETA: 1:11 - loss: 0.3495 - acc: 0.520 - ETA: 1:10 - loss: 0.3331 - acc: 0.523 - ETA: 1:10 - loss: 0.3241 - acc: 0.528 - ETA: 1:09 - loss: 0.3130 - acc: 0.531 - ETA: 1:09 - loss: 0.3094 - acc: 0.531 - ETA: 1:08 - loss: 0.3120 - acc: 0.531 - ETA: 1:08 - loss: 0.3091 - acc: 0.531 - ETA: 1:07 - loss: 0.2956 - acc: 0.534 - ETA: 1:07 - loss: 0.2835 - acc: 0.537 - ETA: 1:06 - loss: 0.2601 - acc: 0.536 - ETA: 1:06 - loss: 0.2393 - acc: 0.538 - ETA: 1:05 - loss: 0.2307 - acc: 0.540 - ETA: 1:05 - loss: 0.2304 - acc: 0.541 - ETA: 1:05 - loss: 0.2121 - acc: 0.544 - ETA: 1:04 - loss: 0.2013 - acc: 0.543 - ETA: 1:04 - loss: 0.1903 - acc: 0.544 - ETA: 1:03 - loss: 0.1911 - acc: 0.547 - ETA: 1:03 - loss: 0.1662 - acc: 0.548 - ETA: 1:02 - loss: 0.1671 - acc: 0.550 - ETA: 1:02 - loss: 0.1545 - acc: 0.549 - ETA: 1:01 - loss: 0.1466 - acc: 0.550 - ETA: 1:01 - loss: 0.1263 - acc: 0.550 - ETA: 1:01 - loss: 0.1247 - acc: 0.552 - ETA: 1:00 - loss: 0.1002 - acc: 0.552 - ETA: 1:00 - loss: 0.0943 - acc: 0.554 - ETA: 59s - loss: 0.0945 - acc: 0.555 - ETA: 59s - loss: 0.0787 - acc: 0.55 - ETA: 59s - loss: 0.0608 - acc: 0.55 - ETA: 58s - loss: 0.0525 - acc: 0.56 - ETA: 58s - loss: 0.0448 - acc: 0.56 - ETA: 57s - loss: 0.0399 - acc: 0.56 - ETA: 57s - loss: 0.0137 - acc: 0.57 - ETA: 57s - loss: -0.0301 - acc: 0.572 - ETA: 56s - loss: -0.0501 - acc: 0.571 - ETA: 56s - loss: -0.0516 - acc: 0.573 - ETA: 56s - loss: -0.0697 - acc: 0.573 - ETA: 55s - loss: -0.0575 - acc: 0.572 - ETA: 55s - loss: -0.0765 - acc: 0.572 - ETA: 55s - loss: -0.0828 - acc: 0.572 - ETA: 54s - loss: -0.1065 - acc: 0.572 - ETA: 54s - loss: -0.0974 - acc: 0.572 - ETA: 54s - loss: -0.1027 - acc: 0.573 - ETA: 53s - loss: -0.1330 - acc: 0.574 - ETA: 53s - loss: -0.1501 - acc: 0.576 - ETA: 52s - loss: -0.1707 - acc: 0.578 - ETA: 52s - loss: -0.2010 - acc: 0.578 - ETA: 52s - loss: -0.2131 - acc: 0.579 - ETA: 52s - loss: -0.2287 - acc: 0.581 - ETA: 51s - loss: -0.2277 - acc: 0.583 - ETA: 51s - loss: -0.2377 - acc: 0.583 - ETA: 51s - loss: -0.2526 - acc: 0.585 - ETA: 50s - loss: -0.2649 - acc: 0.587 - ETA: 50s - loss: -0.2787 - acc: 0.588 - ETA: 49s - loss: -0.2765 - acc: 0.589 - ETA: 49s - loss: -0.2864 - acc: 0.589 - ETA: 49s - loss: -0.3101 - acc: 0.590 - ETA: 48s - loss: -0.3306 - acc: 0.591 - ETA: 48s - loss: -0.3283 - acc: 0.593 - ETA: 48s - loss: -0.3220 - acc: 0.594 - ETA: 48s - loss: -0.3230 - acc: 0.595 - ETA: 47s - loss: -0.3240 - acc: 0.597 - ETA: 47s - loss: -0.3298 - acc: 0.597 - ETA: 47s - loss: -0.3315 - acc: 0.599 - ETA: 46s - loss: -0.3467 - acc: 0.599 - ETA: 46s - loss: -0.3478 - acc: 0.601 - ETA: 46s - loss: -0.3481 - acc: 0.602 - ETA: 46s - loss: -0.3535 - acc: 0.604 - ETA: 45s - loss: -0.3583 - acc: 0.605 - ETA: 45s - loss: -0.3553 - acc: 0.607 - ETA: 45s - loss: -0.3836 - acc: 0.607 - ETA: 44s - loss: -0.4017 - acc: 0.608 - ETA: 44s - loss: -0.4040 - acc: 0.609 - ETA: 44s - loss: -0.4190 - acc: 0.610 - ETA: 43s - loss: -0.4194 - acc: 0.611 - ETA: 43s - loss: -0.4322 - acc: 0.611 - ETA: 43s - loss: -0.4472 - acc: 0.612 - ETA: 42s - loss: -0.4551 - acc: 0.613 - ETA: 42s - loss: -0.4667 - acc: 0.614 - ETA: 42s - loss: -0.4735 - acc: 0.614 - ETA: 42s - loss: -0.4936 - acc: 0.613 - ETA: 41s - loss: -0.5046 - acc: 0.614 - ETA: 41s - loss: -0.5096 - acc: 0.615 - ETA: 41s - loss: -0.5027 - acc: 0.614 - ETA: 41s - loss: -0.5049 - acc: 0.615 - ETA: 40s - loss: -0.5108 - acc: 0.616 - ETA: 40s - loss: -0.5027 - acc: 0.618 - ETA: 40s - loss: -0.4975 - acc: 0.619 - ETA: 39s - loss: -0.5184 - acc: 0.620 - ETA: 39s - loss: -0.5303 - acc: 0.620 - ETA: 39s - loss: -0.5370 - acc: 0.621 - ETA: 39s - loss: -0.5436 - acc: 0.622 - ETA: 38s - loss: -0.5417 - acc: 0.622 - ETA: 38s - loss: -0.5530 - acc: 0.622 - ETA: 38s - loss: -0.5734 - acc: 0.622 - ETA: 38s - loss: -0.5945 - acc: 0.622 - ETA: 37s - loss: -0.5967 - acc: 0.623 - ETA: 37s - loss: -0.6107 - acc: 0.623 - ETA: 37s - loss: -0.6221 - acc: 0.623 - ETA: 36s - loss: -0.6226 - acc: 0.624 - ETA: 36s - loss: -0.6317 - acc: 0.625 - ETA: 36s - loss: -0.6434 - acc: 0.626 - ETA: 35s - loss: -0.6593 - acc: 0.626 - ETA: 35s - loss: -0.6633 - acc: 0.627 - ETA: 35s - loss: -0.6700 - acc: 0.628 - ETA: 35s - loss: -0.6799 - acc: 0.629 - ETA: 34s - loss: -0.6916 - acc: 0.630 - ETA: 34s - loss: -0.7006 - acc: 0.629 - ETA: 34s - loss: -0.7050 - acc: 0.630 - ETA: 34s - loss: -0.7184 - acc: 0.631 - ETA: 33s - loss: -0.7189 - acc: 0.631 - ETA: 33s - loss: -0.7153 - acc: 0.633 - ETA: 33s - loss: -0.7277 - acc: 0.634 - ETA: 32s - loss: -0.7277 - acc: 0.635 - ETA: 32s - loss: -0.7366 - acc: 0.635 - ETA: 32s - loss: -0.7481 - acc: 0.636 - ETA: 31s - loss: -0.7617 - acc: 0.636 - ETA: 31s - loss: -0.7628 - acc: 0.637 - ETA: 31s - loss: -0.7634 - acc: 0.638 - ETA: 31s - loss: -0.7709 - acc: 0.638 - ETA: 30s - loss: -0.7741 - acc: 0.639 - ETA: 30s - loss: -0.7740 - acc: 0.640 - ETA: 30s - loss: -0.7708 - acc: 0.641 - ETA: 30s - loss: -0.7735 - acc: 0.642 - ETA: 29s - loss: -0.7725 - acc: 0.641 - ETA: 29s - loss: -0.7806 - acc: 0.641 - ETA: 29s - loss: -0.7888 - acc: 0.642 - ETA: 28s - loss: -0.7838 - acc: 0.642 - ETA: 28s - loss: -0.7950 - acc: 0.642 - ETA: 28s - loss: -0.8012 - acc: 0.643 - ETA: 27s - loss: -0.8061 - acc: 0.644 - ETA: 27s - loss: -0.8258 - acc: 0.644 - ETA: 27s - loss: -0.8253 - acc: 0.645 - ETA: 26s - loss: -0.8296 - acc: 0.645 - ETA: 26s - loss: -0.8388 - acc: 0.646 - ETA: 26s - loss: -0.8382 - acc: 0.647 - ETA: 25s - loss: -0.8405 - acc: 0.648 - ETA: 25s - loss: -0.8509 - acc: 0.648 - ETA: 25s - loss: -0.8533 - acc: 0.648 - ETA: 24s - loss: -0.8597 - acc: 0.649 - ETA: 24s - loss: -0.8660 - acc: 0.649 - ETA: 24s - loss: -0.8715 - acc: 0.649 - ETA: 23s - loss: -0.8707 - acc: 0.651 - ETA: 23s - loss: -0.8764 - acc: 0.651 - ETA: 23s - loss: -0.8815 - acc: 0.651 - ETA: 22s - loss: -0.8837 - acc: 0.652 - ETA: 22s - loss: -0.8912 - acc: 0.652 - ETA: 22s - loss: -0.9034 - acc: 0.652 - ETA: 22s - loss: -0.9095 - acc: 0.653 - ETA: 21s - loss: -0.9112 - acc: 0.653 - ETA: 21s - loss: -0.9124 - acc: 0.6541\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8705/8705 [==============================] - ETA: 21s - loss: -0.9140 - acc: 0.654 - ETA: 20s - loss: -0.9085 - acc: 0.655 - ETA: 20s - loss: -0.9055 - acc: 0.656 - ETA: 20s - loss: -0.9126 - acc: 0.656 - ETA: 19s - loss: -0.9170 - acc: 0.656 - ETA: 19s - loss: -0.9317 - acc: 0.657 - ETA: 19s - loss: -0.9415 - acc: 0.657 - ETA: 18s - loss: -0.9474 - acc: 0.657 - ETA: 18s - loss: -0.9447 - acc: 0.658 - ETA: 18s - loss: -0.9395 - acc: 0.659 - ETA: 17s - loss: -0.9508 - acc: 0.659 - ETA: 17s - loss: -0.9493 - acc: 0.660 - ETA: 17s - loss: -0.9485 - acc: 0.660 - ETA: 16s - loss: -0.9671 - acc: 0.660 - ETA: 16s - loss: -0.9734 - acc: 0.660 - ETA: 16s - loss: -0.9746 - acc: 0.661 - ETA: 16s - loss: -0.9829 - acc: 0.661 - ETA: 15s - loss: -0.9940 - acc: 0.661 - ETA: 15s - loss: -0.9995 - acc: 0.661 - ETA: 15s - loss: -1.0010 - acc: 0.661 - ETA: 14s - loss: -1.0036 - acc: 0.661 - ETA: 14s - loss: -1.0024 - acc: 0.662 - ETA: 14s - loss: -1.0024 - acc: 0.662 - ETA: 13s - loss: -1.0035 - acc: 0.663 - ETA: 13s - loss: -1.0065 - acc: 0.663 - ETA: 13s - loss: -1.0166 - acc: 0.663 - ETA: 12s - loss: -1.0201 - acc: 0.663 - ETA: 12s - loss: -1.0223 - acc: 0.664 - ETA: 12s - loss: -1.0304 - acc: 0.664 - ETA: 11s - loss: -1.0317 - acc: 0.664 - ETA: 11s - loss: -1.0271 - acc: 0.664 - ETA: 11s - loss: -1.0248 - acc: 0.664 - ETA: 11s - loss: -1.0325 - acc: 0.664 - ETA: 10s - loss: -1.0444 - acc: 0.664 - ETA: 10s - loss: -1.0476 - acc: 0.664 - ETA: 10s - loss: -1.0478 - acc: 0.665 - ETA: 9s - loss: -1.0558 - acc: 0.665 - ETA: 9s - loss: -1.0610 - acc: 0.66 - ETA: 9s - loss: -1.0681 - acc: 0.66 - ETA: 8s - loss: -1.0692 - acc: 0.66 - ETA: 8s - loss: -1.0774 - acc: 0.66 - ETA: 8s - loss: -1.0753 - acc: 0.66 - ETA: 8s - loss: -1.0719 - acc: 0.66 - ETA: 7s - loss: -1.0847 - acc: 0.66 - ETA: 7s - loss: -1.0954 - acc: 0.66 - ETA: 7s - loss: -1.1028 - acc: 0.66 - ETA: 6s - loss: -1.1026 - acc: 0.66 - ETA: 6s - loss: -1.1044 - acc: 0.66 - ETA: 6s - loss: -1.1026 - acc: 0.66 - ETA: 5s - loss: -1.1035 - acc: 0.66 - ETA: 5s - loss: -1.1004 - acc: 0.66 - ETA: 5s - loss: -1.1092 - acc: 0.66 - ETA: 5s - loss: -1.1124 - acc: 0.66 - ETA: 4s - loss: -1.1190 - acc: 0.66 - ETA: 4s - loss: -1.1229 - acc: 0.66 - ETA: 4s - loss: -1.1223 - acc: 0.67 - ETA: 3s - loss: -1.1241 - acc: 0.67 - ETA: 3s - loss: -1.1212 - acc: 0.67 - ETA: 3s - loss: -1.1229 - acc: 0.67 - ETA: 3s - loss: -1.1211 - acc: 0.67 - ETA: 2s - loss: -1.1194 - acc: 0.67 - ETA: 2s - loss: -1.1213 - acc: 0.67 - ETA: 2s - loss: -1.1226 - acc: 0.67 - ETA: 1s - loss: -1.1274 - acc: 0.67 - ETA: 1s - loss: -1.1380 - acc: 0.67 - ETA: 1s - loss: -1.1451 - acc: 0.67 - ETA: 0s - loss: -1.1466 - acc: 0.67 - ETA: 0s - loss: -1.1475 - acc: 0.67 - ETA: 0s - loss: -1.1517 - acc: 0.67 - ETA: 0s - loss: -1.1566 - acc: 0.67 - 84s 10ms/step - loss: -1.1565 - acc: 0.6762 - val_loss: -2.0235 - val_acc: 0.6660\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.66598, saving model to CNN_best_weights.01-0.6660.hdf5\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528/8705 [=====================>........] - ETA: 1:09 - loss: -1.5260 - acc: 0.81 - ETA: 1:10 - loss: -1.7018 - acc: 0.82 - ETA: 1:11 - loss: -2.0151 - acc: 0.77 - ETA: 1:10 - loss: -2.3883 - acc: 0.74 - ETA: 1:10 - loss: -2.3129 - acc: 0.75 - ETA: 1:09 - loss: -2.2479 - acc: 0.76 - ETA: 1:09 - loss: -2.1386 - acc: 0.78 - ETA: 1:08 - loss: -2.0940 - acc: 0.78 - ETA: 1:08 - loss: -2.1092 - acc: 0.79 - ETA: 1:07 - loss: -2.0223 - acc: 0.79 - ETA: 1:07 - loss: -2.1216 - acc: 0.78 - ETA: 1:07 - loss: -2.0692 - acc: 0.79 - ETA: 1:06 - loss: -1.9626 - acc: 0.79 - ETA: 1:06 - loss: -1.8521 - acc: 0.80 - ETA: 1:06 - loss: -1.9122 - acc: 0.79 - ETA: 1:06 - loss: -1.9953 - acc: 0.78 - ETA: 1:06 - loss: -1.9500 - acc: 0.79 - ETA: 1:07 - loss: -1.9038 - acc: 0.79 - ETA: 1:06 - loss: -1.9540 - acc: 0.79 - ETA: 1:06 - loss: -1.9446 - acc: 0.79 - ETA: 1:06 - loss: -1.9698 - acc: 0.79 - ETA: 1:06 - loss: -1.9553 - acc: 0.79 - ETA: 1:06 - loss: -1.9690 - acc: 0.79 - ETA: 1:06 - loss: -1.9424 - acc: 0.79 - ETA: 1:06 - loss: -1.9822 - acc: 0.79 - ETA: 1:06 - loss: -1.9563 - acc: 0.79 - ETA: 1:06 - loss: -1.9307 - acc: 0.79 - ETA: 1:06 - loss: -1.9557 - acc: 0.78 - ETA: 1:05 - loss: -1.9726 - acc: 0.78 - ETA: 1:05 - loss: -1.9201 - acc: 0.79 - ETA: 1:05 - loss: -1.8867 - acc: 0.79 - ETA: 1:05 - loss: -1.8567 - acc: 0.79 - ETA: 1:04 - loss: -1.8891 - acc: 0.79 - ETA: 1:04 - loss: -1.9018 - acc: 0.79 - ETA: 1:04 - loss: -1.8612 - acc: 0.79 - ETA: 1:04 - loss: -1.8431 - acc: 0.79 - ETA: 1:03 - loss: -1.8331 - acc: 0.79 - ETA: 1:03 - loss: -1.8358 - acc: 0.80 - ETA: 1:03 - loss: -1.7962 - acc: 0.80 - ETA: 1:03 - loss: -1.8195 - acc: 0.80 - ETA: 1:03 - loss: -1.7900 - acc: 0.80 - ETA: 1:03 - loss: -1.7642 - acc: 0.80 - ETA: 1:03 - loss: -1.7462 - acc: 0.80 - ETA: 1:02 - loss: -1.8044 - acc: 0.80 - ETA: 1:02 - loss: -1.7831 - acc: 0.80 - ETA: 1:02 - loss: -1.7623 - acc: 0.80 - ETA: 1:02 - loss: -1.7795 - acc: 0.80 - ETA: 1:02 - loss: -1.8086 - acc: 0.80 - ETA: 1:02 - loss: -1.8243 - acc: 0.79 - ETA: 1:02 - loss: -1.8294 - acc: 0.79 - ETA: 1:02 - loss: -1.8319 - acc: 0.79 - ETA: 1:02 - loss: -1.8358 - acc: 0.79 - ETA: 1:02 - loss: -1.8718 - acc: 0.79 - ETA: 1:02 - loss: -1.8805 - acc: 0.79 - ETA: 1:02 - loss: -1.9175 - acc: 0.78 - ETA: 1:03 - loss: -1.9154 - acc: 0.78 - ETA: 1:04 - loss: -1.8971 - acc: 0.78 - ETA: 1:04 - loss: -1.8972 - acc: 0.78 - ETA: 1:05 - loss: -1.8796 - acc: 0.79 - ETA: 1:04 - loss: -1.8689 - acc: 0.79 - ETA: 1:04 - loss: -1.8613 - acc: 0.79 - ETA: 1:04 - loss: -1.8580 - acc: 0.79 - ETA: 1:04 - loss: -1.8602 - acc: 0.79 - ETA: 1:04 - loss: -1.8623 - acc: 0.79 - ETA: 1:04 - loss: -1.8349 - acc: 0.79 - ETA: 1:04 - loss: -1.8310 - acc: 0.79 - ETA: 1:04 - loss: -1.8347 - acc: 0.79 - ETA: 1:04 - loss: -1.8293 - acc: 0.79 - ETA: 1:04 - loss: -1.8451 - acc: 0.79 - ETA: 1:04 - loss: -1.8700 - acc: 0.79 - ETA: 1:04 - loss: -1.9209 - acc: 0.78 - ETA: 1:03 - loss: -1.9531 - acc: 0.78 - ETA: 1:03 - loss: -1.9548 - acc: 0.78 - ETA: 1:03 - loss: -1.9451 - acc: 0.78 - ETA: 1:03 - loss: -1.9643 - acc: 0.78 - ETA: 1:03 - loss: -1.9633 - acc: 0.78 - ETA: 1:03 - loss: -1.9684 - acc: 0.78 - ETA: 1:03 - loss: -1.9462 - acc: 0.78 - ETA: 1:02 - loss: -1.9354 - acc: 0.78 - ETA: 1:02 - loss: -1.9257 - acc: 0.78 - ETA: 1:02 - loss: -1.9431 - acc: 0.78 - ETA: 1:02 - loss: -1.9482 - acc: 0.78 - ETA: 1:01 - loss: -1.9407 - acc: 0.78 - ETA: 1:01 - loss: -1.9174 - acc: 0.78 - ETA: 1:01 - loss: -1.9013 - acc: 0.78 - ETA: 1:00 - loss: -1.9051 - acc: 0.78 - ETA: 1:00 - loss: -1.9049 - acc: 0.78 - ETA: 1:00 - loss: -1.9110 - acc: 0.78 - ETA: 59s - loss: -1.9118 - acc: 0.7886 - ETA: 59s - loss: -1.9347 - acc: 0.788 - ETA: 59s - loss: -1.9506 - acc: 0.787 - ETA: 59s - loss: -1.9517 - acc: 0.786 - ETA: 58s - loss: -1.9394 - acc: 0.788 - ETA: 58s - loss: -1.9483 - acc: 0.787 - ETA: 58s - loss: -1.9724 - acc: 0.786 - ETA: 58s - loss: -1.9696 - acc: 0.785 - ETA: 57s - loss: -1.9649 - acc: 0.786 - ETA: 57s - loss: -1.9428 - acc: 0.787 - ETA: 57s - loss: -1.9401 - acc: 0.786 - ETA: 56s - loss: -1.9490 - acc: 0.786 - ETA: 56s - loss: -1.9434 - acc: 0.787 - ETA: 56s - loss: -1.9462 - acc: 0.787 - ETA: 56s - loss: -1.9484 - acc: 0.787 - ETA: 55s - loss: -1.9477 - acc: 0.787 - ETA: 55s - loss: -1.9644 - acc: 0.786 - ETA: 55s - loss: -1.9686 - acc: 0.786 - ETA: 54s - loss: -1.9582 - acc: 0.785 - ETA: 54s - loss: -1.9704 - acc: 0.785 - ETA: 53s - loss: -1.9676 - acc: 0.785 - ETA: 53s - loss: -1.9718 - acc: 0.785 - ETA: 53s - loss: -1.9741 - acc: 0.785 - ETA: 52s - loss: -1.9838 - acc: 0.784 - ETA: 52s - loss: -1.9910 - acc: 0.784 - ETA: 52s - loss: -1.9888 - acc: 0.784 - ETA: 51s - loss: -1.9877 - acc: 0.784 - ETA: 51s - loss: -2.0021 - acc: 0.783 - ETA: 51s - loss: -2.0118 - acc: 0.782 - ETA: 50s - loss: -2.0005 - acc: 0.782 - ETA: 50s - loss: -2.0109 - acc: 0.782 - ETA: 50s - loss: -2.0171 - acc: 0.782 - ETA: 49s - loss: -2.0140 - acc: 0.782 - ETA: 49s - loss: -2.0068 - acc: 0.782 - ETA: 48s - loss: -2.0097 - acc: 0.782 - ETA: 48s - loss: -2.0044 - acc: 0.782 - ETA: 48s - loss: -1.9917 - acc: 0.782 - ETA: 47s - loss: -2.0038 - acc: 0.781 - ETA: 47s - loss: -2.0026 - acc: 0.780 - ETA: 47s - loss: -2.0096 - acc: 0.779 - ETA: 46s - loss: -2.0040 - acc: 0.779 - ETA: 46s - loss: -2.0085 - acc: 0.779 - ETA: 45s - loss: -2.0132 - acc: 0.779 - ETA: 45s - loss: -2.0195 - acc: 0.778 - ETA: 45s - loss: -2.0173 - acc: 0.779 - ETA: 44s - loss: -2.0156 - acc: 0.779 - ETA: 44s - loss: -2.0212 - acc: 0.779 - ETA: 44s - loss: -2.0214 - acc: 0.779 - ETA: 43s - loss: -2.0302 - acc: 0.779 - ETA: 43s - loss: -2.0333 - acc: 0.779 - ETA: 43s - loss: -2.0342 - acc: 0.779 - ETA: 42s - loss: -2.0331 - acc: 0.779 - ETA: 42s - loss: -2.0523 - acc: 0.778 - ETA: 41s - loss: -2.0477 - acc: 0.777 - ETA: 41s - loss: -2.0509 - acc: 0.777 - ETA: 41s - loss: -2.0535 - acc: 0.777 - ETA: 40s - loss: -2.0527 - acc: 0.777 - ETA: 40s - loss: -2.0518 - acc: 0.778 - ETA: 40s - loss: -2.0501 - acc: 0.777 - ETA: 39s - loss: -2.0443 - acc: 0.778 - ETA: 39s - loss: -2.0452 - acc: 0.778 - ETA: 39s - loss: -2.0444 - acc: 0.778 - ETA: 38s - loss: -2.0479 - acc: 0.777 - ETA: 38s - loss: -2.0519 - acc: 0.778 - ETA: 37s - loss: -2.0515 - acc: 0.777 - ETA: 37s - loss: -2.0502 - acc: 0.777 - ETA: 37s - loss: -2.0487 - acc: 0.777 - ETA: 36s - loss: -2.0558 - acc: 0.777 - ETA: 36s - loss: -2.0550 - acc: 0.777 - ETA: 36s - loss: -2.0626 - acc: 0.777 - ETA: 35s - loss: -2.0650 - acc: 0.777 - ETA: 35s - loss: -2.0643 - acc: 0.778 - ETA: 35s - loss: -2.0573 - acc: 0.778 - ETA: 34s - loss: -2.0562 - acc: 0.778 - ETA: 34s - loss: -2.0549 - acc: 0.778 - ETA: 33s - loss: -2.0455 - acc: 0.777 - ETA: 33s - loss: -2.0420 - acc: 0.777 - ETA: 33s - loss: -2.0366 - acc: 0.778 - ETA: 32s - loss: -2.0330 - acc: 0.779 - ETA: 32s - loss: -2.0441 - acc: 0.778 - ETA: 32s - loss: -2.0438 - acc: 0.779 - ETA: 31s - loss: -2.0522 - acc: 0.779 - ETA: 31s - loss: -2.0541 - acc: 0.779 - ETA: 31s - loss: -2.0488 - acc: 0.779 - ETA: 30s - loss: -2.0486 - acc: 0.779 - ETA: 30s - loss: -2.0564 - acc: 0.778 - ETA: 30s - loss: -2.0529 - acc: 0.778 - ETA: 29s - loss: -2.0579 - acc: 0.778 - ETA: 29s - loss: -2.0534 - acc: 0.779 - ETA: 29s - loss: -2.0467 - acc: 0.778 - ETA: 28s - loss: -2.0459 - acc: 0.778 - ETA: 28s - loss: -2.0406 - acc: 0.778 - ETA: 28s - loss: -2.0394 - acc: 0.778 - ETA: 27s - loss: -2.0514 - acc: 0.778 - ETA: 27s - loss: -2.0502 - acc: 0.778 - ETA: 27s - loss: -2.0412 - acc: 0.778 - ETA: 26s - loss: -2.0397 - acc: 0.778 - ETA: 26s - loss: -2.0327 - acc: 0.779 - ETA: 26s - loss: -2.0282 - acc: 0.779 - ETA: 25s - loss: -2.0248 - acc: 0.780 - ETA: 25s - loss: -2.0307 - acc: 0.779 - ETA: 25s - loss: -2.0279 - acc: 0.779 - ETA: 24s - loss: -2.0288 - acc: 0.780 - ETA: 24s - loss: -2.0162 - acc: 0.780 - ETA: 24s - loss: -2.0146 - acc: 0.780 - ETA: 23s - loss: -2.0096 - acc: 0.780 - ETA: 23s - loss: -2.0087 - acc: 0.780 - ETA: 23s - loss: -2.0096 - acc: 0.780 - ETA: 22s - loss: -2.0260 - acc: 0.780 - ETA: 22s - loss: -2.0275 - acc: 0.780 - ETA: 22s - loss: -2.0204 - acc: 0.780 - ETA: 21s - loss: -2.0140 - acc: 0.780 - ETA: 21s - loss: -2.0156 - acc: 0.779 - ETA: 21s - loss: -2.0226 - acc: 0.779 - ETA: 20s - loss: -2.0190 - acc: 0.780 - ETA: 20s - loss: -2.0158 - acc: 0.7805"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8705/8705 [==============================] - ETA: 20s - loss: -2.0191 - acc: 0.780 - ETA: 19s - loss: -2.0141 - acc: 0.781 - ETA: 19s - loss: -2.0099 - acc: 0.781 - ETA: 19s - loss: -2.0114 - acc: 0.781 - ETA: 18s - loss: -2.0249 - acc: 0.781 - ETA: 18s - loss: -2.0237 - acc: 0.781 - ETA: 18s - loss: -2.0228 - acc: 0.781 - ETA: 17s - loss: -2.0262 - acc: 0.781 - ETA: 17s - loss: -2.0279 - acc: 0.781 - ETA: 17s - loss: -2.0176 - acc: 0.782 - ETA: 17s - loss: -2.0184 - acc: 0.781 - ETA: 16s - loss: -2.0189 - acc: 0.781 - ETA: 16s - loss: -2.0159 - acc: 0.782 - ETA: 16s - loss: -2.0157 - acc: 0.782 - ETA: 15s - loss: -2.0211 - acc: 0.781 - ETA: 15s - loss: -2.0185 - acc: 0.781 - ETA: 15s - loss: -2.0209 - acc: 0.781 - ETA: 14s - loss: -2.0249 - acc: 0.781 - ETA: 14s - loss: -2.0200 - acc: 0.781 - ETA: 14s - loss: -2.0183 - acc: 0.781 - ETA: 13s - loss: -2.0197 - acc: 0.781 - ETA: 13s - loss: -2.0217 - acc: 0.782 - ETA: 13s - loss: -2.0231 - acc: 0.782 - ETA: 13s - loss: -2.0216 - acc: 0.781 - ETA: 12s - loss: -2.0227 - acc: 0.781 - ETA: 12s - loss: -2.0192 - acc: 0.781 - ETA: 12s - loss: -2.0187 - acc: 0.782 - ETA: 11s - loss: -2.0160 - acc: 0.782 - ETA: 11s - loss: -2.0176 - acc: 0.782 - ETA: 11s - loss: -2.0194 - acc: 0.781 - ETA: 10s - loss: -2.0126 - acc: 0.782 - ETA: 10s - loss: -2.0145 - acc: 0.782 - ETA: 10s - loss: -2.0086 - acc: 0.782 - ETA: 10s - loss: -2.0214 - acc: 0.782 - ETA: 9s - loss: -2.0263 - acc: 0.781 - ETA: 9s - loss: -2.0262 - acc: 0.78 - ETA: 9s - loss: -2.0255 - acc: 0.78 - ETA: 8s - loss: -2.0166 - acc: 0.78 - ETA: 8s - loss: -2.0184 - acc: 0.78 - ETA: 8s - loss: -2.0188 - acc: 0.78 - ETA: 7s - loss: -2.0219 - acc: 0.78 - ETA: 7s - loss: -2.0345 - acc: 0.78 - ETA: 7s - loss: -2.0381 - acc: 0.78 - ETA: 7s - loss: -2.0443 - acc: 0.78 - ETA: 6s - loss: -2.0469 - acc: 0.78 - ETA: 6s - loss: -2.0498 - acc: 0.77 - ETA: 6s - loss: -2.0494 - acc: 0.77 - ETA: 5s - loss: -2.0466 - acc: 0.77 - ETA: 5s - loss: -2.0522 - acc: 0.77 - ETA: 5s - loss: -2.0461 - acc: 0.78 - ETA: 5s - loss: -2.0453 - acc: 0.78 - ETA: 4s - loss: -2.0529 - acc: 0.77 - ETA: 4s - loss: -2.0483 - acc: 0.77 - ETA: 4s - loss: -2.0455 - acc: 0.78 - ETA: 3s - loss: -2.0459 - acc: 0.77 - ETA: 3s - loss: -2.0437 - acc: 0.78 - ETA: 3s - loss: -2.0450 - acc: 0.78 - ETA: 2s - loss: -2.0487 - acc: 0.77 - ETA: 2s - loss: -2.0462 - acc: 0.78 - ETA: 2s - loss: -2.0478 - acc: 0.77 - ETA: 2s - loss: -2.0417 - acc: 0.77 - ETA: 1s - loss: -2.0379 - acc: 0.77 - ETA: 1s - loss: -2.0358 - acc: 0.78 - ETA: 1s - loss: -2.0369 - acc: 0.78 - ETA: 0s - loss: -2.0421 - acc: 0.78 - ETA: 0s - loss: -2.0506 - acc: 0.77 - ETA: 0s - loss: -2.0461 - acc: 0.77 - ETA: 0s - loss: -2.0452 - acc: 0.78 - 81s 9ms/step - loss: -2.0468 - acc: 0.7800 - val_loss: -2.1304 - val_acc: 0.7215\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.66598 to 0.72148, saving model to CNN_best_weights.02-0.7215.hdf5\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528/8705 [=====================>........] - ETA: 1:05 - loss: -1.0951 - acc: 0.81 - ETA: 1:05 - loss: -1.0020 - acc: 0.79 - ETA: 1:08 - loss: -1.4915 - acc: 0.81 - ETA: 1:10 - loss: -1.5163 - acc: 0.80 - ETA: 1:10 - loss: -1.9609 - acc: 0.78 - ETA: 1:10 - loss: -1.9404 - acc: 0.79 - ETA: 1:11 - loss: -2.0872 - acc: 0.79 - ETA: 1:10 - loss: -2.0901 - acc: 0.79 - ETA: 1:10 - loss: -2.0752 - acc: 0.79 - ETA: 1:09 - loss: -2.0915 - acc: 0.80 - ETA: 1:08 - loss: -1.9904 - acc: 0.80 - ETA: 1:08 - loss: -2.0570 - acc: 0.80 - ETA: 1:07 - loss: -2.0100 - acc: 0.81 - ETA: 1:07 - loss: -1.9677 - acc: 0.81 - ETA: 1:07 - loss: -1.9997 - acc: 0.81 - ETA: 1:07 - loss: -2.2269 - acc: 0.80 - ETA: 1:07 - loss: -2.2905 - acc: 0.79 - ETA: 1:07 - loss: -2.4273 - acc: 0.78 - ETA: 1:06 - loss: -2.3470 - acc: 0.79 - ETA: 1:06 - loss: -2.2967 - acc: 0.79 - ETA: 1:06 - loss: -2.2094 - acc: 0.79 - ETA: 1:06 - loss: -2.2528 - acc: 0.79 - ETA: 1:05 - loss: -2.3109 - acc: 0.79 - ETA: 1:05 - loss: -2.3048 - acc: 0.79 - ETA: 1:05 - loss: -2.3110 - acc: 0.79 - ETA: 1:05 - loss: -2.2904 - acc: 0.79 - ETA: 1:05 - loss: -2.2565 - acc: 0.79 - ETA: 1:05 - loss: -2.2632 - acc: 0.79 - ETA: 1:05 - loss: -2.2873 - acc: 0.79 - ETA: 1:05 - loss: -2.2746 - acc: 0.79 - ETA: 1:05 - loss: -2.2409 - acc: 0.79 - ETA: 1:04 - loss: -2.1696 - acc: 0.80 - ETA: 1:04 - loss: -2.1730 - acc: 0.80 - ETA: 1:04 - loss: -2.1222 - acc: 0.80 - ETA: 1:03 - loss: -2.0866 - acc: 0.80 - ETA: 1:03 - loss: -2.0950 - acc: 0.80 - ETA: 1:02 - loss: -2.1191 - acc: 0.80 - ETA: 1:02 - loss: -2.0832 - acc: 0.80 - ETA: 1:02 - loss: -2.1021 - acc: 0.80 - ETA: 1:01 - loss: -2.1110 - acc: 0.80 - ETA: 1:01 - loss: -2.0930 - acc: 0.80 - ETA: 1:01 - loss: -2.1020 - acc: 0.80 - ETA: 1:00 - loss: -2.1105 - acc: 0.80 - ETA: 1:00 - loss: -2.1129 - acc: 0.80 - ETA: 1:00 - loss: -2.1333 - acc: 0.80 - ETA: 59s - loss: -2.1504 - acc: 0.8037 - ETA: 59s - loss: -2.1421 - acc: 0.803 - ETA: 59s - loss: -2.1545 - acc: 0.802 - ETA: 59s - loss: -2.1591 - acc: 0.802 - ETA: 59s - loss: -2.1474 - acc: 0.800 - ETA: 58s - loss: -2.1480 - acc: 0.800 - ETA: 58s - loss: -2.1524 - acc: 0.799 - ETA: 58s - loss: -2.1675 - acc: 0.797 - ETA: 58s - loss: -2.1647 - acc: 0.798 - ETA: 58s - loss: -2.1671 - acc: 0.798 - ETA: 57s - loss: -2.1540 - acc: 0.799 - ETA: 57s - loss: -2.1393 - acc: 0.799 - ETA: 57s - loss: -2.1795 - acc: 0.798 - ETA: 57s - loss: -2.1675 - acc: 0.800 - ETA: 56s - loss: -2.1546 - acc: 0.801 - ETA: 56s - loss: -2.1490 - acc: 0.801 - ETA: 56s - loss: -2.1545 - acc: 0.802 - ETA: 55s - loss: -2.1345 - acc: 0.803 - ETA: 55s - loss: -2.1300 - acc: 0.802 - ETA: 55s - loss: -2.1249 - acc: 0.802 - ETA: 55s - loss: -2.1194 - acc: 0.802 - ETA: 54s - loss: -2.1021 - acc: 0.804 - ETA: 54s - loss: -2.1037 - acc: 0.804 - ETA: 54s - loss: -2.1165 - acc: 0.804 - ETA: 54s - loss: -2.1217 - acc: 0.804 - ETA: 53s - loss: -2.1053 - acc: 0.806 - ETA: 53s - loss: -2.0911 - acc: 0.806 - ETA: 53s - loss: -2.0756 - acc: 0.807 - ETA: 52s - loss: -2.0743 - acc: 0.808 - ETA: 52s - loss: -2.0856 - acc: 0.808 - ETA: 52s - loss: -2.1023 - acc: 0.807 - ETA: 51s - loss: -2.0862 - acc: 0.808 - ETA: 51s - loss: -2.0861 - acc: 0.808 - ETA: 51s - loss: -2.1006 - acc: 0.807 - ETA: 50s - loss: -2.1053 - acc: 0.807 - ETA: 50s - loss: -2.1216 - acc: 0.807 - ETA: 50s - loss: -2.1075 - acc: 0.807 - ETA: 50s - loss: -2.1038 - acc: 0.806 - ETA: 49s - loss: -2.1139 - acc: 0.806 - ETA: 49s - loss: -2.1341 - acc: 0.804 - ETA: 49s - loss: -2.1447 - acc: 0.804 - ETA: 48s - loss: -2.1199 - acc: 0.804 - ETA: 48s - loss: -2.1395 - acc: 0.803 - ETA: 48s - loss: -2.1253 - acc: 0.803 - ETA: 47s - loss: -2.1311 - acc: 0.803 - ETA: 47s - loss: -2.1369 - acc: 0.803 - ETA: 47s - loss: -2.1413 - acc: 0.801 - ETA: 47s - loss: -2.1303 - acc: 0.800 - ETA: 46s - loss: -2.1437 - acc: 0.800 - ETA: 46s - loss: -2.1473 - acc: 0.799 - ETA: 46s - loss: -2.1393 - acc: 0.800 - ETA: 45s - loss: -2.1427 - acc: 0.800 - ETA: 45s - loss: -2.1481 - acc: 0.799 - ETA: 45s - loss: -2.1615 - acc: 0.799 - ETA: 45s - loss: -2.1537 - acc: 0.800 - ETA: 44s - loss: -2.1538 - acc: 0.800 - ETA: 44s - loss: -2.1608 - acc: 0.799 - ETA: 44s - loss: -2.1737 - acc: 0.799 - ETA: 43s - loss: -2.1808 - acc: 0.799 - ETA: 43s - loss: -2.1836 - acc: 0.799 - ETA: 43s - loss: -2.2054 - acc: 0.798 - ETA: 43s - loss: -2.2029 - acc: 0.798 - ETA: 42s - loss: -2.2074 - acc: 0.798 - ETA: 42s - loss: -2.2015 - acc: 0.799 - ETA: 42s - loss: -2.1978 - acc: 0.799 - ETA: 41s - loss: -2.2044 - acc: 0.798 - ETA: 41s - loss: -2.2246 - acc: 0.798 - ETA: 41s - loss: -2.2231 - acc: 0.798 - ETA: 41s - loss: -2.2243 - acc: 0.798 - ETA: 40s - loss: -2.2290 - acc: 0.798 - ETA: 40s - loss: -2.2346 - acc: 0.798 - ETA: 40s - loss: -2.2402 - acc: 0.797 - ETA: 40s - loss: -2.2374 - acc: 0.798 - ETA: 39s - loss: -2.2449 - acc: 0.796 - ETA: 39s - loss: -2.2468 - acc: 0.797 - ETA: 39s - loss: -2.2394 - acc: 0.797 - ETA: 39s - loss: -2.2328 - acc: 0.798 - ETA: 38s - loss: -2.2337 - acc: 0.798 - ETA: 38s - loss: -2.2376 - acc: 0.798 - ETA: 38s - loss: -2.2157 - acc: 0.799 - ETA: 38s - loss: -2.2200 - acc: 0.798 - ETA: 37s - loss: -2.2182 - acc: 0.799 - ETA: 37s - loss: -2.2278 - acc: 0.799 - ETA: 37s - loss: -2.2327 - acc: 0.798 - ETA: 36s - loss: -2.2345 - acc: 0.798 - ETA: 36s - loss: -2.2278 - acc: 0.798 - ETA: 36s - loss: -2.2320 - acc: 0.798 - ETA: 36s - loss: -2.2410 - acc: 0.797 - ETA: 35s - loss: -2.2577 - acc: 0.797 - ETA: 35s - loss: -2.2553 - acc: 0.796 - ETA: 35s - loss: -2.2493 - acc: 0.796 - ETA: 35s - loss: -2.2426 - acc: 0.796 - ETA: 34s - loss: -2.2364 - acc: 0.797 - ETA: 34s - loss: -2.2411 - acc: 0.797 - ETA: 34s - loss: -2.2478 - acc: 0.796 - ETA: 33s - loss: -2.2565 - acc: 0.796 - ETA: 33s - loss: -2.2527 - acc: 0.796 - ETA: 33s - loss: -2.2648 - acc: 0.796 - ETA: 33s - loss: -2.2558 - acc: 0.797 - ETA: 32s - loss: -2.2460 - acc: 0.797 - ETA: 32s - loss: -2.2496 - acc: 0.797 - ETA: 32s - loss: -2.2602 - acc: 0.796 - ETA: 32s - loss: -2.2548 - acc: 0.797 - ETA: 31s - loss: -2.2619 - acc: 0.796 - ETA: 31s - loss: -2.2686 - acc: 0.796 - ETA: 31s - loss: -2.2683 - acc: 0.796 - ETA: 31s - loss: -2.2779 - acc: 0.795 - ETA: 30s - loss: -2.2889 - acc: 0.795 - ETA: 30s - loss: -2.2960 - acc: 0.795 - ETA: 30s - loss: -2.2919 - acc: 0.795 - ETA: 30s - loss: -2.3019 - acc: 0.794 - ETA: 29s - loss: -2.3029 - acc: 0.795 - ETA: 29s - loss: -2.3041 - acc: 0.795 - ETA: 29s - loss: -2.3072 - acc: 0.795 - ETA: 28s - loss: -2.3044 - acc: 0.795 - ETA: 28s - loss: -2.2974 - acc: 0.795 - ETA: 28s - loss: -2.2982 - acc: 0.795 - ETA: 28s - loss: -2.2863 - acc: 0.796 - ETA: 28s - loss: -2.2959 - acc: 0.796 - ETA: 27s - loss: -2.2910 - acc: 0.796 - ETA: 27s - loss: -2.2938 - acc: 0.796 - ETA: 27s - loss: -2.2890 - acc: 0.796 - ETA: 26s - loss: -2.2873 - acc: 0.796 - ETA: 26s - loss: -2.2825 - acc: 0.797 - ETA: 26s - loss: -2.2865 - acc: 0.797 - ETA: 26s - loss: -2.2932 - acc: 0.797 - ETA: 25s - loss: -2.2976 - acc: 0.796 - ETA: 25s - loss: -2.2987 - acc: 0.796 - ETA: 25s - loss: -2.2953 - acc: 0.796 - ETA: 25s - loss: -2.2900 - acc: 0.796 - ETA: 24s - loss: -2.2767 - acc: 0.797 - ETA: 24s - loss: -2.2778 - acc: 0.797 - ETA: 24s - loss: -2.2827 - acc: 0.797 - ETA: 24s - loss: -2.2894 - acc: 0.797 - ETA: 23s - loss: -2.2961 - acc: 0.797 - ETA: 23s - loss: -2.3026 - acc: 0.797 - ETA: 23s - loss: -2.3009 - acc: 0.797 - ETA: 23s - loss: -2.3125 - acc: 0.797 - ETA: 22s - loss: -2.3083 - acc: 0.797 - ETA: 22s - loss: -2.3066 - acc: 0.797 - ETA: 22s - loss: -2.3102 - acc: 0.797 - ETA: 22s - loss: -2.3085 - acc: 0.798 - ETA: 21s - loss: -2.3041 - acc: 0.798 - ETA: 21s - loss: -2.3019 - acc: 0.798 - ETA: 21s - loss: -2.2994 - acc: 0.799 - ETA: 21s - loss: -2.2997 - acc: 0.798 - ETA: 20s - loss: -2.3033 - acc: 0.799 - ETA: 20s - loss: -2.2990 - acc: 0.799 - ETA: 20s - loss: -2.2974 - acc: 0.799 - ETA: 20s - loss: -2.2879 - acc: 0.800 - ETA: 19s - loss: -2.2889 - acc: 0.800 - ETA: 19s - loss: -2.2913 - acc: 0.800 - ETA: 19s - loss: -2.2873 - acc: 0.801 - ETA: 19s - loss: -2.2932 - acc: 0.800 - ETA: 18s - loss: -2.2951 - acc: 0.800 - ETA: 18s - loss: -2.2908 - acc: 0.800 - ETA: 18s - loss: -2.2832 - acc: 0.801 - ETA: 18s - loss: -2.2784 - acc: 0.801 - ETA: 17s - loss: -2.2728 - acc: 0.8015"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8705/8705 [==============================] - ETA: 17s - loss: -2.2669 - acc: 0.802 - ETA: 17s - loss: -2.2623 - acc: 0.802 - ETA: 17s - loss: -2.2597 - acc: 0.802 - ETA: 16s - loss: -2.2550 - acc: 0.802 - ETA: 16s - loss: -2.2483 - acc: 0.802 - ETA: 16s - loss: -2.2509 - acc: 0.802 - ETA: 15s - loss: -2.2442 - acc: 0.802 - ETA: 15s - loss: -2.2430 - acc: 0.802 - ETA: 15s - loss: -2.2390 - acc: 0.802 - ETA: 15s - loss: -2.2419 - acc: 0.802 - ETA: 14s - loss: -2.2416 - acc: 0.801 - ETA: 14s - loss: -2.2353 - acc: 0.802 - ETA: 14s - loss: -2.2375 - acc: 0.801 - ETA: 14s - loss: -2.2393 - acc: 0.801 - ETA: 13s - loss: -2.2410 - acc: 0.801 - ETA: 13s - loss: -2.2372 - acc: 0.801 - ETA: 13s - loss: -2.2332 - acc: 0.802 - ETA: 13s - loss: -2.2323 - acc: 0.802 - ETA: 12s - loss: -2.2418 - acc: 0.801 - ETA: 12s - loss: -2.2562 - acc: 0.800 - ETA: 12s - loss: -2.2528 - acc: 0.801 - ETA: 12s - loss: -2.2626 - acc: 0.800 - ETA: 11s - loss: -2.2656 - acc: 0.800 - ETA: 11s - loss: -2.2565 - acc: 0.801 - ETA: 11s - loss: -2.2503 - acc: 0.801 - ETA: 10s - loss: -2.2554 - acc: 0.801 - ETA: 10s - loss: -2.2600 - acc: 0.800 - ETA: 10s - loss: -2.2637 - acc: 0.800 - ETA: 10s - loss: -2.2668 - acc: 0.800 - ETA: 9s - loss: -2.2604 - acc: 0.801 - ETA: 9s - loss: -2.2677 - acc: 0.80 - ETA: 9s - loss: -2.2623 - acc: 0.80 - ETA: 9s - loss: -2.2608 - acc: 0.80 - ETA: 8s - loss: -2.2563 - acc: 0.80 - ETA: 8s - loss: -2.2565 - acc: 0.80 - ETA: 8s - loss: -2.2567 - acc: 0.80 - ETA: 8s - loss: -2.2552 - acc: 0.80 - ETA: 7s - loss: -2.2471 - acc: 0.80 - ETA: 7s - loss: -2.2506 - acc: 0.80 - ETA: 7s - loss: -2.2454 - acc: 0.80 - ETA: 7s - loss: -2.2400 - acc: 0.80 - ETA: 6s - loss: -2.2430 - acc: 0.80 - ETA: 6s - loss: -2.2415 - acc: 0.80 - ETA: 6s - loss: -2.2457 - acc: 0.80 - ETA: 6s - loss: -2.2415 - acc: 0.80 - ETA: 5s - loss: -2.2535 - acc: 0.80 - ETA: 5s - loss: -2.2531 - acc: 0.80 - ETA: 5s - loss: -2.2495 - acc: 0.80 - ETA: 4s - loss: -2.2540 - acc: 0.80 - ETA: 4s - loss: -2.2588 - acc: 0.80 - ETA: 4s - loss: -2.2630 - acc: 0.80 - ETA: 4s - loss: -2.2716 - acc: 0.80 - ETA: 3s - loss: -2.2699 - acc: 0.80 - ETA: 3s - loss: -2.2703 - acc: 0.80 - ETA: 3s - loss: -2.2696 - acc: 0.80 - ETA: 3s - loss: -2.2668 - acc: 0.80 - ETA: 2s - loss: -2.2611 - acc: 0.80 - ETA: 2s - loss: -2.2577 - acc: 0.80 - ETA: 2s - loss: -2.2600 - acc: 0.80 - ETA: 2s - loss: -2.2590 - acc: 0.80 - ETA: 1s - loss: -2.2598 - acc: 0.80 - ETA: 1s - loss: -2.2558 - acc: 0.80 - ETA: 1s - loss: -2.2556 - acc: 0.80 - ETA: 1s - loss: -2.2520 - acc: 0.80 - ETA: 0s - loss: -2.2522 - acc: 0.80 - ETA: 0s - loss: -2.2560 - acc: 0.80 - ETA: 0s - loss: -2.2584 - acc: 0.80 - ETA: 0s - loss: -2.2560 - acc: 0.80 - 72s 8ms/step - loss: -2.2557 - acc: 0.8023 - val_loss: -1.9721 - val_acc: 0.7029\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.72148\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528/8705 [=====================>........] - ETA: 1:04 - loss: -1.4791 - acc: 0.90 - ETA: 1:05 - loss: -2.2227 - acc: 0.84 - ETA: 1:05 - loss: -2.8023 - acc: 0.81 - ETA: 1:05 - loss: -3.3275 - acc: 0.73 - ETA: 1:04 - loss: -3.6461 - acc: 0.71 - ETA: 1:05 - loss: -3.3565 - acc: 0.72 - ETA: 1:04 - loss: -3.4839 - acc: 0.71 - ETA: 1:05 - loss: -3.2877 - acc: 0.73 - ETA: 1:04 - loss: -3.1191 - acc: 0.73 - ETA: 1:04 - loss: -3.1555 - acc: 0.74 - ETA: 1:04 - loss: -2.9128 - acc: 0.75 - ETA: 1:03 - loss: -2.7935 - acc: 0.77 - ETA: 1:03 - loss: -2.8465 - acc: 0.77 - ETA: 1:03 - loss: -2.8183 - acc: 0.77 - ETA: 1:02 - loss: -2.7390 - acc: 0.77 - ETA: 1:02 - loss: -2.7201 - acc: 0.77 - ETA: 1:02 - loss: -2.7700 - acc: 0.77 - ETA: 1:02 - loss: -2.6959 - acc: 0.77 - ETA: 1:01 - loss: -2.7557 - acc: 0.77 - ETA: 1:02 - loss: -2.7923 - acc: 0.77 - ETA: 1:02 - loss: -2.7066 - acc: 0.77 - ETA: 1:02 - loss: -2.7119 - acc: 0.77 - ETA: 1:01 - loss: -2.6415 - acc: 0.77 - ETA: 1:01 - loss: -2.6965 - acc: 0.77 - ETA: 1:02 - loss: -2.6390 - acc: 0.77 - ETA: 1:02 - loss: -2.6811 - acc: 0.77 - ETA: 1:01 - loss: -2.7477 - acc: 0.77 - ETA: 1:01 - loss: -2.7027 - acc: 0.77 - ETA: 1:01 - loss: -2.7293 - acc: 0.77 - ETA: 1:01 - loss: -2.6707 - acc: 0.77 - ETA: 1:01 - loss: -2.6158 - acc: 0.78 - ETA: 1:01 - loss: -2.6258 - acc: 0.78 - ETA: 1:00 - loss: -2.6457 - acc: 0.78 - ETA: 1:00 - loss: -2.6106 - acc: 0.78 - ETA: 1:00 - loss: -2.5922 - acc: 0.78 - ETA: 1:00 - loss: -2.5983 - acc: 0.78 - ETA: 1:00 - loss: -2.5529 - acc: 0.78 - ETA: 1:00 - loss: -2.5774 - acc: 0.78 - ETA: 1:00 - loss: -2.5959 - acc: 0.78 - ETA: 1:00 - loss: -2.5931 - acc: 0.78 - ETA: 1:00 - loss: -2.5410 - acc: 0.78 - ETA: 1:00 - loss: -2.6109 - acc: 0.78 - ETA: 1:00 - loss: -2.6072 - acc: 0.78 - ETA: 1:00 - loss: -2.6377 - acc: 0.78 - ETA: 1:00 - loss: -2.5867 - acc: 0.78 - ETA: 59s - loss: -2.5621 - acc: 0.7901 - ETA: 1:00 - loss: -2.5498 - acc: 0.79 - ETA: 59s - loss: -2.5381 - acc: 0.7936 - ETA: 59s - loss: -2.5773 - acc: 0.792 - ETA: 59s - loss: -2.5876 - acc: 0.790 - ETA: 58s - loss: -2.6147 - acc: 0.789 - ETA: 58s - loss: -2.6295 - acc: 0.788 - ETA: 58s - loss: -2.6264 - acc: 0.788 - ETA: 58s - loss: -2.6054 - acc: 0.790 - ETA: 57s - loss: -2.5904 - acc: 0.790 - ETA: 57s - loss: -2.5791 - acc: 0.791 - ETA: 57s - loss: -2.5688 - acc: 0.793 - ETA: 56s - loss: -2.5867 - acc: 0.791 - ETA: 56s - loss: -2.5739 - acc: 0.791 - ETA: 56s - loss: -2.5538 - acc: 0.792 - ETA: 55s - loss: -2.5282 - acc: 0.792 - ETA: 55s - loss: -2.5474 - acc: 0.790 - ETA: 55s - loss: -2.5601 - acc: 0.790 - ETA: 54s - loss: -2.5590 - acc: 0.791 - ETA: 54s - loss: -2.5414 - acc: 0.792 - ETA: 54s - loss: -2.5606 - acc: 0.790 - ETA: 53s - loss: -2.5447 - acc: 0.792 - ETA: 53s - loss: -2.5357 - acc: 0.793 - ETA: 53s - loss: -2.5388 - acc: 0.792 - ETA: 52s - loss: -2.5381 - acc: 0.792 - ETA: 52s - loss: -2.5208 - acc: 0.793 - ETA: 52s - loss: -2.5063 - acc: 0.795 - ETA: 51s - loss: -2.4924 - acc: 0.796 - ETA: 51s - loss: -2.5047 - acc: 0.796 - ETA: 51s - loss: -2.5107 - acc: 0.795 - ETA: 51s - loss: -2.5038 - acc: 0.796 - ETA: 50s - loss: -2.4841 - acc: 0.797 - ETA: 50s - loss: -2.5012 - acc: 0.797 - ETA: 50s - loss: -2.4918 - acc: 0.798 - ETA: 50s - loss: -2.5229 - acc: 0.796 - ETA: 49s - loss: -2.4962 - acc: 0.798 - ETA: 49s - loss: -2.4957 - acc: 0.798 - ETA: 49s - loss: -2.4902 - acc: 0.798 - ETA: 49s - loss: -2.4956 - acc: 0.798 - ETA: 49s - loss: -2.5131 - acc: 0.797 - ETA: 48s - loss: -2.5405 - acc: 0.796 - ETA: 48s - loss: -2.5342 - acc: 0.797 - ETA: 48s - loss: -2.5450 - acc: 0.797 - ETA: 48s - loss: -2.5443 - acc: 0.797 - ETA: 47s - loss: -2.5356 - acc: 0.798 - ETA: 47s - loss: -2.5459 - acc: 0.798 - ETA: 47s - loss: -2.5452 - acc: 0.797 - ETA: 47s - loss: -2.5339 - acc: 0.799 - ETA: 46s - loss: -2.5234 - acc: 0.799 - ETA: 46s - loss: -2.5163 - acc: 0.799 - ETA: 46s - loss: -2.4946 - acc: 0.801 - ETA: 46s - loss: -2.5071 - acc: 0.800 - ETA: 45s - loss: -2.5069 - acc: 0.801 - ETA: 45s - loss: -2.5152 - acc: 0.800 - ETA: 45s - loss: -2.5095 - acc: 0.800 - ETA: 45s - loss: -2.5088 - acc: 0.801 - ETA: 44s - loss: -2.5081 - acc: 0.801 - ETA: 44s - loss: -2.5039 - acc: 0.801 - ETA: 44s - loss: -2.5036 - acc: 0.801 - ETA: 44s - loss: -2.4970 - acc: 0.801 - ETA: 43s - loss: -2.4748 - acc: 0.803 - ETA: 43s - loss: -2.4606 - acc: 0.804 - ETA: 43s - loss: -2.4641 - acc: 0.803 - ETA: 43s - loss: -2.4510 - acc: 0.804 - ETA: 42s - loss: -2.4271 - acc: 0.805 - ETA: 42s - loss: -2.4127 - acc: 0.806 - ETA: 42s - loss: -2.4235 - acc: 0.805 - ETA: 41s - loss: -2.4301 - acc: 0.804 - ETA: 41s - loss: -2.4216 - acc: 0.804 - ETA: 41s - loss: -2.4220 - acc: 0.805 - ETA: 41s - loss: -2.4180 - acc: 0.805 - ETA: 40s - loss: -2.4353 - acc: 0.804 - ETA: 40s - loss: -2.4352 - acc: 0.805 - ETA: 40s - loss: -2.4440 - acc: 0.804 - ETA: 39s - loss: -2.4444 - acc: 0.805 - ETA: 39s - loss: -2.4364 - acc: 0.806 - ETA: 39s - loss: -2.4487 - acc: 0.805 - ETA: 39s - loss: -2.4406 - acc: 0.805 - ETA: 38s - loss: -2.4490 - acc: 0.805 - ETA: 38s - loss: -2.4453 - acc: 0.806 - ETA: 38s - loss: -2.4455 - acc: 0.806 - ETA: 37s - loss: -2.4550 - acc: 0.805 - ETA: 37s - loss: -2.4475 - acc: 0.806 - ETA: 37s - loss: -2.4588 - acc: 0.805 - ETA: 37s - loss: -2.4527 - acc: 0.805 - ETA: 36s - loss: -2.4553 - acc: 0.805 - ETA: 36s - loss: -2.4544 - acc: 0.805 - ETA: 36s - loss: -2.4486 - acc: 0.805 - ETA: 36s - loss: -2.4412 - acc: 0.806 - ETA: 35s - loss: -2.4379 - acc: 0.806 - ETA: 35s - loss: -2.4419 - acc: 0.806 - ETA: 35s - loss: -2.4250 - acc: 0.807 - ETA: 35s - loss: -2.4327 - acc: 0.807 - ETA: 34s - loss: -2.4331 - acc: 0.807 - ETA: 34s - loss: -2.4298 - acc: 0.807 - ETA: 34s - loss: -2.4302 - acc: 0.808 - ETA: 34s - loss: -2.4306 - acc: 0.808 - ETA: 33s - loss: -2.4291 - acc: 0.808 - ETA: 33s - loss: -2.4226 - acc: 0.809 - ETA: 33s - loss: -2.4162 - acc: 0.809 - ETA: 33s - loss: -2.4066 - acc: 0.810 - ETA: 32s - loss: -2.4084 - acc: 0.810 - ETA: 32s - loss: -2.4194 - acc: 0.809 - ETA: 32s - loss: -2.4162 - acc: 0.809 - ETA: 32s - loss: -2.4128 - acc: 0.810 - ETA: 31s - loss: -2.4023 - acc: 0.810 - ETA: 31s - loss: -2.3992 - acc: 0.810 - ETA: 31s - loss: -2.3963 - acc: 0.810 - ETA: 31s - loss: -2.4001 - acc: 0.810 - ETA: 30s - loss: -2.3938 - acc: 0.811 - ETA: 30s - loss: -2.3939 - acc: 0.811 - ETA: 30s - loss: -2.4009 - acc: 0.810 - ETA: 30s - loss: -2.4046 - acc: 0.810 - ETA: 29s - loss: -2.3988 - acc: 0.811 - ETA: 29s - loss: -2.3994 - acc: 0.811 - ETA: 29s - loss: -2.4030 - acc: 0.811 - ETA: 29s - loss: -2.4125 - acc: 0.810 - ETA: 28s - loss: -2.4130 - acc: 0.810 - ETA: 28s - loss: -2.4224 - acc: 0.810 - ETA: 28s - loss: -2.4280 - acc: 0.809 - ETA: 28s - loss: -2.4240 - acc: 0.809 - ETA: 27s - loss: -2.4151 - acc: 0.810 - ETA: 27s - loss: -2.4186 - acc: 0.810 - ETA: 27s - loss: -2.4337 - acc: 0.809 - ETA: 27s - loss: -2.4268 - acc: 0.810 - ETA: 26s - loss: -2.4242 - acc: 0.810 - ETA: 26s - loss: -2.4176 - acc: 0.810 - ETA: 26s - loss: -2.4123 - acc: 0.811 - ETA: 26s - loss: -2.4127 - acc: 0.811 - ETA: 25s - loss: -2.4059 - acc: 0.811 - ETA: 25s - loss: -2.4058 - acc: 0.811 - ETA: 25s - loss: -2.4091 - acc: 0.811 - ETA: 24s - loss: -2.4123 - acc: 0.811 - ETA: 24s - loss: -2.4127 - acc: 0.811 - ETA: 24s - loss: -2.4127 - acc: 0.811 - ETA: 24s - loss: -2.4104 - acc: 0.812 - ETA: 23s - loss: -2.4074 - acc: 0.812 - ETA: 23s - loss: -2.4050 - acc: 0.812 - ETA: 23s - loss: -2.4106 - acc: 0.812 - ETA: 23s - loss: -2.4192 - acc: 0.812 - ETA: 22s - loss: -2.4147 - acc: 0.812 - ETA: 22s - loss: -2.4229 - acc: 0.811 - ETA: 22s - loss: -2.4225 - acc: 0.811 - ETA: 22s - loss: -2.4277 - acc: 0.811 - ETA: 21s - loss: -2.4306 - acc: 0.811 - ETA: 21s - loss: -2.4382 - acc: 0.811 - ETA: 21s - loss: -2.4359 - acc: 0.811 - ETA: 20s - loss: -2.4408 - acc: 0.810 - ETA: 20s - loss: -2.4385 - acc: 0.811 - ETA: 20s - loss: -2.4362 - acc: 0.811 - ETA: 20s - loss: -2.4313 - acc: 0.811 - ETA: 19s - loss: -2.4360 - acc: 0.810 - ETA: 19s - loss: -2.4311 - acc: 0.811 - ETA: 19s - loss: -2.4313 - acc: 0.811 - ETA: 19s - loss: -2.4337 - acc: 0.810 - ETA: 18s - loss: -2.4245 - acc: 0.811 - ETA: 18s - loss: -2.4248 - acc: 0.811 - ETA: 18s - loss: -2.4300 - acc: 0.811 - ETA: 17s - loss: -2.4278 - acc: 0.8117"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8705/8705 [==============================] - ETA: 17s - loss: -2.4257 - acc: 0.812 - ETA: 17s - loss: -2.4207 - acc: 0.812 - ETA: 17s - loss: -2.4138 - acc: 0.813 - ETA: 16s - loss: -2.4093 - acc: 0.813 - ETA: 16s - loss: -2.3978 - acc: 0.814 - ETA: 16s - loss: -2.4006 - acc: 0.814 - ETA: 16s - loss: -2.3981 - acc: 0.814 - ETA: 15s - loss: -2.3944 - acc: 0.814 - ETA: 15s - loss: -2.3948 - acc: 0.814 - ETA: 15s - loss: -2.3900 - acc: 0.814 - ETA: 15s - loss: -2.3852 - acc: 0.815 - ETA: 14s - loss: -2.3902 - acc: 0.814 - ETA: 14s - loss: -2.3930 - acc: 0.814 - ETA: 14s - loss: -2.3889 - acc: 0.815 - ETA: 13s - loss: -2.3870 - acc: 0.815 - ETA: 13s - loss: -2.3822 - acc: 0.815 - ETA: 13s - loss: -2.3886 - acc: 0.815 - ETA: 13s - loss: -2.3883 - acc: 0.815 - ETA: 12s - loss: -2.3903 - acc: 0.814 - ETA: 12s - loss: -2.3840 - acc: 0.815 - ETA: 12s - loss: -2.3882 - acc: 0.815 - ETA: 12s - loss: -2.3864 - acc: 0.815 - ETA: 11s - loss: -2.3798 - acc: 0.815 - ETA: 11s - loss: -2.3868 - acc: 0.815 - ETA: 11s - loss: -2.3894 - acc: 0.815 - ETA: 11s - loss: -2.3919 - acc: 0.815 - ETA: 10s - loss: -2.3858 - acc: 0.815 - ETA: 10s - loss: -2.3884 - acc: 0.815 - ETA: 10s - loss: -2.3837 - acc: 0.816 - ETA: 10s - loss: -2.3805 - acc: 0.816 - ETA: 9s - loss: -2.3808 - acc: 0.816 - ETA: 9s - loss: -2.3770 - acc: 0.81 - ETA: 9s - loss: -2.3866 - acc: 0.81 - ETA: 8s - loss: -2.3840 - acc: 0.81 - ETA: 8s - loss: -2.3823 - acc: 0.81 - ETA: 8s - loss: -2.3846 - acc: 0.81 - ETA: 8s - loss: -2.3849 - acc: 0.81 - ETA: 7s - loss: -2.3871 - acc: 0.81 - ETA: 7s - loss: -2.3791 - acc: 0.81 - ETA: 7s - loss: -2.3755 - acc: 0.81 - ETA: 7s - loss: -2.3734 - acc: 0.81 - ETA: 6s - loss: -2.3712 - acc: 0.81 - ETA: 6s - loss: -2.3696 - acc: 0.81 - ETA: 6s - loss: -2.3638 - acc: 0.81 - ETA: 6s - loss: -2.3601 - acc: 0.81 - ETA: 5s - loss: -2.3625 - acc: 0.81 - ETA: 5s - loss: -2.3622 - acc: 0.81 - ETA: 5s - loss: -2.3587 - acc: 0.81 - ETA: 5s - loss: -2.3651 - acc: 0.81 - ETA: 4s - loss: -2.3672 - acc: 0.81 - ETA: 4s - loss: -2.3629 - acc: 0.81 - ETA: 4s - loss: -2.3724 - acc: 0.81 - ETA: 3s - loss: -2.3799 - acc: 0.81 - ETA: 3s - loss: -2.3741 - acc: 0.81 - ETA: 3s - loss: -2.3758 - acc: 0.81 - ETA: 3s - loss: -2.3794 - acc: 0.81 - ETA: 2s - loss: -2.3779 - acc: 0.81 - ETA: 2s - loss: -2.3819 - acc: 0.81 - ETA: 2s - loss: -2.3821 - acc: 0.81 - ETA: 2s - loss: -2.3776 - acc: 0.81 - ETA: 1s - loss: -2.3759 - acc: 0.81 - ETA: 1s - loss: -2.3742 - acc: 0.81 - ETA: 1s - loss: -2.3746 - acc: 0.81 - ETA: 1s - loss: -2.3769 - acc: 0.81 - ETA: 0s - loss: -2.3753 - acc: 0.81 - ETA: 0s - loss: -2.3698 - acc: 0.81 - ETA: 0s - loss: -2.3720 - acc: 0.81 - ETA: 0s - loss: -2.3739 - acc: 0.81 - 73s 8ms/step - loss: -2.3736 - acc: 0.8162 - val_loss: -2.0316 - val_acc: 0.7408\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.72148 to 0.74078, saving model to CNN_best_weights.04-0.7408.hdf5\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6528/8705 [=====================>........] - ETA: 1:10 - loss: -2.9878 - acc: 0.81 - ETA: 1:12 - loss: -2.2408 - acc: 0.85 - ETA: 1:13 - loss: -1.9886 - acc: 0.87 - ETA: 1:12 - loss: -1.8650 - acc: 0.88 - ETA: 1:14 - loss: -1.6868 - acc: 0.89 - ETA: 1:13 - loss: -1.4886 - acc: 0.90 - ETA: 1:11 - loss: -1.5418 - acc: 0.89 - ETA: 1:11 - loss: -1.6513 - acc: 0.88 - ETA: 1:10 - loss: -1.7445 - acc: 0.88 - ETA: 1:10 - loss: -1.7194 - acc: 0.88 - ETA: 1:09 - loss: -1.8777 - acc: 0.87 - ETA: 1:08 - loss: -1.8105 - acc: 0.87 - ETA: 1:08 - loss: -1.7859 - acc: 0.87 - ETA: 1:07 - loss: -1.7998 - acc: 0.87 - ETA: 1:07 - loss: -2.0110 - acc: 0.86 - ETA: 1:07 - loss: -1.9782 - acc: 0.86 - ETA: 1:06 - loss: -1.9787 - acc: 0.86 - ETA: 1:06 - loss: -1.8962 - acc: 0.87 - ETA: 1:05 - loss: -1.8743 - acc: 0.87 - ETA: 1:05 - loss: -1.9332 - acc: 0.86 - ETA: 1:04 - loss: -1.9120 - acc: 0.86 - ETA: 1:04 - loss: -1.9377 - acc: 0.86 - ETA: 1:04 - loss: -2.0050 - acc: 0.86 - ETA: 1:03 - loss: -2.0251 - acc: 0.86 - ETA: 1:04 - loss: -2.0634 - acc: 0.86 - ETA: 1:04 - loss: -2.0397 - acc: 0.86 - ETA: 1:04 - loss: -2.1117 - acc: 0.85 - ETA: 1:03 - loss: -2.1546 - acc: 0.85 - ETA: 1:03 - loss: -2.1489 - acc: 0.85 - ETA: 1:03 - loss: -2.1602 - acc: 0.85 - ETA: 1:03 - loss: -2.2511 - acc: 0.84 - ETA: 1:02 - loss: -2.2741 - acc: 0.84 - ETA: 1:02 - loss: -2.3560 - acc: 0.84 - ETA: 1:02 - loss: -2.3447 - acc: 0.84 - ETA: 1:02 - loss: -2.3119 - acc: 0.84 - ETA: 1:02 - loss: -2.3436 - acc: 0.84 - ETA: 1:02 - loss: -2.3610 - acc: 0.84 - ETA: 1:01 - loss: -2.3640 - acc: 0.84 - ETA: 1:01 - loss: -2.3800 - acc: 0.83 - ETA: 1:01 - loss: -2.4195 - acc: 0.83 - ETA: 1:01 - loss: -2.3969 - acc: 0.83 - ETA: 1:01 - loss: -2.4229 - acc: 0.83 - ETA: 1:00 - loss: -2.4552 - acc: 0.83 - ETA: 1:00 - loss: -2.4446 - acc: 0.83 - ETA: 1:00 - loss: -2.4455 - acc: 0.83 - ETA: 1:00 - loss: -2.4461 - acc: 0.83 - ETA: 1:00 - loss: -2.4673 - acc: 0.83 - ETA: 1:00 - loss: -2.4781 - acc: 0.83 - ETA: 59s - loss: -2.4986 - acc: 0.8316 - ETA: 59s - loss: -2.4785 - acc: 0.833 - ETA: 59s - loss: -2.4492 - acc: 0.835 - ETA: 59s - loss: -2.4397 - acc: 0.834 - ETA: 58s - loss: -2.4876 - acc: 0.831 - ETA: 58s - loss: -2.4692 - acc: 0.832 - ETA: 58s - loss: -2.4726 - acc: 0.832 - ETA: 58s - loss: -2.4593 - acc: 0.832 - ETA: 58s - loss: -2.4599 - acc: 0.832 - ETA: 57s - loss: -2.4776 - acc: 0.831 - ETA: 57s - loss: -2.4777 - acc: 0.831 - ETA: 57s - loss: -2.4863 - acc: 0.831 - ETA: 56s - loss: -2.5055 - acc: 0.829 - ETA: 56s - loss: -2.4969 - acc: 0.830 - ETA: 56s - loss: -2.4733 - acc: 0.831 - ETA: 55s - loss: -2.4636 - acc: 0.831 - ETA: 55s - loss: -2.4563 - acc: 0.832 - ETA: 55s - loss: -2.4492 - acc: 0.832 - ETA: 55s - loss: -2.4347 - acc: 0.834 - ETA: 54s - loss: -2.4494 - acc: 0.832 - ETA: 54s - loss: -2.4561 - acc: 0.832 - ETA: 54s - loss: -2.4494 - acc: 0.831 - ETA: 54s - loss: -2.4499 - acc: 0.831 - ETA: 53s - loss: -2.4712 - acc: 0.830 - ETA: 53s - loss: -2.4971 - acc: 0.828 - ETA: 53s - loss: -2.4902 - acc: 0.829 - ETA: 52s - loss: -2.5034 - acc: 0.828 - ETA: 52s - loss: -2.5027 - acc: 0.828 - ETA: 52s - loss: -2.5009 - acc: 0.828 - ETA: 51s - loss: -2.4877 - acc: 0.829 - ETA: 51s - loss: -2.4814 - acc: 0.830 - ETA: 51s - loss: -2.4752 - acc: 0.830 - ETA: 50s - loss: -2.4692 - acc: 0.831 - ETA: 50s - loss: -2.4633 - acc: 0.831 - ETA: 50s - loss: -2.4696 - acc: 0.831 - ETA: 50s - loss: -2.4876 - acc: 0.830 - ETA: 50s - loss: -2.4760 - acc: 0.831 - ETA: 49s - loss: -2.4645 - acc: 0.831 - ETA: 49s - loss: -2.4648 - acc: 0.831 - ETA: 49s - loss: -2.4535 - acc: 0.832 - ETA: 49s - loss: -2.4763 - acc: 0.831 - ETA: 48s - loss: -2.4706 - acc: 0.831 - ETA: 48s - loss: -2.4870 - acc: 0.830 - ETA: 48s - loss: -2.4866 - acc: 0.830 - ETA: 48s - loss: -2.5001 - acc: 0.829 - ETA: 47s - loss: -2.4832 - acc: 0.829 - ETA: 47s - loss: -2.4885 - acc: 0.829 - ETA: 47s - loss: -2.4781 - acc: 0.830 - ETA: 47s - loss: -2.4629 - acc: 0.831 - ETA: 46s - loss: -2.4576 - acc: 0.831 - ETA: 46s - loss: -2.4326 - acc: 0.832 - ETA: 46s - loss: -2.4231 - acc: 0.832 - ETA: 46s - loss: -2.4386 - acc: 0.832 - ETA: 45s - loss: -2.4732 - acc: 0.830 - ETA: 45s - loss: -2.4733 - acc: 0.830 - ETA: 45s - loss: -2.4777 - acc: 0.829 - ETA: 45s - loss: -2.4968 - acc: 0.828 - ETA: 44s - loss: -2.4967 - acc: 0.828 - ETA: 44s - loss: -2.4941 - acc: 0.828 - ETA: 44s - loss: -2.4757 - acc: 0.828 - ETA: 44s - loss: -2.4573 - acc: 0.829 - ETA: 43s - loss: -2.4667 - acc: 0.829 - ETA: 43s - loss: -2.4624 - acc: 0.829 - ETA: 43s - loss: -2.4472 - acc: 0.830 - ETA: 43s - loss: -2.4475 - acc: 0.830 - ETA: 43s - loss: -2.4654 - acc: 0.829 - ETA: 42s - loss: -2.4743 - acc: 0.828 - ETA: 42s - loss: -2.4650 - acc: 0.829 - ETA: 42s - loss: -2.4554 - acc: 0.829 - ETA: 41s - loss: -2.4473 - acc: 0.830 - ETA: 41s - loss: -2.4501 - acc: 0.829 - ETA: 41s - loss: -2.4537 - acc: 0.829 - ETA: 41s - loss: -2.4540 - acc: 0.829 - ETA: 40s - loss: -2.4623 - acc: 0.829 - ETA: 40s - loss: -2.4827 - acc: 0.828 - ETA: 40s - loss: -2.4626 - acc: 0.829 - ETA: 40s - loss: -2.4628 - acc: 0.829 - ETA: 39s - loss: -2.4709 - acc: 0.828 - ETA: 39s - loss: -2.4788 - acc: 0.828 - ETA: 39s - loss: -2.4664 - acc: 0.828 - ETA: 39s - loss: -2.4554 - acc: 0.829 - ETA: 38s - loss: -2.4441 - acc: 0.829 - ETA: 38s - loss: -2.4507 - acc: 0.828 - ETA: 38s - loss: -2.4509 - acc: 0.828 - ETA: 37s - loss: -2.4474 - acc: 0.829 - ETA: 37s - loss: -2.4401 - acc: 0.829 - ETA: 37s - loss: -2.4479 - acc: 0.829 - ETA: 37s - loss: -2.4407 - acc: 0.830 - ETA: 36s - loss: -2.4338 - acc: 0.830 - ETA: 36s - loss: -2.4342 - acc: 0.830 - ETA: 36s - loss: -2.4274 - acc: 0.831 - ETA: 36s - loss: -2.4278 - acc: 0.830 - ETA: 35s - loss: -2.4316 - acc: 0.830 - ETA: 35s - loss: -2.4180 - acc: 0.830 - ETA: 35s - loss: -2.4214 - acc: 0.830 - ETA: 35s - loss: -2.4172 - acc: 0.830 - ETA: 34s - loss: -2.4177 - acc: 0.830 - ETA: 34s - loss: -2.4182 - acc: 0.830 - ETA: 34s - loss: -2.4110 - acc: 0.830 - ETA: 34s - loss: -2.4111 - acc: 0.830 - ETA: 33s - loss: -2.4284 - acc: 0.829 - ETA: 33s - loss: -2.4314 - acc: 0.829 - ETA: 33s - loss: -2.4473 - acc: 0.828 - ETA: 32s - loss: -2.4473 - acc: 0.827 - ETA: 32s - loss: -2.4573 - acc: 0.827 - ETA: 32s - loss: -2.4574 - acc: 0.826 - ETA: 31s - loss: -2.4576 - acc: 0.827 - ETA: 31s - loss: -2.4610 - acc: 0.826 - ETA: 31s - loss: -2.4608 - acc: 0.826 - ETA: 31s - loss: -2.4523 - acc: 0.826 - ETA: 30s - loss: -2.4525 - acc: 0.827 - ETA: 30s - loss: -2.4424 - acc: 0.827 - ETA: 30s - loss: -2.4365 - acc: 0.828 - ETA: 30s - loss: -2.4338 - acc: 0.827 - ETA: 29s - loss: -2.4280 - acc: 0.828 - ETA: 29s - loss: -2.4249 - acc: 0.828 - ETA: 29s - loss: -2.4161 - acc: 0.829 - ETA: 28s - loss: -2.4192 - acc: 0.829 - ETA: 28s - loss: -2.4107 - acc: 0.829 - ETA: 28s - loss: -2.4082 - acc: 0.829 - ETA: 28s - loss: -2.4028 - acc: 0.830 - ETA: 27s - loss: -2.3974 - acc: 0.830 - ETA: 27s - loss: -2.3921 - acc: 0.831 - ETA: 27s - loss: -2.3926 - acc: 0.831 - ETA: 27s - loss: -2.3959 - acc: 0.830 - ETA: 26s - loss: -2.4021 - acc: 0.830 - ETA: 26s - loss: -2.4054 - acc: 0.830 - ETA: 26s - loss: -2.4071 - acc: 0.830 - ETA: 25s - loss: -2.4081 - acc: 0.829 - ETA: 25s - loss: -2.4058 - acc: 0.829 - ETA: 25s - loss: -2.3951 - acc: 0.830 - ETA: 25s - loss: -2.3922 - acc: 0.829 - ETA: 24s - loss: -2.3982 - acc: 0.829 - ETA: 24s - loss: -2.4041 - acc: 0.829 - ETA: 24s - loss: -2.3963 - acc: 0.829 - ETA: 23s - loss: -2.3901 - acc: 0.829 - ETA: 23s - loss: -2.3933 - acc: 0.829 - ETA: 23s - loss: -2.3911 - acc: 0.829 - ETA: 23s - loss: -2.3854 - acc: 0.829 - ETA: 22s - loss: -2.3822 - acc: 0.829 - ETA: 22s - loss: -2.3801 - acc: 0.830 - ETA: 22s - loss: -2.3911 - acc: 0.829 - ETA: 22s - loss: -2.3772 - acc: 0.829 - ETA: 21s - loss: -2.3700 - acc: 0.829 - ETA: 21s - loss: -2.3706 - acc: 0.829 - ETA: 21s - loss: -2.3686 - acc: 0.829 - ETA: 21s - loss: -2.3768 - acc: 0.829 - ETA: 20s - loss: -2.3672 - acc: 0.830 - ETA: 20s - loss: -2.3624 - acc: 0.830 - ETA: 20s - loss: -2.3781 - acc: 0.829 - ETA: 19s - loss: -2.3686 - acc: 0.830 - ETA: 19s - loss: -2.3765 - acc: 0.829 - ETA: 19s - loss: -2.3746 - acc: 0.829 - ETA: 19s - loss: -2.3689 - acc: 0.830 - ETA: 18s - loss: -2.3646 - acc: 0.830 - ETA: 18s - loss: -2.3741 - acc: 0.8300"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8705/8705 [==============================] - ETA: 18s - loss: -2.3769 - acc: 0.829 - ETA: 17s - loss: -2.3798 - acc: 0.829 - ETA: 17s - loss: -2.3752 - acc: 0.829 - ETA: 17s - loss: -2.3769 - acc: 0.829 - ETA: 17s - loss: -2.3774 - acc: 0.829 - ETA: 16s - loss: -2.3651 - acc: 0.829 - ETA: 16s - loss: -2.3680 - acc: 0.829 - ETA: 16s - loss: -2.3662 - acc: 0.829 - ETA: 15s - loss: -2.3733 - acc: 0.829 - ETA: 15s - loss: -2.3712 - acc: 0.829 - ETA: 15s - loss: -2.3764 - acc: 0.828 - ETA: 15s - loss: -2.3746 - acc: 0.829 - ETA: 14s - loss: -2.3705 - acc: 0.829 - ETA: 14s - loss: -2.3711 - acc: 0.829 - ETA: 14s - loss: -2.3716 - acc: 0.829 - ETA: 14s - loss: -2.3766 - acc: 0.829 - ETA: 13s - loss: -2.3793 - acc: 0.829 - ETA: 13s - loss: -2.3774 - acc: 0.829 - ETA: 13s - loss: -2.3799 - acc: 0.829 - ETA: 13s - loss: -2.3760 - acc: 0.829 - ETA: 12s - loss: -2.3785 - acc: 0.828 - ETA: 12s - loss: -2.3789 - acc: 0.829 - ETA: 12s - loss: -2.3816 - acc: 0.828 - ETA: 11s - loss: -2.3908 - acc: 0.828 - ETA: 11s - loss: -2.3845 - acc: 0.828 - ETA: 11s - loss: -2.3848 - acc: 0.828 - ETA: 11s - loss: -2.3896 - acc: 0.828 - ETA: 10s - loss: -2.3984 - acc: 0.828 - ETA: 10s - loss: -2.3967 - acc: 0.828 - ETA: 10s - loss: -2.3949 - acc: 0.828 - ETA: 10s - loss: -2.3910 - acc: 0.828 - ETA: 9s - loss: -2.3872 - acc: 0.829 - ETA: 9s - loss: -2.4002 - acc: 0.82 - ETA: 9s - loss: -2.3964 - acc: 0.82 - ETA: 8s - loss: -2.3946 - acc: 0.82 - ETA: 8s - loss: -2.3899 - acc: 0.82 - ETA: 8s - loss: -2.3875 - acc: 0.82 - ETA: 8s - loss: -2.3981 - acc: 0.82 - ETA: 7s - loss: -2.3984 - acc: 0.82 - ETA: 7s - loss: -2.3988 - acc: 0.82 - ETA: 7s - loss: -2.3991 - acc: 0.82 - ETA: 7s - loss: -2.3943 - acc: 0.82 - ETA: 6s - loss: -2.3942 - acc: 0.82 - ETA: 6s - loss: -2.3925 - acc: 0.82 - ETA: 6s - loss: -2.3970 - acc: 0.82 - ETA: 5s - loss: -2.3954 - acc: 0.82 - ETA: 5s - loss: -2.3951 - acc: 0.82 - ETA: 5s - loss: -2.3975 - acc: 0.82 - ETA: 5s - loss: -2.3996 - acc: 0.82 - ETA: 4s - loss: -2.4038 - acc: 0.82 - ETA: 4s - loss: -2.4016 - acc: 0.82 - ETA: 4s - loss: -2.4077 - acc: 0.82 - ETA: 4s - loss: -2.4157 - acc: 0.82 - ETA: 3s - loss: -2.4102 - acc: 0.82 - ETA: 3s - loss: -2.4144 - acc: 0.82 - ETA: 3s - loss: -2.4146 - acc: 0.82 - ETA: 2s - loss: -2.4168 - acc: 0.82 - ETA: 2s - loss: -2.4168 - acc: 0.82 - ETA: 2s - loss: -2.4171 - acc: 0.82 - ETA: 2s - loss: -2.4136 - acc: 0.82 - ETA: 1s - loss: -2.4116 - acc: 0.82 - ETA: 1s - loss: -2.4194 - acc: 0.82 - ETA: 1s - loss: -2.4253 - acc: 0.82 - ETA: 1s - loss: -2.4232 - acc: 0.82 - ETA: 0s - loss: -2.4216 - acc: 0.82 - ETA: 0s - loss: -2.4195 - acc: 0.82 - ETA: 0s - loss: -2.4176 - acc: 0.82 - ETA: 0s - loss: -2.4225 - acc: 0.82 - 75s 9ms/step - loss: -2.4222 - acc: 0.8267 - val_loss: -2.0182 - val_acc: 0.7411\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.74078 to 0.74112, saving model to CNN_best_weights.05-0.7411.hdf5\n",
      "End : 2019-01-03 01:46:33\n",
      "This step took 393.565 seconds\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(\"Start : {0}\".format(start.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "filepath=\"CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model.fit(X_train_seq[:CUSTOM_TRAINING_SET_SIZE], \n",
    "          Y_train[:CUSTOM_TRAINING_SET_SIZE], \n",
    "          batch_size=32,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=(X_valid_seq[:CUSTOM_VALIDATION_SET_SIZE], Y_valid[:CUSTOM_VALIDATION_SET_SIZE]),\n",
    "          callbacks = [checkpoint])\n",
    "\n",
    "end = datetime.now()\n",
    "print(\"End : {0}\".format(end.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "print(\"This step took {0:.3f} seconds\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-rw-rw-rw-  1 user 0 122063888 2018-12-28 00:21 CNN_best_weights.01-0.8251.hdf5',\n",
       " '-rw-rw-rw-  1 user 0 122063888 2018-12-28 01:57 CNN_best_weights.02-0.8259.hdf5',\n",
       " '-rw-rw-rw-  1 user 0 122063960 2018-12-29 19:43 CNN_best_weights.01-0.8249.hdf5',\n",
       " '-rw-rw-rw-  1 user 0 122063960 2018-12-29 21:13 CNN_best_weights.02-0.8272.hdf5',\n",
       " '-rw-rw-rw-  1 user 0 122063888 2019-01-03 01:41 CNN_best_weights.01-0.6660.hdf5',\n",
       " '-rw-rw-rw-  1 user 0 122063888 2019-01-03 01:42 CNN_best_weights.02-0.7215.hdf5',\n",
       " '-rw-rw-rw-  1 user 0 122063888 2019-01-03 01:45 CNN_best_weights.04-0.7408.hdf5',\n",
       " '-rw-rw-rw-  1 user 0 122063888 2019-01-03 01:46 CNN_best_weights.05-0.7411.hdf5']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = !ls -ltr | grep CNN_best_weights\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are sorted by time (feel free to remove the \"-ltr\" part so that they are sorted by name). We will get the name of the last entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CNN_best_weights.05-0.7411.hdf5'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = filenames[-1].split(\" \")[-1]\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load that file and evaluate on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2901/2901 [==============================] - ETA: 13 - ETA: 3 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 465us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-2.0182063535282175, 0.7411237505130709]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_CNN_model = load_model(filename)\n",
    "loaded_CNN_model.evaluate(x=X_valid_seq, y=Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... also evaluate on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2903/2903 [==============================] - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 1s 403us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1.723641890679562, 0.7599035480742696]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_CNN_model.evaluate(x=X_test_seq, y=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results are good, but still worse than the Scikit Learn Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's again create the prediction probabilities for the test set (which we will need while creating the ROC curve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat_cnn = loaded_CNN_model.predict(X_test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
